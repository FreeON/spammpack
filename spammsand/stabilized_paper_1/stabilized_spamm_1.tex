\documentclass[letterpaper,twocolumn,amsmath,amsfont,amssymb,english,aps,jcp,preprintnumbers,groupaddress,nofootinbib,tightenlines,floatfix]{revtex4}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{amsthm}

%\documentclass[aps,prb,letterpaper,twocolumn,nofootinbib,showkeys]{revtex4-1}
%\documentclass[aps,amssymb,prl,letterpaper,twocolumn,nofootinbib,showkeys]{revtex4-1}

%\usepackage[backend=bibtex]{biblatex}

%    backend=biber,
%    style=authoryear,
%    natbib=true,
%    sortlocale=en_US,
%    url=false,
%    doi=true,f
%    eprint=false
%]{biblatex}
%\usepackage{hyperref}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\mmat}[1]{\widetilde{\boldsymbol{#1}}}
\newcommand{\matT}[1]{\boldsymbol{#1}^\dagger}
\newcommand{\ot}{  {\scriptstyle \otimes}_{ \tau } }
\newcommand{\ots}{ {\scriptstyle \otimes}_{ \! \tau_s } }
\newcommand{\oto}{ {\scriptstyle \otimes}_{ \! \tau_0 } }
\newcommand{\otm}{ {\scriptstyle \otimes}_{ \! \tau_m } }
\newcommand{\otmm}{ {\scriptstyle \otimes}_{ \! \tau_{m-1}}}


\newtheorem{thm}{\protect\theoremname}
  \theoremstyle{plain}
  \newtheorem{lem}[thm]{\protect\lemmaname}
  \theoremstyle{remark}
  \newtheorem{rem}[thm]{\protect\remarkname}
  \theoremstyle{plain}
  \newtheorem{prop}[thm]{\protect\propositionname}


  \providecommand{\lemmaname}{Lemma}
  \providecommand{\propositionname}{Proposition}
  \providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}


%\hypersetup{pdftitle={FreeON Project Report 1}}
%\hypersetup{pdfauthor={Matt Challacombe and Nicolas Bock}}
%\hypersetup{pdfsubject={A SpAMM Stabilized Newton Schulz Preconditioner: Fighting Error with Error}}

%\bibstyle{aipnum4-1}

\begin{document}

\title{Stability and Algebraic Locality In Square Root Iteration}

\author{Matt Challacombe}
\email{matt.challacombe@freeon.org}
\homepage{http://www.freeon.org}

\author{Terry Haut}
\email{haut@lanl.gov}

\author{Nicolas Bock}
\email{nicolasbock@freeon.org}
\homepage{http://www.freeon.org}

\affiliation{Theoretical Division, Los Alamos National Laboratory}

\begin{abstract}
\end{abstract}

\maketitle
\section{Introduction}

In many areas of application, finite correlations lead to matrices
with decay properties.  By decay, we mean an approximate (perhaps
bounded \cite{}) inverse relationship between matrix elements and an
associated distance; this may be a simple inverse 
relationship between matrix elements and the Cartesian distance between
corresponding support functions, or it may involve a generalized distance, {\em
  e.g.}~a generalized measure between character strings.  In electronic
structure, correlations manifest in decay properties of the gap
shifted matrix sign function, as projector of the effective
Hamiltonian (Fig.~\ref{figure1}).  More broadly, matrix decay
properties may correspond to statistical matrices \cite{penrose1974,
  voit00, Anselin2003, Hardin2013, Krishtal2014}, including learned
correlations in a generalized, non-orthogonal metric \cite{}.  More
broadly still, problems with local, non-orthogonal support are often
solved with congruence transformations of the matrix inverse square
root \cite{Lowdin56, naidu11} or a related factorization
\cite{Krishtal2014}; these transformations correlate local support
with a representation independent form, {\em eg.}~of the eigenproblem.
Interestingly, the matrix sign function and the matrix inverse square
root function are related by Higham's identity:
\begin{equation}
\rm{sign} \left( \begin{bmatrix} 0 & \mat{s}      \\ \mat{I}       & 0\end{bmatrix} \right)  =
                 \begin{bmatrix} 0 & \mat{s}^{1/2} \\ \mat{s}^{-1/2} & 0\end{bmatrix}  .
\end{equation}
A complete overivew of matrix function theory and computation is given
in Higham's enjoyable reference \cite{Higham08}.

A well conditioned matrix $\mat{s}$ may often correspond to matrix
sign and inverse square root functions with rapid exponential decay,
and be amenable to the sparse matrix approximation
$\bar{\mat{s}} = \mat{s}+ \mat{\epsilon}^{\mat{s}}_\tau$,
where $\mat{\epsilon}^{\mat{s}}_\tau$ is the error introduced
according to some criterion $\tau$.  Supporting this approximation are
useful bounds to matrix function elements \cite{Benzi99b, }.
The criterion $\tau$ might be a drop-tolerence,
$\epsilon^{\mat{s}}_{\tau} = \{-s_{ij}*\hat{\mat{e}}_i \, | \, |s_{ij}|<\tau \}$,
a radial cutoff,
$\epsilon^{\mat{s}}_{\tau} = \{-s_{ij}*\hat{\mat{e}}_i \, | \, \lVert \mat{r}_i - \mat{r}_j \rVert > \tau \}$,
or some other approach to truncation, perhaps involving a sparsity
pattern chosen {\em a priori}.
Then, conventional computational kernels may be employed, such as the
sparse general matrix-matrix multiply ($\tt{SpGEMM}$)
\cite{Gustavson78, Toledo97, challacombe00, bowler00}, yielding fast
solutions for multiplication rich iterations and a modulated {\bf
  (what do you mean with modulated?)} fill-in.
These and related incomplete/inexact approaches to the computation of
sparse approximate matrix functions often lead to ${\cal O}(n)$
algorithms, finding wide use in technologically important
preconditioning schemes, the information sciences, electronic
structure and many other disciplines. Comprehensive surveys of these
methods in the numerical linear algebra are given by Benzi
\cite{Benzi99,Benzi02}, and by Bowler \cite{Bowler12} and Benzi \cite{Benzi13}
for electronic structure.

\begin{figure}[t]\label{figure1}
 \includegraphics[width=3.5in]{decay_picture.png}
  \caption{Examples from electronic structure of decay for the
    spectral projector (gap shifted sign function) with respect to
    local (atomic) support.  Shown is decay for systems with
    correlations that are short (insulating water), medium
    (semi-conducting 4,3 nanotube), and long (metalic 3,3 nanotube)
    ranged, from exponential (insulating) to algebraic (metallic). }
\end{figure}

Because the truncated multiplication is controlled only by absolute,
additive errors in the product,
\begin{equation} \label{sparseapprox}
\overline{ \mat{a} \cdot \mat{b} }\; = \; \mat{a}\cdot\mat{b} \; +\; \mat{\epsilon}^{\mat{a}}_\tau \cdot \mat{b} \;+\;
 \mat{a} \cdot \mat{\epsilon}^{\mat{b}}_\tau  \; + \;   {\mathcal O}(\tau^2)
\end{equation}
achieving sparse, stable and rapidly convergent iteration for
ill-conditioned problems can be challenging \cite{}.  In cases of
extreme degeneracy, hierarchical semi-seperable (reduced rank)
algorithms can offer effective complexity reduction \cite{}.  However,
many practical cases are somewhere in-between sparse and meaningfully
degenerate regimes; effectively dense but without an exploitable
reduction in rank.  This is the case in electronic structure for
strong but non-metallic correlation, {\em e.g.}~towards the Mott
transition \cite{}, and also in the case of local atomic support
towards completeness \cite{Others, Hutter, Gigi}.

\pagebreak

\section{Sparse Approximate Matrix Multiplication}

In this contribution, we consider an $N$-body approach to the
approximation of matrix functions with decay, based on the quadtree
data structure \cite{wise, samet}
\begin{equation}
\mat{a}^i = \begin{bmatrix} \,  \mat{a}^{i+1}_{00} \, & \,  \mat{a}^{i+1}_{01} \,  \\[0.2cm]  \, \mat{a}^{i+1}_{10} \,  & \,\mat{a}^{i+1}_{11} \, \end{bmatrix} \, ,
\end{equation}
and orderings that are locality preserving \cite{}.  Orderings that
preserve data locality are well developed in the database theory
\cite{}, providing fast spatial and metric queries.  Locality
enabled, fast data access is central to the $N$-Body approximation
\cite{}, and an important problem for enterprise \cite{} and runtime
systems \cite{}, with memory hierarchies becoming increasingly
asynchronous and decentralized \cite{cache}.  For matrices with
decay, orderings that preserve locality lead to blocked-by-magnitude
matrix structures with well segregated neighborhoods, inhabited by
matrix elements of like size, and efficiently resolved by the quadtree
data structure \cite{}.


\subsection{A Bounded Occlusion and Cull }

$\tt SpAMM$ has evolved from a row-coloumn oriented skipout \cite{} to recursive occlusion and culling \cite{}
based on sub-multiplicative norms $\lVert \cdot \rVert \equiv \lVert \cdot \rVert_F$ and the 
Cauchy-Schwarz inequality,  $\lVert \mat{a}\cdot \mat{b} \rVert < \lVert \mat{a} \rVert \lVert \mat{b} \rVert$ \cite{kahan}.
Occlusion involves avoiding work, whilst culling is the collecting of tasks. 
Here, we ammend the previous (n\"{a}ive) occlusion and cull with the following recursion:
\begin{widetext}
\begin{equation}
\mat{a}^{i} \ot \mat{b}^{i} =
\left\{
        \begin{array}{ll}
                 \emptyset \quad \tt{if}\quad \lVert \mat{a}^i \rVert \lVert \mat{b}^i \rVert < \tau \lVert a \rVert \lVert b \rVert \\[0.2cm]
                 \mat{a} ^i \cdot \mat{b}^i \quad  \tt{if}(i=\tt{leaf}) \\[0.2cm]
\begin{bmatrix} \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{00} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{10} \; , \; &
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11}  \\[0.2cm]
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11} \; , \; &
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11}
\end{bmatrix}  \quad \tt{else}
                \end{array}
              \right.  \, ,
\end{equation}
\end{widetext}
which bounds the relative occlusion error,  
\begin{equation}\label{bound}
\frac{\lVert \mat{\Delta}^{a \cdot b}_{\tau} \rVert}{N^2 }  \, \leq \, \tau \, \lVert \mat{a} \rVert  \,  \lVert \mat{b} \rVert \, ,
\end{equation}
occuring in the approximate product:
\begin{equation}
\widetilde{\mat{a}\cdot \mat{b}} \,  \equiv \, \mat{a} \ot \mat{b} \,
  = \, \mat{a} \cdot \mat{b} + \mat{\Delta}^{a \cdot b}_{\tau} \,.
\end{equation}

\subsection{Proof}

We now prove (\ref{bound}).

\begin{prop}
\label{lem:SpAMM mult, prop}
Let $\tau_{A,B} = \tau \| A \| \| B \| $. Then for each $i,j$,

\[
\left|\left(A\otimes_{\tau}B\right)_{ij}-\left(A\cdot B\right)_{ij}\right|\leq n\tau_{A,B},
\]
and
\[
\left\Vert A\otimes_{\tau}B-A\cdot B\right\Vert _{F}\leq n^{2}\tau_{A,B}.
\]
\end{prop}



\begin{proof}


We first show the following technical result: it is possible to choose $\alpha_{lij}\in\left\{ 0,1\right\} $ such that 

\begin{equation}
\left(A\otimes_{\tau}B\right)_{ij}=\sum_{l=1}^{n}A_{il}B_{lj}\alpha_{lij},\label{eq:spamm form, lemma}
\end{equation}
In addition, if $\alpha_{lij}=0$, then \textup{$\left|A_{il}\right|\left|B_{lj}\right|<\tau_{A,B}$. }. To show this, we use 
induction on the number $k_{\max}$ of levels. 

First, if $k_{\max}=0$,
\[
A\otimes_{\tau}B=\begin{cases}
0 & \,\,\text{if}\,\,\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}<\tau_{A,B},\\
A\cdot B & \,\,\text{else}.
\end{cases}
\]
Therefore, $A\otimes_{\tau}B$ is of the form (\ref{eq:spamm form, lemma})
with either all $\alpha_{lij}=0$ or all $\alpha_{lij}=1$. Moreover,
if $\alpha_{lij}=0$, then $\left|A_{il}\right|\left|B_{lj}\right|\leq\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}<\tau_{A,B}$. 

Now assume that the claim holds for $k_{\max}-1$. We show that it
holds for $k_{\max}$. Indeed, if $\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}<\tau_{A,B}$,
we have that $A\otimes_{\tau}B=0$, which is of the form (\ref{eq:spamm form, lemma})
with all $\alpha_{lij}=0$. Also, if $\alpha_{lij}=0$, then $\left|A_{il}\right|\left|B_{lj}\right|<\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}<\tau_{A,B}$.

Now assume that $\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}\geq\tau_{A,B}$.
Then
\[
A\otimes_{\tau}B=\left(\begin{array}{cc}
A_{11}\otimes_{\tau}B_{11}+A_{12}\otimes_{\tau}B_{21} & A_{11}\otimes_{\tau}B_{12}+A_{12}\otimes_{\tau}B_{22}\\
A_{21}\otimes_{\tau}B_{11}+A_{22}\otimes_{\tau}B_{21} & A_{21}\otimes_{\tau}B_{21}+A_{22}\otimes_{\tau}B_{22}
\end{array}\right).
\]
We need to consider four cases: $i\leq n/2$ and $j\leq n/2$, $i>n/2$
and $j>n/2$, $i>n/2$ and $j\leq n/2$, and, finally, $i>n/2$ and
$j>n/2$. Since the analysis is similar for all four cases, we only
consider $i\leq n/2$ and $j\leq n/2$. We have that 
\begin{eqnarray*}
\left(A\otimes_{\tau}B\right)_{ij} & = & \left(A_{11}\otimes_{\tau}B_{11}+A_{12}\otimes_{\tau}B_{21}\right)_{ij}\\
 & = & \sum_{l=1}^{n/2}\left(A_{11}\right)_{il}\left(B_{11}\right)_{lj}\alpha_{lij}^{(1)} + \\
 & & \,\,\,\,\,\,\,\, \sum_{l=1}^{n/2}\left(A_{12}\right)_{il}\left(B_{21}\right)_{lj}\alpha_{lij}^{(2)}\\
 & = & \sum_{l=1}^{n}A_{il}B_{lj}\alpha_{lij},
\end{eqnarray*}
where we used the induction hypothesis in the second equality.

Now suppose that $\alpha_{lij}=0$ for some $l$. Then $\tilde{\alpha}_{lij}^{(1)}=0$
if $l\leq n/2$ or $\tilde{\alpha}_{l-n/2,ij}^{(2)}=0$ $l>n/2$.
If, e.g., $\tilde{\alpha}_{l-n/2,ij}^{(2)}=0$, then $\left|A_{il}\right|\left|B_{lj}\right|=\left|\left(A_{12}\right)_{i,l-n/2}\right|\left|\left(B_{21}\right)_{l-n/2,j}\right|<\tau_{A,B}$,
where we used the induction hypothesis in the final inequality. The
analysis for $l\leq n/2$ is similar, and the claim
follows.

We can now finish the proof of Proposition~\ref{lem:SpAMM mult, prop}. Indeed, by (\ref{eq:spamm form, lemma}),
\begin{eqnarray*}
\left|\left(A\otimes_{\tau}B\right)_{ij}-\left(A\cdot B\right)_{ij}\right| & \leq & \sum_{l=1}^{n}\left|A_{il}B_{lj}\right|\left|\alpha_{lij}-1\right|\\
 & = & \sum_{\alpha_{lij}=0}\left|A_{il}B_{lj}\right|.
\end{eqnarray*}
In addition, if $\alpha_{lij}=0$, then $\left|A_{il}B_{lj}\right|<\tau_{A,B}$
and the lemma follows.


\end{proof}

\subsection{Related Research} 

$\tt SpAMM$ is perhaps most closely related to the Strassen-like branch of fast matrix multiplication 
\cite{springerlink:10.1007/BF02165411,Ballard2014}.
In the Strassen-like approach, disjoint volumes in (abstract) tensor intermediates are omitted recursively \cite{}.  
In the $\tt SpAMM$ approach to fast multixplication, the numerically most significant volumes in 
 n\"{a}ive ($ijk$) tensor intermediates are culled, with error bounded by Eq.~(\ref{bound}).  
This bound makes $\ot$ a {\em stable} form of fast multiplication,  as explained by Demmel, Dumitriu and Holz (DDH; Ref.~\cite{Demmel07}).

$\tt SpAMM$ is a $n$-body method for fast matrix multiplication, related to the 
generalized methods popularized by Grey \cite{Gray2001,Gray2003}. In our development,
generalization reflects the {\em genericity} \cite{} of recursive data access \cite{}, 
enabling range querries, metric querries, higher dimensional querries and so on, with common frameworks, 
structures and runtimes.  Recently for example, we generalized the double (left-right) metric querry in 
Eq.~(\ref{}) to hextree metric querries of the exact Fock exchange \cite{}.  

Also, $n$-body methods offer well established protocols for turning spatial and metric locality into 
data and temporal locality \cite{}; recently, x, y  and Yellik showed perfect strong scaling and communication optimality 
for pairwise $n$-body methods \cite{Yellik}.   Bridging the gap between 
$n$-body solver and fast matrix multiplication, we recently demonstrated strong scaling for fast matrix multiplication ($\tt SpAMM$) \cite{}.  
The introduction of algebraic and strong Euclidean locality in this work may further enhance high performance implementaitons. 

%As noted by Aluru \cite{}, the top-down $n$-body model and breadth-first map-reduction are equivalent \cite{}, offering
%the potential febraic and strong Euclidean locality developed in this work may further enhance high performance implementaiton. 
%or alignment with emergent enterprise frameworks \cite{} and functional programming 
%languages that support generacity \cite{}. Language suppo
%with very simple (skeletonized) frameworks, lowering barriers to entry, enhancing performance towards decentralized memory landscapes, 
%and following a sustainable commodity trend \cite{softwaresustainanbilty} that offers 
%increasingly cheap compute cycles over the next few decades \cite{}. 

This work offers a data local alternative to fast non-deterministic methods for sampling the product, 
which include sketching \cite{Sarlos2006,Drineas2006,Mahoney2012,Pagh2013,Sivertsen2014,Woodruff2015},
joining \cite{Mishra92,Hoel94,Jacox03,Chen07,Amossen09,Lieberman08,Kim09}, 
sensing \cite{} and probing \cite{}.  These  methods involve a wheighted (probablistic) 
and on the fly sampling, with the potential for complexity reduction in the case of random distributions. 
$\tt SpAMM$ also employs on the fly wheighted sampling,  but with 
compresion through locality, brought about by algebraic correlations (towards identity) and also in the
metric structure, through strong Euclidean locality.

Methods that achieve compression in the stream of product intermediates are different from reduced rank algorithms that achieve 
matrix compression in a step that preceeds multiplication (seperability)  \cite{}.   However, matrix compressions are 
generally compatible with the quadtree, as are additional fast (generalized) solvers that add complex functionality ({\em e.g.} 
in electronic structure theory \cite{}).  Thus, generality 
and interoperability may enable deeply layered, thin and generic solvers easy access to in place data. 
Further, language support may provide simple (skeletonized) frameworks for generic recursion, 
offering opportunities to greatly simplify codebase at the systems level, 
lowering barriers to entry and enhancing concurence (in the Erlang sense). 


Finally, previous work on the scaled NS iteration has heavily influenced this work.  Formost is 
Higham, Mackey, Mackey and T (HMMT; Ref.~\cite{}) masterwork on convergence of NS iteration under all groups,
wherein HMMT also develop Fr\"{e}chet analyses for stable square root iteration at the fixed point.
Also, important inspiration comes from Chen and Chow's \cite{} approach to scaled NS iteration for ill-conditioned problems \cite{}, 
and from the Helgaker groups work on NS iteration, whose notation we follow in part \cite{}.  

% which leads to a non-associative algebra and error flows with
% properties of the Lie bracket
% \begin{equation} \label{braket}
% \widetilde{\left[ \mat{a} , \mat{b} \right]} \equiv \mat{a} \ot \mat{b}-\mat{b} \ot \mat{a}
% =  \left[ \mat{a} , \mat{b} \right]
% + \mat{\Delta}^{a\cdot b}_{\tau} -\mat{\Delta}^{b\cdot a}_{\tau} \,.
% \end{equation}

% Finally, the

\section{First Order Newton-Shulz Iteration}

There are two common, first order NS iterations; the sign iteration
and the square root iteration, related by the square, $\mat{I}\left(
\cdot \right)= {\rm sign}^2\left( \cdot \right) $.  These equivalent
iterations converge linearly at first, then enter a basin of stability
marked by super-linear convergence.  Our interest is to access this
basin with the most permissive $\tau$ possible, building a foundation
for future refinement
%, {\em e.g.}
at a reduced cost and with a higher precision ($\tau \rightarrow 0$)
\cite{MChallacombe16}.

\subsection{Sign iteration}

For the NS sign iteration, this basin is marked by a behavioral change
in the difference $\delta \mat{X}_k = \widetilde{\mat{X}}_k -\mat{X}_k
= {\rm sign} \left(\mat{X}_{k-1}+\delta \mat{X}_{k-1} \right) -{\rm
  sign} \left(\mat{X}_{k-1} \right)$, where $\delta \mat{X}_{k-1}$ is
some previous error.  The change in behavior is associated with the
onset of idempotence and the bounded eigenvalues of ${\rm sign}'\left(
\cdot \right)$, leading to stable iteration when ${\rm sign}'\left(
\mat{X}_{k-1} \right) \delta \mat{X}_{k-1} < 1 $.  Global perturbative
bounds on this iteration have been derived by Bai and Demmel
\cite{Bai98usingthe}, while Byers, He and Mehrmann \cite{} developed
asymptotic bounds.  The automatic stability of sign iteration is a
well developed theme in Ref.\cite{Higham08}.

\subsection{Square root iteration}

In this work, we are concerned with resolution of the identity \cite{}
\begin{equation}
\mat{I} \left( \mat{s} \right) =\mat{s}^{1/2} \cdot \mat{s}^{-1/2} \, ,
\end{equation}
and the cooresponding canonical (dual) square root iteration \cite{};
\begin{eqnarray}\label{cannonical}
\mat{y}_k &\leftarrow& h_\alpha \left[ \mat{y}_{k-1} \cdot \mat{z}_{k-1} \right] \cdot \mat{y}_{k-1}  \nonumber \\
\mat{z}_k &\leftarrow& \mat{z}_{k-1} \cdot h_\alpha \left[ \mat{y}_{k-1} \cdot \mat{z}_{k-1} \right] \; ,
\end{eqnarray}
with eigenvalues in the proper domain aggregated towards 0 or 1 by the
NS map $h_\alpha[\mat{x}]=\frac{\sqrt{\alpha}}{2} \left(3-\alpha
\mat{x} \right)$ \cite{}.  Then, starting with $\mat{z}_0=\mat{I}$ and


 $\mat{s} \leftarrow \mat{s}/s_{N-1}$, 

$\mat{x}_0=\mat{y}_0=\mat{s}$, ${\mat{y}}_k \rightarrow
\mat{s}^{1/2}$, ${\mat{z}}_k \rightarrow \mat{s}^{-1/2}$ and
${\mat{x}}_k \rightarrow {\mat{I}}$.  As in the case of sign
iteration, this dual iteration was shown by Higham, Mackey, Mackey and
Tisseur \cite{Higham2005} to remain bounded in the superlinear regime,
by idempotent Frechet derivatives about the fixed point
$\left(\mat{s}^{1/2},\mat{s}^{-1/2}\right)$, in the direction $\left(
\delta \mat{y}_{k-1} , \delta \mat{z}_{k-1} \right)$:
\begin{eqnarray}
\delta \mat{y}_k &=& \frac{1}{2} \delta \mat{y}_{k-1} - \frac{1}{2} \mat{s}^{1/2} \cdot \delta \mat{z}_{k-1} \cdot \mat{s}^{1/2} \\
\delta \mat{z}_k &=& \frac{1}{2} \delta \mat{z}_{k-1} - \frac{1}{2} \mat{s}^{-1/2} \cdot \delta \mat{y}_{k-1} \cdot \mat{s}^{-1/2} \;.
\end{eqnarray}
In this contribution, we consider another aspect of convergence,
namely the (hopefully) linear approach towards stability of the
iteration
\begin{equation}
\widetilde{\mat{x}}_k \leftarrow
 \widetilde{\mat{y}}_k \left( \widetilde{\mat{x}}_{k-1} \right)
\, \ot \, \widetilde{\mat{z}}_k \left( \widetilde{\mat{x}}_{k-1} \right) \, ,
\end{equation}
made difficult by ill-conditioning and a sketchy $\ot$.

\subsection{the NS map}

Initially, $h'_\alpha$ at the smallest eigenvalue $x_0$ controls the
rate of progress towards idempotence.  As recently shown by Jie and
Chen \cite{Chen2014}, for very ill-conditioned problems, a factor of
two reduction in the number of NS steps can be achieved by choosing
$\alpha \sim 2.85$, which is at the edge of stability.  As argued by
Pan and Schreiber \cite{Pan1991}, Jie and Chen \cite{Chen2014},
switching or damping the scaling factor towards $\alpha=1$ at
convergence is important, shifting emphasis away from the behavior of
$x_0$ towards {\em e.g.}~$x_i \in [0.01,1]$, emphasizing overall
convergence of the broad distribution \cite{Pan and Scriber}.  In an
approximate algebra like $\tt SpAMM$, the potential for eigenvalues to
fluctuate out of the domain of convergence must be considered.  This
is addressed in Section \ref{}.

\subsection{Ill-conditioning, Stability and Implementation}

There are a number of nominally equivalent instances of the square root iteration, related by commutations and transpositions. 
However, these instances may have very different stability properties,  controled to first order by the Frechet derivatives
\begin{equation}
  \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}
= \lim_{\tau \rightarrow 0} \slantfrac{ \mat{x} \left( \mat{y}_{k-1} + \tau \delta \widehat{\mat{y}}_{k-1} ,  {\mat{z}}_{k-1}  \right)
                                     -\mat{x}_k    }{\tau} 
 \end{equation}
and
 \begin{equation}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} = \lim_{\tau \rightarrow 0}
\slantfrac{ \mat{x} \left( {\mat{y}}_{k-1} , \mat{z}_{k-1} +\tau  \delta \widehat{\mat{z}}_{k-1} \right) - \mat{x}_k   }{\tau}  \, , 
 \end{equation}
along the unit directions of the previous errors $\delta \widehat{\mat{y}}_{k-1}$ and $\delta \widehat{\mat{z}}_{k-1}$, corresponding 
to the associated displacement magnitudes $\delta y_{k-1} = \lVert \delta \mat{y}_{k-1} \rVert$  and  $\delta z_{k-1}=\lVert \delta \mat{z}_{k-1} \rVert$.
Then, the differential 
\begin{equation} \label{firstorderdual}
\delta \mat{x}_k = \,  { \mat{x}}_{\delta \widehat{\mat{y}}_{k-1}}  \, {\scriptstyle \times} \, \delta y_{k-1}
                 \, + \,  { \mat{x}}_{\delta \widehat{\mat{z}}_{k-1}}  \, {\scriptstyle \times} \, \delta z_{k-1}  \, + {\cal O}(\tau^2) 
\end{equation}
determines the total first order stability. 

This formulation allows to consider orientational effects involving eigenvector fidelity and convergence of 
derivatives towards zero seperately from displacement effects involving accumulation and $\tt SpAMM$ source errors.
In some cases, instabilities may be associated with derivatives that do not vanish towards identity, yeilding an unbounded iteration \cite{}.
In other instances, an instability may be associated with rapidly increasing displacements, due to a too large $\tau$.  Instability 
may also arize due to the numerical corruption of the eigenvectors, also resulting in derivatives that vanish too slowly (or blow up altogether).    

The potential for
instability is increased with ill-conditioning through the terms $\lVert \mat{z}_{k} \rVert  \rightarrow \sqrt{\kappa\left(\mat{s} \right)}$.
Also for ill-conditioned systems, scaling is nessesary to accelerate convergence.  However with scaling, increasing the map 
derivative $h'_\alpha$ can also further enhance the rate of error accumulation. 

In following sections, we'll examine how these effects differ from the ideal (double precision) cannonical (dual) square root iteration
for ill-conditioned systems and in the strongly non-associative, sketchy $\ot$ regime corresponding to permisive values of $\tau$.
At this early stage, we are interested in hazzards and opportunities associated with
different formulations and implementational details.   In addition to deviations from the full precision dual instance,
we will develop the  ``stabilized'' instance,
\begin{eqnarray}\label{stabilized}
\mat{z}_k &\leftarrow& \mat{z}_{k-1} \cdot h_\alpha \left[ \mat{x}_{k-1} \right] \; , \nonumber \\
\mat{x}_k &\leftarrow&  \mat{z}^\dagger_{k} \cdot \mat{s} \cdot \mat{z}_{k-1} \; ,
\end{eqnarray}
with the corresponding differential;
\begin{equation} \label{firstorderstab}
\delta \mat{x}_k = \,  { \mat{x}}_{\delta \widehat{\mat{z}}_{k-1}}  \, {\scriptstyle \times} \, \delta z_{k-1}  \, + {\cal O}(\tau^2)  \, .
\end{equation}

Nominally, $\mat{y}^{\rm dual}$ is equivalent to $\mat{y}^{\rm stab}_k \equiv \mat{z}^\dagger_{k} \cdot \mat{s}$ 
is also equivalent to $\mat{y}^{\rm naive}_k \equiv \mat{z}_{k} \cdot \mat{s}$.
However, with ill-conditioning and in only double precision, these two instances may diverge due to non-associative errors that rapidly compound.
In the case of the duals iteration under $\tt SpAMM$ approximation, the $\widetilde{\mat{y}}^{\rm dual}_k$ channel does not retain contact 
with the eigenvectors, span $\mat{s}$, whilst the stab instance does.  
In the duals iteration, the $\widetilde{\mat{y}}_k$  $\tt SpAMM$ update 
is mild, with errors in the relative product remaining well conditioned.  
In the stab instance, conection with $\mat{s}$ is retained at each step, but at the price of the 
$\mat{y}^{\rm stab}_k$ update involving magnitudes that vary widely in the $\tt SpAMM$ product.     


\section{Implementation}

\subsection{programming}

FP, F08, OpenMP 4.0
In the current implementation, all persistence data
(norms, flops, branches \& {\em etc.}) are accumulated compactly in the backward recurrence.  This persistence data
 that may be achieved by minimal locally essential trees \cite{}.

\subsection{scaling}

\subsection{stabilization}

For these reasons, maintaining connection to the eigenvectors of $\mat{s}$ through 
a tighter first product is nessesary.  In the stab instance, and with a 
tighter ``{\em s}'' product, $\tau_s \ll \tau$, we find very interesting left/right differences; 
namely, the right first product 
\begin{equation} 
\widetilde{\mat{x}}^R_k \leftarrow \widetilde{\mat{z}}^\dagger_{k} \, \ot  \, \left( \mat{s} \,  \ots \, \widetilde{\mat{z}}_{k-1}  \right) \; ,
\end{equation}
is  different from the left first product 
\begin{equation} 
\widetilde{\mat{x}}^L_k \leftarrow \left(  \widetilde{\mat{z}}^\dagger_{k} \, \ots \, \mat{s} \right) \,  \ot  \, \widetilde{\mat{z}}_{k-1} \; .
\end{equation}

\subsection{regularization}
damping the inversion and the small value to be added c is called Marquardt-Levenberg coefficient

\subsection{convergence}

Map switching and etc based on TrX

\section{Data} \label{data}

\subsubsection{double exponential ill-conditioning}
3,3 carbon nanotube with diffuse $sp$-function
double exponential (Fig.)

\subsubsection{three-dimensional, periodic}
%\subsubsection{water boxes}
%\subsubsection{ordering}
\subsubsection{Matrix Market}

\section{Error Flows in Square Root Iteration}

\subsection{The cannonical (dual) instance}
 
Refering back to Eq.~( \ref{firstorderdual} ), we develop the Fr\'{e}chet analyses \cite{} with the goal of understanding
the contractive approach to identity in competition with error accumulations and $\tt SpAMM$ sources.
Of interest are the derivatives 

% \begin{equation}
%   \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}
% =   \mat{y}_{\delta \widehat{ \mat{y}}_{k-1}} \cdot \mat{z}_{k}  + \mat{y}_{k}  \cdot \mat{z}_{\delta \widehat{ \mat{y}}_{k-1}}
%  \end{equation}
% and
%  \begin{equation}
%  \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} =
% \mat{y}_{\delta \widehat{ \mat{z}}_{k-1}} \cdot \mat{z}_{k}  + \mat{y}_{k}  \cdot \mat{z}_{\delta \widehat{ \mat{z}}_{k-1}} \, .
%  \end{equation}


% For $\mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}$ we have
% \begin{multline}
%  \mat{y}_{\delta \widehat{ \mat{y}}_{k-1}} = h_\alpha \left[ \mat{x}_{k-1} \right] \cdot \delta \widehat{\mat{y}}_{k-1} \\
%                                      + h'_\alpha \cdot \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1} \cdot \mat{y}_{k-1}
% \end{multline}
% and
% \begin{equation}
%  \mat{z}_{\delta \widehat{ \mat{y}}_{k-1}} =  \mat{z}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1}
% \end{equation}
% yeilding
% Also, for $\mat{x}_{\delta \widehat{ \mat{z}}_{k-1}}$ we have
% \begin{equation}
%  \mat{y}_{\delta \widehat{ \mat{z}}_{k-1}} =  {\mat{y}}_{k-1} \cdot  h'_\alpha \delta \widehat{ \mat{z}}_{k-1} \cdot  \mat{y}_{k-1}
% \end{equation}
% and
% \begin{multline}
%  \mat{z}_{\delta \widehat{ \mat{z}}_{k-1}} = \delta \widehat{\mat{z}}_{k-1} \cdot   h_\alpha \left[ \mat{x}_{k-1} \right] \\
%                                      + \mat{z}_{k-1} \cdot {\mat{y}}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{z}}_{k-1}  \; ,
% \end{multline}
% yeilding


\begin{multline}
  \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}} = h_\alpha \left[ \mat{x}_{k-1} \right]  \cdot \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k} \\
+  h'_\alpha  \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1} \cdot  \mat{y}_{k-1} \cdot  \mat{z}_{k} \\
 + \mat{y}_{k} \cdot \mat{z}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1}  \, .
\end{multline}


\begin{multline}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} =  {\mat{y}}_{k-1} \cdot  h'_\alpha \delta \widehat{ \mat{z}}_{k-1} \cdot  \mat{y}_{k-1}  \cdot \mat{z}_{k} \\
\qquad + \mat{y}_k \cdot  \delta \widehat{\mat{z}}_{k-1} \cdot   h_\alpha \left[ \mat{x}_{k-1} \right] \\
\qquad \qquad +  \mat{y}_{k} \cdot  \mat{z}_{k-1} \cdot {\mat{y}}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{z}}_{k-1} \, .
\end{multline}

Closer to a fixed point orbit,  $\mat{y}_k \cdot \mat{z}_{k-1} \rightarrow \mat{I}$, $\mat{y}_{k-1} \cdot \mat{z}_{k} \rightarrow \mat{I}$,
$h_\alpha \left[ \mat{x}_{k} \right] \rightarrow \mat{I}$ and $h'_\alpha \rightarrow - \frac{1}{2}$ \cite{higham2005}.  Then,

\begin{equation} \label{yorbit}
 \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}} \rightarrow \delta \widehat{\mat{y}}_{k-1} \cdot \left( \mat{z}_k-\mat{z}_{k-1} \right)
\end{equation}
and
\begin{equation} \label{zorbit}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} \rightarrow \left( \mat{y}_k-\mat{y}_{k-1} \right) \cdot \delta \widehat{\mat{z}}_{k-1} .
\end{equation}
Thus, contributions along $\delta \widehat{\mat{y}}_{k-1}$ and $\delta \widehat{\mat{z}}_{k-1}$ are tightly shut down in the 
region of superlinear convergence.  Achieving a contractive fixed point orbit, however requires that the three terms in Eq.~(\ref{}),  
with potentially different error accumulations and $\tt SpAMM$ sources, must cancel faster than $\delta y_{k-1}$ 
and $\delta z_{k-1}$ accumulate.

In this analysis, we've seperated the directional component of the error from its distance, because in addition to the
previous compounding error, each displacement contains also a first order $\tt SpAMM$ source error.  Its simpler to 
consider these effects serpately, at least in this first contribution. 

To understand $\delta \mat{z}_{k-1}$, we partially unwind the approxinate  $\widetilde{\mat{z}}_{k-1}$;
%\begin{equation}
\begin{eqnarray} \label{widetildez}
 \widetilde{\mat{z}}_{k-1} &=&  \widetilde{\mat{z}}_{k-2}  \, \ot \, h_\alpha[\widetilde{\mat{x}}_{k-2}]\\
&=& \Delta^{\widetilde{\mat{z}}_{k-2} \cdot h_\alpha \left[ \widetilde{\mat{x}}_{k-2}\right]}_\tau
+ \widetilde{\mat{z}}_{k-2} \cdot h_\alpha\left[ \widetilde{\mat{x}}_{k-2}\right]
%\end{equation}
\end{eqnarray}
Then, using
\begin{equation}
  h_\alpha \left[ \widetilde{\mat{x}}_{k-2} \right]
=  h_\alpha \left[ \mat{x}_{k-2} \right] +  h'_\alpha  \delta \mat{x}_{k-2} \, 
\end{equation}
and taking $\mat{z}_{k-1}$ from both sides, we find  
%Eq.~(\ref{widetildez}) yeilds 
\begin{multline}
 \delta {\mat{z}}_{k-1} =\Delta^{\widetilde{\mat{z}}_{k-2} \cdot h_\alpha \left[ \widetilde{\mat{x}}_{k-2}\right]}_\tau
\\ +\delta \mat{z}_{k-2} \cdot h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]
+ \mat{z}_{k-2} \cdot h'_\alpha \delta \mat{x}_{k-2}  \, ,
\end{multline}
bounded by 
% \begin{multline}
%  \delta {z}_{k-1} <
% \lVert \mat{z}_{k-2} \rVert \left( \;  \tau \, \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]  \rVert
% + h'_\alpha \delta {x}_{k-2} \right)  \\ +
% \delta {z}_{k-2}  \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2}  \right] \rVert 
% \end{multline}
 \begin{multline}
  \delta {z}_{k-1} <
 \lVert \mat{z}_{k-2} \rVert \left( \tau \, \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]  \rVert
 + h'_\alpha  \delta y_{k-2} \lVert z_{k-2} \rVert \right)  \\ 
 + \delta {z}_{k-2} \left( \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2}  \right] \rVert  + \lVert y_{k-2} \rVert \right) .
 \end{multline}


primary error channels contibuting to $\delta z_{k-1}$ are through the first order $\tt SpAMM$ error
$ \tau \lVert \mat{z}_{k-2} \rVert \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]\rVert$
and the volatile term $h'_\alpha  \delta y_{k-2} { \lVert \mat{z}_{k-2} \rVert }^2$.

%  \begin{eqnarray}
% \widetilde{\mat{y}}_{k-1}&=&
% h_\alpha [ \widetilde{\mat{x}}_{k-2}] \, \ots \, \widetilde{\mat{y}}_{k-2}  \\
% &=&  \mat{\Delta}^{  h_\alpha[ \widetilde{\mat{x}}_{k-2}] \cdot \widetilde{\mat{y}}_{k-2} }_{\tau_s}  \, + \,
%      h_\alpha[ \widetilde{\mat{x}}_{k-2}] \cdot \widetilde{\mat{y}}_{k-2}
% \end{eqnarray}
% \begin{multline}
% \delta \mat{y}_{k-1}= \mat{\Delta}^{h_\alpha [ \widetilde{\mat{x}}_{k-2}] \cdot \mat{y}_{k-2} }_\tau  \\ +
% h'_\alpha \delta \mat{x}_{k-2} \cdot \mat{y}_{k-2}
% + h_\alpha [ \widetilde{\mat{x}}_{k-2}] \cdot \delta \mat{y}_{k-2} \, ,
% \end{multline}

corresponding to  basis corruption and controlled by $\ots$, with $\tau_s \ll \tau$. 
As above, we can unwind this sensitive term, to find 
\begin{multline}
\delta y_{k-2} <   \lVert \mat{y}_{k-3} \rVert  \left( \tau_s \lVert h_\alpha [ \widetilde{\mat{x}}_{k-3}] \rVert + h'_\alpha \delta z_{k-3} \right )\\
+  \delta y_{k-3}  \left( \lVert \widetilde{\mat{z}}_{k-3}]  \rVert + \lVert h_\alpha [ \widetilde{\mat{x}}_{k-3}]  \rVert
  \right)
\, .
\end{multline}


\subsection{The stabilized  (stab) instance}

Here, we carry on from Eq.~(\ref{firstorderstab}) in the ``stabilized'' instance, with the single channel differential 
\begin{equation}
 \mat{x}_{\widehat{\mat{z}}_{k-1}} =   \mat{z}^\dagger_{\widehat{\mat{z}}_{k-1}} \cdot \mat{s} \cdot \mat{z}_{k} + 
                                  \mat{z}^\dagger_k \cdot \mat{s} \cdot \mat{z}_{\widehat{\mat{z}}_{k-1}} 
\end{equation}



\begin{multline}
\mat{z}_{\widehat{\mat{z}}_{k-1} } = \delta \widehat{\mat{z}}_{k-1} \cdot h_\alpha[ \widetilde{\mat{x}}_{k-1} ] + \mat{z}_{k-1} \cdot \left( \right. \\
 \left.  h'_\alpha \delta \widehat{\mat{z}}^\dagger_{k-1} \cdot \mat{s} \cdot \mat{z}_{k-1} +  \mat{z}^\dagger_{k-1} \cdot \mat{s} \cdot h'_\alpha \delta \widehat{\mat{z}}_{k-1}
 \right) 
\end{multline}

\begin{eqnarray}
\widetilde{\mat{y}}^{\rm stab}_{k-1} &=& \widetilde{\mat{z}}^{\dagger}_{k-1} \, \ot \, \mat{s} \\
                                  &=& \mat{\Delta}^{\widetilde{\mat{z}}^\dagger_{k-1} \cdot \, \mat{s}} \, + \,
\left( \widetilde{\mat{z}}_{k-2} \cdot h_\alpha[ \widetilde{\mat{x}}_{k-2}] \right)^\dagger \, \cdot \, \mat{s}
\end{eqnarray}

\subsection{Bifurcations}

\begin{figure}[h] \label{flow_noscale_dual}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_noscaling/33_nanotube_cond10_noscale_dual.eps}
\caption{Derivatives, displacements and the approximate trace of the unscaled, dual NS iteration for a (3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and $\tau_y=\{10^{-8}, 10^{-9}, 10^{-10}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}


% \begin{figure}[h]
% \includegraphics[width=3.5in]{fig_33_tube_cond_10_noscaling/33_nanotube_cond10_noscale_stab.eps}
% \caption{Derivatives, displacements and the approximate trace of the unscaled, stablized NS iteration for a (3,3) 
% nanotube with $\kappa =10^{10}$. 
% Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and 
% $\tau_y=\{10^{-4}, 10^{-5}, 10^{-6}$  are the dashed lines.  The trace expectation is shown as a full black line. }
% \end{figure}


\begin{figure}[h] \label{flow_scaled_dual}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_scaled/33_tube_k10_scale_dual.eps}
\caption{Derivatives, displacements and the approximate trace of the scaled, stablized NS iteration for a
(3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$,
$\tau=10^{-3}$ and $\tau_y=\{10^{-3},10^{-4},10^{-6}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}

\begin{figure}[h]\label{flow_scaled_stab}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_scaled/33_tube_k10_scale_stab.eps}
\caption{Derivatives, displacements and the approximate trace of the unscaled, dual NS iteration for a (3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and $\tau_y=\{10^{-8}, 10^{-9}, 10^{-10}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}

Differences in occlusion between stab and dual magnified as bounds for s.z not as tight as bounds for h.y.

 *lot* of overlap too (reproducing hilberts etc).


\section{TEMPERARY:NOTES FOR REGULARIZATION SECTION} 



The idea of preconditioning is that we can use a low tolerance $\tau_{0}$
(e.g. $\tau_{0}=10^{-2}$) to cheaply obtain an approximation $R_{0}\approx S^{-1/2}$.
Then since $S_{1}\equiv R_{0}SR_{0}$ is close to the identity matrix
$I$, 
\[
\left\Vert R_{0}SR_{0}-I\right\Vert _{F}\lesssim\tau_{0},
\]
we can use Newton Schulz on $S_{1}$ with a higher tolerance $\tau_{1}$
to get an accurate approximation $R_{1}=S_{1}^{-1/2}$ and using only
a few iterations:
\[
\left\Vert R_{1}S_{1}R_{1}-I\right\Vert _{F}\lesssim\tau_{1}.
\]
In particular, the matrix $S_{1}$, being close to the identity, is
better conditioned than $S$ and computing $S_{1}^{-1/2}$ requires
much fewer Newton Schulz iterations. Moreover, since 
\[
\left\Vert \left(R_{1}R_{0}\right)S\left(R_{0}R_{1}\right)-I\right\Vert _{F}\lesssim\tau_{1},
\]
we see that $R_{1}R_{0}$ is a $\tau_{1}$ approximation to $S^{-1/2}$.
Notice that, from the stability bound for SpAMM, we can replace all
of the exact matrix multiplications with SpAMM multiplications. 

To formalize this, let $S_{0}=S$, and suppose that $R_{j}$ is the
approximation to $S_{j}^{-1/2}$ obtained via the Newton Schulz iteration
with SpAMM tolerance $\tau_{j}$, so that
\[
\left\Vert R_{j}\otimes_{\tau_{j}}S_{j}\otimes_{\tau_{j}}R_{j}-I\right\Vert _{F}\lesssim\tau_{j}.
\]
Then define $S_{j+1}=R_{j}\otimes_{\tau_{j}}S_{j}\otimes_{\tau_{j}}R_{j}$
and let $R_{j+1}$ the approximation to $S_{j+1}^{-1/2}$ obtained
via the Newton Schulz iteration with SpAMM tolerance $\tau_{j+1}$,
so that 
\[
\left\Vert R_{j+1}\otimes_{\tau_{j+1}}S_{j+1}\otimes_{\tau_{j+1}}R_{j+1}-I\right\Vert _{F}\lesssim\tau_{j+1}.
\]
Then since $S_{j+1}=R_{j}\otimes_{\tau_{j}}S_{j}\otimes_{\tau_{j}}R_{j}$,
\[
\left\Vert R_{j+1}\otimes_{\tau_{j+1}}\left(R_{j}\otimes_{\tau_{j}}S_{j}\otimes_{\tau_{j}}R_{j}\right)\otimes_{\tau_{j+1}}R_{j+1}-I\right\Vert _{F}\lesssim\tau_{j+1}.
\]
In general, a defining 
\[
R_{\text{left}}\equiv R_{j+1}\otimes_{\tau_{j+1}}R_{j}\otimes_{\tau_{j}}R_{j-1}\dots\otimes_{\tau_{1}}R_{0},
\]
and 
\[
R_{\text{right}}\equiv R_{0}\otimes_{\tau_{1}}R_{1}\dots\otimes_{\tau_{j}}R_{j}\dots\otimes_{\tau_{j+1}}R_{j+1},
\]
it follows by induction that 
\[
\left\Vert R_{\text{left}}SR_{\text{right}}-I\right\Vert _{F}\lesssim\tau_{j+1}.
\]
Since (NOTE: NEED TO CHECK BUT THINK THIS IS TRIVIAL), 
\[
\left\Vert R_{\text{left}}R_{\text{right}}^{-1}-I\right\Vert _{F}\lesssim\tau_{j+1},
\]
we see that $R_{\text{left}}$ is a $\tau_{j+1}$ approximation to
$S^{-1/2}$. 

We can therefore write the following symbolic representation 
\[
S^{-1/2}=S_{\tau_{j+1}}^{-1/2}\otimes_{\tau_{j+1}}S_{\tau_{j}}^{-1/2}\otimes_{\tau_{j}}S_{\tau_{j-1}}^{-1/2}\dots\otimes_{\tau_{1}}S_{\tau_{0}}^{-1/2}+\mathcal{O}\left(\tau_{j+1}\right),
\]
where $S_{\tau_{k}}^{-1/2}$ is a $\tau_{k}$ approximation to the
inverse square root of $S_{k}=S_{\tau_{k-1}}^{-1/2}\otimes_{\tau_{k-1}}S_{k-1}\otimes_{\tau_{j}}S_{\tau_{k-1}}^{-1/2}$. 






\section{Iterated Regularization}\label{regularization}

Shown in the preceeding section, stabiliyty limits application of the NS square root iteration under 
agressive $\tt SpAMM$ approximation.  These limits can be circumvented through Tikhonov regularization \cite{}, 
involving a small level shift of eigenvalues,  $\mat{s}_\mu \leftarrow \mat{s}+\mu \mat{I}$, leading to a more 
well conditioned matrix with $\kappa( \mat{s}_\mu) = \frac{\sqrt{s^2_{N-1} + \mu^2}}{\sqrt{s^2_0+\mu^2}}$ \cite{}.  
However, achieving substantial acceleration with severe ill-conditioning  may require a large level shift, 
producing inverse factors of little practical use.  One approach to recover a more accurate inverse
factor is Riley's method \cite{}; 
\begin{equation}
\mat{s}^{-1/2} = \mat{s}^{-1/2}_{\mu} \cdot \left( \mat{I}+\frac{\mu}{2} \mat{s}^{-1}_{\mu}
                                                   +\frac{3 \mu^2}{8} \mat{s}^{-2}_{\mu} + \dots
   \right) \; ,
\end{equation}
but this is ineffective when $\mu$ is large, and invovles powers of the full inverse. 

Here, we outline an alternative,  nested product represenation of the inverse factor and show preliminary 
results for a first, most approximate solution.  This most approximate (but effective) solution is (ideally) representative 
of one order in the precision, $\tau_0\sim .1$, and corrective by one order in the condition, $\mu_0\sim .1$,
yeilding a thin, $0^{\rm th}$ preconditioner, $\mat{s}^{-1/2}_{\tau_0 \mu_0}$.  
This ``thin'' iteration may bring spectral resolution into alignment with norm magnitudes 
towards the resolvent $\mat{I}_{\tau_0\mu_0}\equiv \widetilde{\mat{I}}\left(\mat{s}_{\tau_0\mu_0}\right)$,
strengthening Eq.~(\ref{bound}).

Culled $\tt SpAMM$ volumes for this most approximate solution are shown with increasing system size in 
Fig.~\ref{regularized_stab} for the stable instance, and in Fig.~\ref{regularized_dual} for the dual instance, 
cooresponding to ``thin NS'' iteration for the (3,3) $\kappa(\mat{s})=10^{10}$ nanotube series.  
The behavior of these instances is very different; in the ``stable'' 
instance, a stable iteration could not be found at precision $\tau_0=.1$, even with $\mu_0=.1$ regularization.  Also, this stable iteration
sees a weakly convergent trace with inflating cull-volumes. On the other hand, volume of the dual iteration 
is strongly contracted with resolution of the identity.  
 
most-approximate-yet-effective-by-one-order (MAYEBOO) 

\begin{figure}[h]\label{regularized_stab}
 \includegraphics[width=3.5in]{fig_33_tube_cond_10_regularized/33_tube_k10_regularized_stab.eps}
\caption{
Culled volumes in the thin slice, stable instance approximation of $\mat{s}^{-1/2}_{\tau_0 \mu_0}$
for the (3,3) nanotube, $\kappa(\mat{s})=10^{10}$ matrix series 
described in Section \ref{data}.  In the ``stable'' instance, it was not possible to achieve stability with $\tau_0=.1$
In this ``stable'' case, a thin slice cooresponds to $\mu_0=.1, \tau_0=10^{-2} \;  \&  \; \tau_s=10^{-4}$, and volumes are
$\rm v_{\widetilde{\mat{z}_k}} = \left( {\rm vol}_{ \widetilde{\mat{z}}_{k-1}\ot h[\widetilde{\mat{x}}_{k-1}] } \right) \times 100\% /N^3$ and
${\rm v}_{\widetilde{\mat{y}}_k} = \left( {\rm vol}_{\mat{s}  \ots  \widetilde{\mat{z}}_{k}} \right) \times 100\% /N^3$.    
Line width increases with increasing system size. 
Also shown is the trace error, ${\rm e}_{k} = \left( N-{\rm tr}\,\mat{x}_k \right)/N$.}
\end{figure} 
\begin{figure}[h]\label{regularized_dual}
 \includegraphics[width=3.5in]{fig_33_tube_cond_10_regularized/33_tube_k10_regularized_dual.eps}
\caption{
Culled volumes in the thin slice, dual instance approximation of $\mat{s}^{-1/2}_{\tau_0 \mu_0}$
for the (3,3) nanotube, $\kappa(\mat{s})=10^{10}$ matrix series 
described in Section \ref{data}. The thin slice cooresponds to $\mu_0=.1, \tau_0=.1 \;  \&  \; \tau_s=.001$ 
with volumes 
${\rm v}_{\widetilde{\mat{y}}_k}= \left( {\rm vol}_{  h[\widetilde{\mat{x}}_{k-1}] \ots \widetilde{\mat{y}}_k }  \right) \times 100\% /N^3$ and  
${\rm v}_{\widetilde{\mat{z}}_k}= \left( {\rm vol}_{\widetilde{\mat{z}}_{k-1} \ot  h[\widetilde{\mat{x}}_{k-1}]} \right) \times 100\% /N^3$.
Line width increases with increasing system size. 
Also shown is the trace error, ${\rm e}_{k} = \left( N-{\rm tr}\,\mat{x}_k \right)/N$, which rapidly approaches $10^{-11}$ (not shown). }
\end{figure} 

These results reflect very different cull-spaces.  In the stable instance, the spectral resolution of powers is not compresive, and  
$\widetilde{\mat{y}}^{\rm stab}_k \rightarrow  \mat{s}^{-1/2}_{\tau_0 \mu_0} \oto \mat{s}_{\mu_0}$ is  poorly bound by Eq.~(\ref{bound}).
In the dual case however, $\widetilde{\mat{y}}^{\rm dual}_k \rightarrow  \mat{I}_{\tau_0 \mu_0} \oto \mat{s}^{1/2}_{\tau_0 \mu_0}$
and $\widetilde{\mat{z}}^{\rm dual}_k \rightarrow  \mat{s}^{-1/2}_{\tau_0 \mu_0} \oto \mat{I}_{\tau_0 \mu_0}$, 
with Eq.~(\ref{bound}) tightening to
\begin{equation}\label{boundY}
\Delta^{\mat{I}_{\tau_0 \mu_0} \cdot \mat{s}^{1/2}_{\tau_0 \mu_0}} <  \tau n \lVert \mat{s}^{1/2}_{\tau_0 \mu_0} \rVert
\end{equation}
and 
\begin{equation}\label{boundZ}
\Delta^{ \mat{s}^{-1/2}_{\tau_0 \mu_0}\cdot \mat{I}_{\tau_0 \mu_0}}  <  \tau n \lVert \mat{s}^{-1/2}_{\tau_0 \mu_0} \rVert \, ,
\end{equation}
as relative and absolute errors converge.  This tightening is compressive, leading to complexities that are quadtree copy in place.   

In the dual instance, the $\tt SpAMM$ approximation can be brought all the way to  $\tau_0 = .1$ in the case of $\mu_0 = .1$.
From this first slice  $\mat{s}^{-1/2}_{\tau_0, \mu_0}$ then, a next level shifted preconditioner can be found, 
$\mat{s}^{-1/2}_{\tau_0 \mu_1}$, based on the residual 
$\left(\mat{s}^{-1/2}_{\tau_0\mu_0} \right)^\dagger \, \oto \, \left(\mat{s}+\mu_1 \mat{I} \right)  \, \oto \,\mat{s}^{-1/2}_{\tau_0 \mu_0} $, with {\em e.g.} 
$\mu_1= .01$. It may then be possible to find the full ($\tt SpAMM$ most approximate) 
factor as the nested product of preconditioned thin slices;
\begin{equation}
\mat{s}^{-1/2}_{\tau_0} = \mat{s}^{-1/2}_{\tau_0 \mu_n} \, \oto \, \mat{s}^{-1/2}_{\tau_0 \mu_{n-1}} \, \oto \, \dots  \,  \mat{s}^{-1/2}_{\tau_0 \mu_0}
\end{equation}
In this way, 
iterative regularization can be used to find a product representation of the inverse square root at a $\tt SpAMM$ resolution 
potentially far more permisive than otherwise possible. 
Likewise, it may be possible to obtain the full factor with increasing $\tt SpAMM$ resolution in the product representation:
\begin{equation}
\mat{s}^{-1/2} = \mat{s}^{-1/2}_{\tau_m} \, \otm \,  \mat{s}^{-1/2}_{\tau_{m-1}} \, \otmm \dots \, \mat{s}^{-1/2}_{\tau_0}
\end{equation}taken over the sequence $1 > \tau_0 > \tau_1 > \dots > \tau_n $.  More generally we write 
\begin{equation} \label{spammsandwich}
\mat{s}^{-1/2} \equiv \bigotimes_{\substack{\tau=\tau_0 \\ \mu=\mu_0   } } {\left|\, \tau\, \mu \, ; \, \scriptstyle{\mat{s}^{-1/2}}  \right>}  \, ,
\end{equation}
acknowledging the potential for a flexible path between precision and regularization. The braket notation marks 
the potential for assymetries in the intermediate represenation.  

This thin product representation may have advantages: 
({\bf{1}}) Each thin solve involves a few generic and well behaved steps that may be narrowly optimized;
({\bf{2}}) Each thin solve can be brought rapidly into compressive identity iteration; 
({\bf{3}}) The {\tt SpAMM} bound is vastly strengthend, via Eqs.~(\ref{boundY}-\ref{boundZ}); 
({\bf{4}}) A new algebraic $n$-body form of locality is exploited; 
({\bf{5}}) The inverse factor can be applied incrementally;
({\bf{6}}) Slice update and application is ammenable to continous temporal partitioning based on {\em e.g.} persistence data.

%\begin{equation}
%\mat(I}\left(\mat{s}\right) = \bigotimes_{\substack{\tau=\tau_0 \\ \mu=\mu_0}}  \bigotimes_{\substack{\tau'=\tau_0 \\ \mu'=\mu_0}} 
%{_{\mat{s}^{1/2}} }{\left<\tau \mu \right|}
%{\left|\tau \mu \right>}_{\mat{s}^{-1/2}} 
%\end{equation}


\begin{figure*}[h] \label{Lensing1}
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/y_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/y_dual_4_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/y_dual_16_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/x_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/x_dual_2_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/x_dual_16_cant_x.png}} 
\caption{
The $ijk$ task and data space for construction of the MAYEBOO preconditioner 
$\left|\tau_0=.1,\mu_0=.1\, ; \,\scriptstyle{\mat{s}^{-1/2}} \right>$, with the 
dual instance of square root iteration  and for an 8$\times$ U.C. (3,3) $\kappa(s)=10^{11}$ nanotube.
$\mat{y}_k$ appears wider than $\mat{z}_k$ because it is computed at a higher precision, $\tau_s=.001$,
and because the first multiply involves $\mat{s}^2$.  At top its  $\mat{y}_k=h_\alpha[ \mat{x}_{k-1} ] \ots \mat{y}_{k-1}$
for $k=0,4,\& 16$, while on the bottom we have $\mat{x}_k=  \mat{y}_{k}  \ot \mat{z}_{k}$ for $k=0,2, \& 16$.
Maroon is $\mat{a}$, purple is $\mat{b}$, green is $\mat{c}$,  and black is the volume ${\rm vol}_{a \ot b}$
in the product $\mat{c}=\mat{a} \ot \mat{b}$.}
\end{figure*}

\begin{figure*}[h] \label{Lensing2}
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noscaling/y_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noscaling/y_dual_4_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noscaling/y_dual_15_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noscaling/x_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noscaling/x_dual_4_cant_x.png}} 
\fbox{ \includegraphics[width=5.2cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noscaling/x_dual_15_cant_x.png}} 
\caption{
The $ijk$ task and data space for construction of the MAYEBOO preconditioner 
$\left|\tau_0=.1,\mu_0=.1\, ; \,\scriptstyle{\mat{s}^{-1/2}} \right>$, with the 
dual instance of square root iteration  and for 6-311G** metric of 100 periodic water molecules
at STP.  At top its  $\mat{y}_k=h_\alpha[ \mat{x}_{k-1} ] \ots \mat{y}_{k-1}$
for $k=0,4,\& 15$, while on the bottom we have $\mat{x}_k=  \mat{y}_{k}  \ot \mat{z}_{k}$ for $k=0,4, \& 15$.
Maroon is $\mat{a}$, purple is $\mat{b}$, green is $\mat{c}$,  and black is the volume ${\rm vol}_{a \ot b}$
in the product $\mat{c}=\mat{a} \ot \mat{b}$.}
\end{figure*}


\section{Locality} 

A main component of the classical $n$-body algorithm involves spatial database methods that 
optimize the fast range querry over Cartesian data.  Often, these technologies are
based on locality preserving properties of a space filling curve \cite{}, which maps points that are
close in space to an index where they are also close.  The block-by-magnitude structuring that empowers 
the $\tt SpAMM$ approximation is a {\em metric locality} cooresponding to the underlying support. \\[.5in]

Also for the $n$-body method,  Warren and Salmon showed how to use previously accumulated histories 
(persistence, temporal locality) to rebalance and repartition a distributed octree task space, 
incrementally re-mapped along the space filling curve.  In a similar way, we showed how persistence data can be used to achieve 
strong parallel scaling for $\tt SpAMM$, using commonly available runtimes \cite{}.  
Persistence data may also be useful in other ways that enhance mathematical approximation.  

In Figures \ref{Lensing1} and \ref{Lensing2},  
we demonstrate a new kind of locality that can be exploited by $n$-body approximation of the square root iteration.
This algebraic locality develops compressively towards convergence as the contractive identity iteration develops.  
We call this compression {\bf \em lensing},  involving deflation of the culled volume about the resolvent's plane diagonal.
This is particularly important for the product yeilding $\mat{y}_k$, because it requires a tighter threshold, $\tau_s$, 
and because iteration in the channel begins with the thicker product $\mat{s} \ots \mat{s}$. 

It is also possible to achieve lensing without regularization, as shown in Figure \ref{}.  In Figure \ref{}, 
the lensing is not as strong as in the regularized instance, showing in addition to the $i=j$ plane, 
also the $i=k$ plane and thin  pillae normal to $ik$ in the $\mat{y}^{\rm dual}_k$ product.  

These additional featues are delocalizations cooresponging to weakness in the bound by Eq.~(\ref{bound}),  
and to the tighter thresholds required to maintain a stable iteration.  In the more extreme case of 
computing $\mat{y}^{\rm stab}_k$, these delocalizations are more extreme because the spread in the corresponding 
spectral resolution is greater, weakening Eq.~(\ref{bound}).

The development of pillae is associated with dimensionallity and reduced Cartesian seperations. 
In addition to algebraic and Euclidean locality, other measures are of interest, including information 
measures, generalizations of the  space filling curve heuristic, as well as graph reorderings 
that localize matrix elements about the diagonal \cite{}.  This latter technique is very common in 
structural mechanics.  We show also unregularized results for the structural matrix ${\tt bb}$. 

In figure \ref{}, 
we show the unregularized  for the $\kappa(\mat{s})=10^{10}$ 



gossamer, reticulate volumes

\section{Conclusions}

\pagebreak

\begin{itemize}
\item{new bound for N-Body approach to fast matrix multiplication: methods that truncate the data do not bound the outcome}
\item{stability analyses and bifurcations of ill-conditioned spamm away from basin of stability }
\item{show how to go past these bifurcations using regularization.
    showed how to find a first, most-approximate-yet-effective-by-one-order (MAYEBOO) preconditioner, 
which brings the norm-bounded and spectral resolutions into close alignment, 
strengthening Eq.~(\ref{bound}).}

\item{shows two new kinds of locality: strong Euclidean locality based on 
Gan and Challacombe's travel order, and algebraic locality associated with lensing,
 and contracting the culled task volume abount 
the plane-diagonal of the resolvent (lensing)}
}


\item{product representation of generic MAYEBOO slices with 
      extreme locality of reference, potential for additional acceleration and stabilization of in place data.
solution to problem developed from on genericity and concurence of the Erlang kind. Ecosystems impact.
algebraic structures cannot be approached by conventional row-coloum orientation
3-dimensional systems that would not be in the conventional (sparse BCSR) regime for $n$-scaling.  
}


\item{Sandwich approach potential for defered, implicit delocalization of the inverse factor, that can be avoided
by incremental application to a target vector or matrix. Potential for incremental preconditionin on multiple time scales (most approximate cooresponding to longest time scale. problems in electronic structure,generalized polar decomposition} and  ill-conditioning in radial basis function problems}
\end{itemize}

\bibliography{MatrixFunctions}

\end{document}
