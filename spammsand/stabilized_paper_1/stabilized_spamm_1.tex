\documentclass[letterpaper,twocolumn,amsmath,amsfont,amssymb,english,aps,jcp,preprintnumbers,groupaddress,nofootinbib,tightenlines,floatfix]{revtex4}

\usepackage{graphicx}
\usepackage{epstopdf}

%\documentclass[aps,prb,letterpaper,twocolumn,nofootinbib,showkeys]{revtex4-1}
%\documentclass[aps,amssymb,prl,letterpaper,twocolumn,nofootinbib,showkeys]{revtex4-1}

%\usepackage[backend=bibtex]{biblatex}

%    backend=biber,
%    style=authoryear,
%    natbib=true,
%    sortlocale=en_US,
%    url=false,
%    doi=true,f
%    eprint=false
%]{biblatex}
%\usepackage{hyperref}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\mmat}[1]{\widetilde{\boldsymbol{#1}}}
\newcommand{\matT}[1]{\boldsymbol{#1}^\dagger}
\newcommand{\ot}{  {\scriptstyle \otimes}_{ \tau } }
\newcommand{\ots}{ {\scriptstyle \otimes}_{ \! \tau_s } }
\newcommand{\oto}{ {\scriptstyle \otimes}_{ \! \tau_0 } }
\newcommand{\otm}{ {\scriptstyle \otimes}_{ \! \tau_m } }
\newcommand{\otmm}{ {\scriptstyle \otimes}_{ \! \tau_{m-1}}}

%\hypersetup{pdftitle={FreeON Project Report 1}}
%\hypersetup{pdfauthor={Matt Challacombe and Nicolas Bock}}
%\hypersetup{pdfsubject={A SpAMM Stabilized Newton Schulz Preconditioner: Fighting Error with Error}}

%\bibstyle{aipnum4-1}

\begin{document}

\title{On Stability of the First Order Newton Schulz Iteration in an Approximate Algebra}

\author{Matt Challacombe}
\email{matt.challacombe@freeon.org}
\homepage{http://www.freeon.org}

\author{Terry Haut}
\email{haut@lanl.gov}

\author{Nicolas Bock}
\email{nicolasbock@freeon.org}
\homepage{http://www.freeon.org}

\affiliation{Theoretical Division, Los Alamos National Laboratory}

\begin{abstract}
\end{abstract}

\maketitle
\section{Introduction}

In many areas of application, finite correlations lead to matrices
with decay properties.  By decay, we mean an approximate (perhaps
bounded \cite{}) inverse relationship between matrix elements and an
associated distance; this may be a simple inverse exponential
relationship between elements and the Cartesian distance between
support functions, or it may involve a generalized distance, {\em
  e.g.}~a statistical measure between strings.  In electronic
structure, correlations manifest in decay properties of the gap
shifted matrix sign function, as projector of the effective
Hamiltonian (Fig.~\ref{figure1}).  More broadly, matrix decay
properties may correspond to statistical matrices \cite{penrose1974,
  voit00, Anselin2003, Hardin2013, Krishtal2014}, including learned
correlations in a generalized, non-orthogonal metric \cite{}.  More
broadly still, problems with local, non-orthogonal support are often
solved with congruence transformations of the matrix inverse square
root \cite{Lowdin56, naidu11} or a related factorization
\cite{Krishtal2014}; these transformations correlate local support
with a representation independent form, {\em eg.}~of the eigenproblem.
Interestingly, the matrix sign function and the matrix inverse square
root function are related by Higham's identity:
\begin{equation}
\rm{sign} \left( \begin{bmatrix} 0 & \mat{s}      \\ \mat{I}       & 0\end{bmatrix} \right)  =
                 \begin{bmatrix} 0 & \mat{s}^{1/2} \\ \mat{s}^{-1/2} & 0\end{bmatrix}  .
\end{equation}
A complete overivew of matrix function theory and computation is given
in Higham's enjoyable reference \cite{Higham08}.

A well conditioned matrix $\mat{s}$ may often correspond to matrix
sign and inverse square root functions with rapid exponential decay,
and be amenable to the sparse matrix approximation
$\bar{\mat{s}} = \mat{s}+ \mat{\epsilon}^{\mat{s}}_\tau$,
where $\mat{\epsilon}^{\mat{s}}_\tau$ is the error introduced
according to some criterion $\tau$.  Supporting this approximation are
useful bounds to matrix function elements \cite{Benzi99b, }.
The criterion $\tau$ might be a drop-tolerence,
$\epsilon^{\mat{s}}_{\tau} = \{-s_{ij}*\hat{\mat{e}}_i \, | \, |s_{ij}|<\tau \}$,
a radial cutoff,
$\epsilon^{\mat{s}}_{\tau} = \{-s_{ij}*\hat{\mat{e}}_i \, | \, \lVert \mat{r}_i - \mat{r}_j \rVert > \tau \}$,
or some other approach to truncation, perhaps involving a sparsity
pattern chosen {\em a priori}.
Then, conventional computational kernels may be employed, such as the
sparse general matrix-matrix multiply ($\tt{SpGEMM}$)
\cite{Gustavson78, Toledo97, challacombe00, bowler00}, yielding fast
solutions for multiplication rich iterations and a modulated {\bf
  (what do you mean with modulated?)} fill-in.
These and related incomplete/inexact approaches to the computation of
sparse approximate matrix functions often lead to ${\cal O}(n)$
algorithms, finding wide use in technologically important
preconditioning schemes, the information sciences, electronic
structure and many other disciplines. Comprehensive surveys of these
methods in the numerical linear algebra are given by Benzi
\cite{Benzi99,Benzi02}, and by Bowler \cite{Bowler12} and Benzi \cite{Benzi13}
for electronic structure.

\begin{figure}[t]\label{figure1}
 \includegraphics[width=3.5in]{decay_picture.png}
  \caption{Examples from electronic structure of decay for the
    spectral projector (gap shifted sign function) with respect to
    local (atomic) support.  Shown is decay for systems with
    correlations that are short (insulating water), medium
    (semi-conducting 4,3 nanotube), and long (metalic 3,3 nanotube)
    ranged, from exponential (insulating) to algebraic (metallic). }
\end{figure}

Because the truncated multiplication is controlled only by absolute,
additive errors in the product,
\begin{equation} \label{sparseapprox}
\overline{ \mat{a} \cdot \mat{b} }\; = \; \mat{a}\cdot\mat{b} \; +\; \mat{\epsilon}^{\mat{a}}_\tau \cdot \mat{b} \;+\;
 \mat{a} \cdot \mat{\epsilon}^{\mat{b}}_\tau  \; + \;   {\mathcal O}(\tau^2)
\end{equation}
achieving sparse, stable and rapidly convergent iteration for
ill-conditioned problems can be challenging \cite{}.  In cases of
extreme degeneracy, hierarchical semi-seperable (reduced rank)
algorithms can offer effective complexity reduction \cite{}.  However,
many practical cases are somewhere in-between sparse and meaningfully
degenerate regimes; effectively dense but without an exploitable
reduction in rank.  This is the case in electronic structure for
strong but non-metallic correlation, {\em e.g.}~towards the Mott
transition \cite{}, and also in the case of local atomic support
towards completeness \cite{Others, Hutter, Gigi}.

\pagebreak

\section{Sparse Approximate Matrix Multiplication}

In this contribution, we consider an $N$-body approach to the
approximation of matrix functions with decay, based on the quadtree
data structure \cite{wise, samet}
\begin{equation}
\mat{a}^i = \begin{bmatrix} \,  \mat{a}^{i+1}_{00} \, & \,  \mat{a}^{i+1}_{01} \,  \\[0.2cm]  \, \mat{a}^{i+1}_{10} \,  & \,\mat{a}^{i+1}_{11} \, \end{bmatrix} \, ,
\end{equation}
and orderings that are locality preserving \cite{}.  Orderings that
preserve data locality are well developed in the database theory
\cite{}, providing fast spatial and metric queries.  Locality
enabled, fast data access is central to the $N$-Body approximation
\cite{}, and an important problem for enterprise \cite{} and runtime
systems \cite{}, with memory hierarchies becoming increasingly
asynchronous and decentralized \cite{cache}.  For matrices with
decay, orderings that preserve locality lead to blocked-by-magnitude
matrix structures with well segregated neighborhoods, inhabited by
matrix elements of like size, and efficiently resolved by the quadtree
data structure \cite{}.


\subsection{A Bounded Occlusion and Cull }

$\tt SpAMM$ has evolved from a row-coloumn oriented skipout \cite{} to recursive occlusion and culling \cite{}
based on sub-multiplicative norms $\lVert \cdot \rVert \equiv \lVert \cdot \rVert_F$ and the 
Cauchy-Schwarz inequality,  $\lVert \mat{a}\cdot \mat{b} \rVert < \lVert \mat{a} \rVert \lVert \mat{b} \rVert$ \cite{kahan}.
Occlusion involves avoiding work, whilst culling is the collecting of tasks. 
Here, we ammend the previous (n\"{a}ive) occlusion and cull with the following recursion:
\begin{widetext}
\begin{equation}
\mat{a}^{i} \ot \mat{b}^{i} =
\left\{
        \begin{array}{ll}
                 \emptyset \quad \tt{if}\quad \lVert \mat{a}^i \rVert \lVert \mat{b}^i \rVert < \tau \lVert a \rVert \lVert b \rVert \\[0.2cm]
                 \mat{a} ^i \cdot \mat{b}^i \quad  \tt{if}(i=\tt{leaf}) \\[0.2cm]
\begin{bmatrix} \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{00} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{10} \; , \; &
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11}  \\[0.2cm]
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11} \; , \; &
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11}
\end{bmatrix}  \quad \tt{else}
                \end{array}
              \right.  \, ,
\end{equation}
\end{widetext}
which bounds the relative occlusion error,  
\begin{equation}\label{bound}
\frac{\lVert \mat{\Delta}^{a \cdot b}_{\tau} \rVert}{N^2 }  \, \leq \, \tau \, \lVert \mat{a} \rVert  \,  \lVert \mat{b} \rVert \, ,
\end{equation}
occuring in the approximate product:
\begin{equation}\begin{equation}
\widetilde{\mat{a}\cdot \mat{b}} \,  \equiv \, \mat{a} \ot \mat{b} \,
  = \, \mat{a} \cdot \mat{b} + \mat{\Delta}^{a \cdot b}_{\tau} \,.
\end{equation}

\subsection{Proof}


\begin{equation}\label{bound}
\lVert \mat{\Delta}^{a \cdot b}_{\tau} \rVert \, \leq \, \tau \, N^2 \,  \lVert \mat{a} \rVert  \,  \lVert \mat{b} \rVert \, ,
\end{equation}

\subsection{Related Research} 

$\tt SpAMM$ is perhaps most closely related to the Strassen-like branch of fast matrix multiplication 
\cite{springerlink:10.1007/BF02165411,Ballard2014}.
In the Strassen-like approach, disjoint volumes in (abstract) tensor intermediates are omitted recursively \cite{}.  
In the $\tt SpAMM$ approach to fast multixplication, the numerically most significant volumes in 
 n\"{a}ive ($ijk$) tensor intermediates are culled, with error bounded by Eq.~(\ref{bound}).  
This bound makes $\ot$ a {\em stable} form of fast multiplication,  as explained by Demmel, Dumitriu and Holz (DDH; Ref.~\cite{Demmel07}).

$\tt SpAMM$ is a $n$-body method for fast matrix multiplication, related to the 
generalized methods popularized by Grey \cite{Gray2001,Gray2003}. In our development,
generalization reflects the {\em genericity} \cite{} of recursive data access \cite{}, 
enabling range querries, metric querries, higher dimensional querries and so on, with common frameworks, 
structures and runtimes.  Recently for example, we generalized the double (left-right) metric querry in 
Eq.~(\ref{}) to hextree metric querries of the exact Fock exchange \cite{}.  

Also, $n$-body methods offer well established protocols for turning spatial and metric locality into 
data and temporal locality \cite{}; recently, x, y  and Yellik showed perfect strong scaling and communication optimality 
for pairwise $n$-body methods \cite{Yellik}.   Bridging the gap between 
$n$-body solver and fast matrix multiplication, we recently demonstrated strong scaling for fast matrix multiplication ($\tt SpAMM$) \cite{}.  
The introduction of algebraic and strong Euclidean locality in this work may further enhance high performance implementaitons. 

%As noted by Aluru \cite{}, the top-down $n$-body model and breadth-first map-reduction are equivalent \cite{}, offering
%the potential febraic and strong Euclidean locality developed in this work may further enhance high performance implementaiton. 
%or alignment with emergent enterprise frameworks \cite{} and functional programming 
%languages that support generacity \cite{}. Language suppo
%with very simple (skeletonized) frameworks, lowering barriers to entry, enhancing performance towards decentralized memory landscapes, 
%and following a sustainable commodity trend \cite{softwaresustainanbilty} that offers 
%increasingly cheap compute cycles over the next few decades \cite{}. 

This work offers a data local alternative to fast non-deterministic methods for sampling the product, 
which include sketching \cite{Sarlos2006,Drineas2006,Mahoney2012,Pagh2013,Sivertsen2014,Woodruff2015},
joining \cite{Mishra92,Hoel94,Jacox03,Chen07,Amossen09,Lieberman08,Kim09}, 
sensing \cite{} and probing \cite{}.  These  methods involve a wheighted (probablistic) 
and on the fly sampling, with the potential for complexity reduction in the case of random distributions. 
$\tt SpAMM$ also employs on the fly wheighted sampling,  but with 
compresion through locality, brought about by algebraic correlations (towards identity) and also in the
metric structure, through strong Euclidean locality.

Methods that achieve compression in the stream of product intermediates are different from reduced rank algorithms that achieve 
matrix compression in a step that preceeds multiplication (seperability)  \cite{}.   However, matrix compressions are 
generally compatible with the quadtree, as are additional fast (generalized) solvers that add complex functionality ({\em e.g.} 
in electronic structure theory \cite{}).  Thus, generality 
and interoperability may enable deeply layered, thin and generic solvers easy access to in place data. 
Further, language support may provide simple (skeletonized) frameworks for generic recursion, 
offering opportunities to greatly simplify codebase at the systems level, 
lowering barriers to entry and enhancing concurence (in the Erlang sense). 


Finally, previous work on the scaled NS iteration has heavily influenced this work.  Formost is 
Higham, Mackey, Mackey and T (HMMT; Ref.~\cite{}) masterwork on convergence of NS iteration under all groups,
wherein HMMT also develop Fr\"{e}chet analyses for stable square root iteration at the fixed point.
Also, important inspiration comes from Chen and Chow's \cite{} approach to scaled NS iteration for ill-conditioned problems \cite{}, 
and from the Helgaker groups work on NS iteration, whose notation we follow in part \cite{}.  

% which leads to a non-associative algebra and error flows with
% properties of the Lie bracket
% \begin{equation} \label{braket}
% \widetilde{\left[ \mat{a} , \mat{b} \right]} \equiv \mat{a} \ot \mat{b}-\mat{b} \ot \mat{a}
% =  \left[ \mat{a} , \mat{b} \right]
% + \mat{\Delta}^{a\cdot b}_{\tau} -\mat{\Delta}^{b\cdot a}_{\tau} \,.
% \end{equation}

% Finally, the

\section{First Order Newton-Shulz Iteration}

There are two common, first order NS iterations; the sign iteration
and the square root iteration, related by the square, $\mat{I}\left(
\cdot \right)= {\rm sign}^2\left( \cdot \right) $.  These equivalent
iterations converge linearly at first, then enter a basin of stability
marked by super-linear convergence.  Our interest is to access this
basin with the most permissive $\tau$ possible, building a foundation
for future refinement
%, {\em e.g.}
at a reduced cost and with a higher precision ($\tau \rightarrow 0$)
\cite{MChallacombe16}.

\subsection{Sign iteration}

For the NS sign iteration, this basin is marked by a behavioral change
in the difference $\delta \mat{X}_k = \widetilde{\mat{X}}_k -\mat{X}_k
= {\rm sign} \left(\mat{X}_{k-1}+\delta \mat{X}_{k-1} \right) -{\rm
  sign} \left(\mat{X}_{k-1} \right)$, where $\delta \mat{X}_{k-1}$ is
some previous error.  The change in behavior is associated with the
onset of idempotence and the bounded eigenvalues of ${\rm sign}'\left(
\cdot \right)$, leading to stable iteration when ${\rm sign}'\left(
\mat{X}_{k-1} \right) \delta \mat{X}_{k-1} < 1 $.  Global perturbative
bounds on this iteration have been derived by Bai and Demmel
\cite{Bai98usingthe}, while Byers, He and Mehrmann \cite{} developed
asymptotic bounds.  The automatic stability of sign iteration is a
well developed theme in Ref.\cite{Higham08}.

\subsection{Square root iteration}

In this work, we are concerned with resolution of the identity \cite{}
\begin{equation}
\mat{I} \left( \mat{s} \right) =\mat{s}^{1/2} \cdot \mat{s}^{-1/2} \, ,
\end{equation}
and the cooresponding canonical (dual) square root iteration \cite{};
\begin{eqnarray}\label{cannonical}
\mat{y}_k &\leftarrow& h_\alpha \left[ \mat{y}_{k-1} \cdot \mat{z}_{k-1} \right] \cdot \mat{y}_{k-1}  \nonumber \\
\mat{z}_k &\leftarrow& \mat{z}_{k-1} \cdot h_\alpha \left[ \mat{y}_{k-1} \cdot \mat{z}_{k-1} \right] \; ,
\end{eqnarray}
with eigenvalues in the proper domain aggregated towards 0 or 1 by the
NS map $h_\alpha[\mat{x}]=\frac{\sqrt{\alpha}}{2} \left(3-\alpha
\mat{x} \right)$ \cite{}.  Then, starting with $\mat{z}_0=\mat{I}$ and
$\mat{x}_0=\mat{y}_0=\mat{s}$, ${\mat{y}}_k \rightarrow
\mat{s}^{1/2}$, ${\mat{z}}_k \rightarrow \mat{s}^{-1/2}$ and
${\mat{x}}_k \rightarrow {\mat{I}}$.  As in the case of sign
iteration, this dual iteration was shown by Higham, Mackey, Mackey and
Tisseur \cite{Higham2005} to remain bounded in the superlinear regime,
by idempotent Frechet derivatives about the fixed point
$\left(\mat{s}^{1/2},\mat{s}^{-1/2}\right)$, in the direction $\left(
\delta \mat{y}_{k-1} , \delta \mat{z}_{k-1} \right)$:
\begin{eqnarray}
\delta \mat{y}_k &=& \frac{1}{2} \delta \mat{y}_{k-1} - \frac{1}{2} \mat{s}^{1/2} \cdot \delta \mat{z}_{k-1} \cdot \mat{s}^{1/2} \\
\delta \mat{z}_k &=& \frac{1}{2} \delta \mat{z}_{k-1} - \frac{1}{2} \mat{s}^{-1/2} \cdot \delta \mat{y}_{k-1} \cdot \mat{s}^{-1/2} \;.
\end{eqnarray}
In this contribution, we consider another aspect of convergence,
namely the (hopefully) linear approach towards stability of the
iteration
\begin{equation}
\widetilde{\mat{x}}_k \leftarrow
 \widetilde{\mat{y}}_k \left( \widetilde{\mat{x}}_{k-1} \right)
\, \ot \, \widetilde{\mat{z}}_k \left( \widetilde{\mat{x}}_{k-1} \right) \, ,
\end{equation}
made difficult by ill-conditioning and a sketchy $\ot$.

\subsection{the NS map}

Initially, $h'_\alpha$ at the smallest eigenvalue $x_0$ controls the
rate of progress towards idempotence.  As recently shown by Jie and
Chen \cite{Chen2014}, for very ill-conditioned problems, a factor of
two reduction in the number of NS steps can be achieved by choosing
$\alpha \sim 2.85$, which is at the edge of stability.  As argued by
Pan and Schreiber \cite{Pan1991}, Jie and Chen \cite{Chen2014},
switching or damping the scaling factor towards $\alpha=1$ at
convergence is important, shifting emphasis away from the behavior of
$x_0$ towards {\em e.g.}~$x_i \in [0.01,1]$, emphasizing overall
convergence of the broad distribution \cite{Pan and Scriber}.  In an
approximate algebra like $\tt SpAMM$, the potential for eigenvalues to
fluctuate out of the domain of convergence must be considered.  This
is addressed in Section \ref{}.

\subsection{Ill-conditioning, Stability and Implementation}

There are a number of nominally equivalent instances of the square root iteration, related by commutations and transpositions. 
However, these instances may have very different stability properties,  controled to first order by the Frechet derivatives
\begin{equation}
  \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}
= \lim_{\tau \rightarrow 0} \slantfrac{ \mat{x} \left( \mat{y}_{k-1} + \tau \delta \widehat{\mat{y}}_{k-1} ,  {\mat{z}}_{k-1}  \right)
                                     -\mat{x}_k    }{\tau} 
 \end{equation}
and
 \begin{equation}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} = \lim_{\tau \rightarrow 0}
\slantfrac{ \mat{x} \left( {\mat{y}}_{k-1} , \mat{z}_{k-1} +\tau  \delta \widehat{\mat{z}}_{k-1} \right) - \mat{x}_k   }{\tau}  \, , 
 \end{equation}
along the unit directions of the previous errors $\delta \widehat{\mat{y}}_{k-1}$ and $\delta \widehat{\mat{z}}_{k-1}$, corresponding 
to the associated displacement magnitudes $\delta y_{k-1} = \lVert \delta \mat{y}_{k-1} \rVert$  and  $\delta z_{k-1}=\lVert \delta \mat{z}_{k-1} \rVert$.
Then, the differential 
\begin{equation} \label{firstorderdual}
\delta \mat{x}_k = \,  { \mat{x}}_{\delta \widehat{\mat{y}}_{k-1}}  \, {\scriptstyle \times} \, \delta y_{k-1}
                 \, + \,  { \mat{x}}_{\delta \widehat{\mat{z}}_{k-1}}  \, {\scriptstyle \times} \, \delta z_{k-1}  \, + {\cal O}(\tau^2) 
\end{equation}
determines the total first order stability. 

This formulation allows to consider orientational effects involving eigenvector fidelity and convergence of 
derivatives towards zero seperately from displacement effects involving accumulation and $\tt SpAMM$ source errors.
In some cases, instabilities may be associated with derivatives that do not vanish towards identity, yeilding an unbounded iteration \cite{}.
In other instances, an instability may be associated with rapidly increasing displacements, due to a too large $\tau$.  Instability 
may also arize due to the numerical corruption of the eigenvectors, also resulting in derivatives that vanish too slowly (or blow up altogether).    

The potential for
instability is increased with ill-conditioning through the terms $\lVert \mat{z}_{k} \rVert  \rightarrow \sqrt{\kappa\left(\mat{s} \right)}$.
Also for ill-conditioned systems, scaling is nessesary to accelerate convergence.  However with scaling, increasing the map 
derivative $h'_\alpha$ can also further enhance the rate of error accumulation. 

In following sections, we'll examine how these effects differ from the ideal (double precision) cannonical (dual) square root iteration
for ill-conditioned systems and in the strongly non-associative, sketchy $\ot$ regime corresponding to permisive values of $\tau$.
At this early stage, we are interested in hazzards and opportunities associated with
different formulations and implementational details.   In addition to deviations from the full precision dual instance,
we will develop the  ``stabilized'' instance,
\begin{eqnarray}\label{stabilized}
\mat{z}_k &\leftarrow& \mat{z}_{k-1} \cdot h_\alpha \left[ \mat{x}_{k-1} \right] \; , \nonumber \\
\mat{x}_k &\leftarrow&  \mat{z}^\dagger_{k} \cdot \mat{s} \cdot \mat{z}_{k-1} \; ,
\end{eqnarray}
with the corresponding differential;
\begin{equation} \label{firstorderstab}
\delta \mat{x}_k = \,  { \mat{x}}_{\delta \widehat{\mat{z}}_{k-1}}  \, {\scriptstyle \times} \, \delta z_{k-1}  \, + {\cal O}(\tau^2)  \, .
\end{equation}

Nominally, $\mat{y}^{\rm dual}$ is equivalent to $\mat{y}^{\rm stab}_k \equiv \mat{z}^\dagger_{k} \cdot \mat{s}$ 
is also equivalent to $\mat{y}^{\rm naive}_k \equiv \mat{z}_{k} \cdot \mat{s}$.
However, with ill-conditioning and in only double precision, these two instances may diverge due to non-associative errors that rapidly compound.
In the case of the duals iteration under $\tt SpAMM$ approximation, the $\widetilde{\mat{y}}^{\rm dual}_k$ channel does not retain contact 
with the eigenvectors, span $\mat{s}$, whilst the stab instance does.  
In the duals iteration, the $\widetilde{\mat{y}}_k$  $\tt SpAMM$ update 
is mild, with errors in the relative product remaining well conditioned.  
In the stab instance, conection with $\mat{s}$ is retained at each step, but at the price of the 
$\mat{y}^{\rm stab}_k$ update involving magnitudes that vary widely in the $\tt SpAMM$ product.     

For these reasons, maintaining connection to the eigenvectors of $\mat{s}$ through 
a tighter first product is nessesary.  In the stab instance, and with a 
tighter ``{\em s}'' product, $\tau_s \ll \tau$, we find very interesting left/right differences; 
namely, the right first product 
\begin{equation} 
\widetilde{\mat{x}}^R_k \leftarrow \widetilde{\mat{z}}^\dagger_{k} \, \ot  \, \left( \mat{s} \,  \ots \, \widetilde{\mat{z}}_{k-1}  \right) \; ,
\end{equation}
is  different from the left first product 
\begin{equation} 
\widetilde{\mat{x}}^L_k \leftarrow \left(  \widetilde{\mat{z}}^\dagger_{k} \, \ots \, \mat{s} \right) \,  \ot  \, \widetilde{\mat{z}}_{k-1} \; .
\end{equation}

\section{Implementation}

\subsection{programming}

FP, F08, OpenMP 4.0
In the current implementation, all persistence data
(norms, flops, branches \& {\em etc.}) are accumulated compactly in the backward recurrence.  This persistence data
 that may be achieved by minimal locally essential trees \cite{}.


\subsection{scaling and stabilization}

\subsection{regularization}
damping the inversion and the small value to be added c is called Marquardt-Levenberg coefficient

\subsection{convergence}

Map switching and etc based on TrX

\section{Data} \label{data}

\subsubsection{double exponential ill-conditioning}
3,3 carbon nanotube with diffuse $sp$-function
double exponential (Fig.)

\subsubsection{three-dimensional, periodic}
%\subsubsection{water boxes}
%\subsubsection{ordering}
\subsubsection{Matrix Market}

\section{Error Flows in Square Root Iteration}

\subsection{The cannonical (dual) instance}
 
Refering back to Eq.~( \ref{firstorderdual} ), we develop the Fr\'{e}chet analyses \cite{} with the goal of understanding
the contractive approach to identity in competition with error accumulations and $\tt SpAMM$ sources.
Of interest are the derivatives 

% \begin{equation}
%   \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}
% =   \mat{y}_{\delta \widehat{ \mat{y}}_{k-1}} \cdot \mat{z}_{k}  + \mat{y}_{k}  \cdot \mat{z}_{\delta \widehat{ \mat{y}}_{k-1}}
%  \end{equation}
% and
%  \begin{equation}
%  \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} =
% \mat{y}_{\delta \widehat{ \mat{z}}_{k-1}} \cdot \mat{z}_{k}  + \mat{y}_{k}  \cdot \mat{z}_{\delta \widehat{ \mat{z}}_{k-1}} \, .
%  \end{equation}


% For $\mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}$ we have
% \begin{multline}
%  \mat{y}_{\delta \widehat{ \mat{y}}_{k-1}} = h_\alpha \left[ \mat{x}_{k-1} \right] \cdot \delta \widehat{\mat{y}}_{k-1} \\
%                                      + h'_\alpha \cdot \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1} \cdot \mat{y}_{k-1}
% \end{multline}
% and
% \begin{equation}
%  \mat{z}_{\delta \widehat{ \mat{y}}_{k-1}} =  \mat{z}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1}
% \end{equation}
% yeilding
% Also, for $\mat{x}_{\delta \widehat{ \mat{z}}_{k-1}}$ we have
% \begin{equation}
%  \mat{y}_{\delta \widehat{ \mat{z}}_{k-1}} =  {\mat{y}}_{k-1} \cdot  h'_\alpha \delta \widehat{ \mat{z}}_{k-1} \cdot  \mat{y}_{k-1}
% \end{equation}
% and
% \begin{multline}
%  \mat{z}_{\delta \widehat{ \mat{z}}_{k-1}} = \delta \widehat{\mat{z}}_{k-1} \cdot   h_\alpha \left[ \mat{x}_{k-1} \right] \\
%                                      + \mat{z}_{k-1} \cdot {\mat{y}}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{z}}_{k-1}  \; ,
% \end{multline}
% yeilding


\begin{multline}
  \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}} = h_\alpha \left[ \mat{x}_{k-1} \right]  \cdot \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k} \\
+  h'_\alpha  \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1} \cdot  \mat{y}_{k-1} \cdot  \mat{z}_{k} \\
 + \mat{y}_{k} \cdot \mat{z}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1}  \, .
\end{multline}


\begin{multline}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} =  {\mat{y}}_{k-1} \cdot  h'_\alpha \delta \widehat{ \mat{z}}_{k-1} \cdot  \mat{y}_{k-1}  \cdot \mat{z}_{k} \\
\qquad + \mat{y}_k \cdot  \delta \widehat{\mat{z}}_{k-1} \cdot   h_\alpha \left[ \mat{x}_{k-1} \right] \\
\qquad \qquad +  \mat{y}_{k} \cdot  \mat{z}_{k-1} \cdot {\mat{y}}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{z}}_{k-1} \, .
\end{multline}

Closer to a fixed point orbit,  $\mat{y}_k \cdot \mat{z}_{k-1} \rightarrow \mat{I}$, $\mat{y}_{k-1} \cdot \mat{z}_{k} \rightarrow \mat{I}$,
$h_\alpha \left[ \mat{x}_{k} \right] \rightarrow \mat{I}$ and $h'_\alpha \rightarrow - \frac{1}{2}$ \cite{higham2005}.  Then,

\begin{equation} \label{yorbit}
 \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}} \rightarrow \delta \widehat{\mat{y}}_{k-1} \cdot \left( \mat{z}_k-\mat{z}_{k-1} \right)
\end{equation}
and
\begin{equation} \label{zorbit}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} \rightarrow \left( \mat{y}_k-\mat{y}_{k-1} \right) \cdot \delta \widehat{\mat{z}}_{k-1} .
\end{equation}
Thus, contributions along $\delta \widehat{\mat{y}}_{k-1}$ and $\delta \widehat{\mat{z}}_{k-1}$ are tightly shut down in the 
region of superlinear convergence.  Achieving a contractive fixed point orbit, however requires that the three terms in Eq.~(\ref{}),  
with potentially different error accumulations and $\tt SpAMM$ sources, must cancel faster than $\delta y_{k-1}$ 
and $\delta z_{k-1}$ accumulate.

In this analysis, we've seperated the directional component of the error from its distance, because in addition to the
previous compounding error, each displacement contains also a first order $\tt SpAMM$ source error.  Its simpler to 
consider these effects serpately, at least in this first contribution. 

To understand $\delta \mat{z}_{k-1}$, we partially unwind the approxinate  $\widetilde{\mat{z}}_{k-1}$;
%\begin{equation}
\begin{eqnarray} \label{widetildez}
 \widetilde{\mat{z}}_{k-1} &=&  \widetilde{\mat{z}}_{k-2}  \, \ot \, h_\alpha[\widetilde{\mat{x}}_{k-2}]\\
&=& \Delta^{\widetilde{\mat{z}}_{k-2} \cdot h_\alpha \left[ \widetilde{\mat{x}}_{k-2}\right]}_\tau
+ \widetilde{\mat{z}}_{k-2} \cdot h_\alpha\left[ \widetilde{\mat{x}}_{k-2}\right]
%\end{equation}
\end{eqnarray}
Then, using
\begin{equation}
  h_\alpha \left[ \widetilde{\mat{x}}_{k-2} \right]
=  h_\alpha \left[ \mat{x}_{k-2} \right] +  h'_\alpha  \delta \mat{x}_{k-2} \, 
\end{equation}
and taking $\mat{z}_{k-1}$ from both sides, we find  
%Eq.~(\ref{widetildez}) yeilds 
\begin{multline}
 \delta {\mat{z}}_{k-1} =\Delta^{\widetilde{\mat{z}}_{k-2} \cdot h_\alpha \left[ \widetilde{\mat{x}}_{k-2}\right]}_\tau
\\ +\delta \mat{z}_{k-2} \cdot h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]
+ \mat{z}_{k-2} \cdot h'_\alpha \delta \mat{x}_{k-2}  \, ,
\end{multline}
bounded by 
% \begin{multline}
%  \delta {z}_{k-1} <
% \lVert \mat{z}_{k-2} \rVert \left( \;  \tau \, \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]  \rVert
% + h'_\alpha \delta {x}_{k-2} \right)  \\ +
% \delta {z}_{k-2}  \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2}  \right] \rVert 
% \end{multline}
 \begin{multline}
  \delta {z}_{k-1} <
 \lVert \mat{z}_{k-2} \rVert \left( \tau \, \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]  \rVert
 + h'_\alpha  \delta y_{k-2} \lVert z_{k-2} \rVert \right)  \\ 
 + \delta {z}_{k-2} \left( \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2}  \right] \rVert  + \lVert y_{k-2} \rVert \right) .
 \end{multline}


primary error channels contibuting to $\delta z_{k-1}$ are through the first order $\tt SpAMM$ error
$ \tau \lVert \mat{z}_{k-2} \rVert \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]\rVert$
and the volatile term $h'_\alpha  \delta y_{k-2} { \lVert \mat{z}_{k-2} \rVert }^2$.

%  \begin{eqnarray}
% \widetilde{\mat{y}}_{k-1}&=&
% h_\alpha [ \widetilde{\mat{x}}_{k-2}] \, \ots \, \widetilde{\mat{y}}_{k-2}  \\
% &=&  \mat{\Delta}^{  h_\alpha[ \widetilde{\mat{x}}_{k-2}] \cdot \widetilde{\mat{y}}_{k-2} }_{\tau_s}  \, + \,
%      h_\alpha[ \widetilde{\mat{x}}_{k-2}] \cdot \widetilde{\mat{y}}_{k-2}
% \end{eqnarray}
% \begin{multline}
% \delta \mat{y}_{k-1}= \mat{\Delta}^{h_\alpha [ \widetilde{\mat{x}}_{k-2}] \cdot \mat{y}_{k-2} }_\tau  \\ +
% h'_\alpha \delta \mat{x}_{k-2} \cdot \mat{y}_{k-2}
% + h_\alpha [ \widetilde{\mat{x}}_{k-2}] \cdot \delta \mat{y}_{k-2} \, ,
% \end{multline}

corresponding to  basis corruption and controlled by $\ots$, with $\tau_s \ll \tau$. 
As above, we can unwind this sensitive term, to find 
\begin{multline}
\delta y_{k-2} <   \lVert \mat{y}_{k-3} \rVert  \left( \tau_s \lVert h_\alpha [ \widetilde{\mat{x}}_{k-3}] \rVert + h'_\alpha \delta z_{k-3} \right )\\
+  \delta y_{k-3}  \left( \lVert \widetilde{\mat{z}}_{k-3}]  \rVert + \lVert h_\alpha [ \widetilde{\mat{x}}_{k-3}]  \rVert
  \right)
\, .
\end{multline}


\subsection{The stabilized  (stab) instance}

Here, we carry on from Eq.~(\ref{firstorderstab}) in the ``stabilized'' instance, with the single channel differential 
\begin{equation}
 \mat{x}_{\widehat{\mat{z}}_{k-1}} =   \mat{z}^\dagger_{\widehat{\mat{z}}_{k-1}} \cdot \mat{s} \cdot \mat{z}_{k} + 
                                  \mat{z}^\dagger_k \cdot \mat{s} \cdot \mat{z}_{\widehat{\mat{z}}_{k-1}} 
\end{equation}



\begin{multline}
\mat{z}_{\widehat{\mat{z}}_{k-1} } = \delta \widehat{\mat{z}}_{k-1} \cdot h_\alpha[ \widetilde{\mat{x}}_{k-1} ] + \mat{z}_{k-1} \cdot \left( \right. \\
 \left.  h'_\alpha \delta \widehat{\mat{z}}^\dagger_{k-1} \cdot \mat{s} \cdot \mat{z}_{k-1} +  \mat{z}^\dagger_{k-1} \cdot \mat{s} \cdot h'_\alpha \delta \widehat{\mat{z}}_{k-1}
 \right) 
\end{multline}

\begin{eqnarray}
\widetilde{\mat{y}}^{\rm stab}_{k-1} &=& \widetilde{\mat{z}}^{\dagger}_{k-1} \, \ot \, \mat{s} \\
                                  &=& \mat{\Delta}^{\widetilde{\mat{z}}^\dagger_{k-1} \cdot \, \mat{s}} \, + \,
\left( \widetilde{\mat{z}}_{k-2} \cdot h_\alpha[ \widetilde{\mat{x}}_{k-2}] \right)^\dagger \, \cdot \, \mat{s}
\end{eqnarray}

\subsection{Bifurcations}

\begin{figure}[h] \label{flow_noscale_dual}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_noscaling/33_nanotube_cond10_noscale_dual.eps}
\caption{Derivatives, displacements and the approximate trace of the unscaled, dual NS iteration for a (3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and $\tau_y=\{10^{-8}, 10^{-9}, 10^{-10}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}


% \begin{figure}[h]
% \includegraphics[width=3.5in]{fig_33_tube_cond_10_noscaling/33_nanotube_cond10_noscale_stab.eps}
% \caption{Derivatives, displacements and the approximate trace of the unscaled, stablized NS iteration for a (3,3) 
% nanotube with $\kappa =10^{10}$. 
% Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and 
% $\tau_y=\{10^{-4}, 10^{-5}, 10^{-6}$  are the dashed lines.  The trace expectation is shown as a full black line. }
% \end{figure}


\begin{figure}[h] \label{flow_scaled_dual}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_scaled/33_tube_k10_scale_dual.eps}
\caption{Derivatives, displacements and the approximate trace of the scaled, stablized NS iteration for a
(3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$,
$\tau=10^{-3}$ and $\tau_y=\{10^{-3},10^{-4},10^{-6}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}

\begin{figure}[h]\label{flow_scaled_stab}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_scaled/33_tube_k10_scale_stab.eps}
\caption{Derivatives, displacements and the approximate trace of the unscaled, dual NS iteration for a (3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and $\tau_y=\{10^{-8}, 10^{-9}, 10^{-10}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}

Differences in occlusion between stab and dual magnified as bounds for s.z not as tight as bounds for h.y.

 *lot* of overlap too (reproducing hilberts etc).

\section{Iterated Regularization}

Shown in the preceeding section, stabiliyty limits application of the NS square root iteration under 
agressive $\tt SpAMM$ approximation.  These limits can be circumvented through Tikhonov regularization \cite{}, 
involving a small level shift of eigenvalues,  $\mat{s}_\mu \leftarrow \mat{s}+\mu \mat{I}$, leading to a more 
well conditioned matrix with $\kappa( \mat{s}_\mu) = \frac{\sqrt{s^2_{N-1} + \mu^2}}{\sqrt{s^2_0+\mu^2}}$ \cite{}.  
However, achieving substantial acceleration with severe ill-conditioning  may require a large level shift, 
producing inverse factors of little practical use.  One approach to recover a more accurate inverse
factor is Riley's method \cite{}; 
\begin{equation}
\mat{s}^{-1/2} = \mat{s}^{-1/2}_{\mu} \cdot \left( \mat{I}+\frac{\mu}{2} \mat{s}^{-1}_{\mu}
                                                   +\frac{3 \mu^2}{8} \mat{s}^{-2}_{\mu} + \dots
   \right) \; ,
\end{equation}
but this is ineffective when $\mu$ is large, and invovles powers of the full inverse. 

Here, we outline a nested product represenation of the full inverse factor and present preliminary 
results for  approximate solutions. 
This product represenation rests on a most permisive (but still effective) 
preconditioner, $\mat{s}^{-1/2}_{\tau_0 \mu_0}$. 
By effective we mean $\mu_0<\, .1$ ({\em ie} taking $\mat{s} \leftarrow \mat{s}/s_{N-1}$, conditioning is improved by at least one order). 
By permisive, we mean $\tau_0$ is large but enables stable NS iterations for the shifted problem. 
In this way, the matrix and product norms may be brought into a tighter distribution 
about the resolvent $\mat{I}_{\tau_0\mu_0}\equiv \widetilde{\mat{I}}\left(\mat{s}_{\tau_0\mu_0}\right)$,
vastly strengthening the bound  Eq.~(\ref{bound}) and yeilding strongly contractive locality properties about
the fixed point.  Thus, it may be possible to iteratively build the full factor with likewise ``thin'', generic 
increments ($\Delta \tau \sim .1$ and $\Delta \mu \sim .1$). 

Culled $\tt SpAMM$ volumes are shown with increasing system size in Fig.~\ref{regularized_stab} for the stable instance, 
and  in Fig.~\ref{regularized_dual} for the dual instance, cooresponding to a first ``thin''  slice preconditer for the 
(3,3) $\kappa(\mat{s})=10^{10}$ nanotube series.  The behavior of these instances is very different; in the stable 
case, a ``stable'' iteration could not be found for the $\mu_0=.1$ regularization.  Also, this stable iteration
sees a weakly convergent trace with increasing $\tt SpAMM$ volumes.   On the other hand, volume of the dual iteration 
is strongly contracted with resolution of the identity.   
These results reflect very different culled subspaces, with 
cancelation of eigen-powers in 
$\widetilde{\mat{y}}^{\rm stab}_k \rightarrow  \mat{s}^{-1/2}_{\tau_0 \mu_0} \oto \mat{s}_{\mu_0}$ weakening Eq.~(\ref{bound}), whilst 
$\widetilde{\mat{y}}^{\rm dual}_k \rightarrow  \mat{I}_{\tau_0 \mu_0} \oto \mat{s}^{1/2}_{\tau_0 \mu_0}$ should approach  quadtree copy in place.   

\begin{figure}[h]\label{regularized_stab}
 \includegraphics[width=3.5in]{fig_33_tube_cond_10_regularized/33_tube_k10_regularized_stab.eps}
\caption{
Culled volumes in the thin slice, stable instance approximation of $\mat{s}^{-1/2}_{\tau_0 \mu_0}$
for the (3,3) nanotube, $\kappa(\mat{s})=10^{10}$ matrix series 
described in Section \ref{data}.  In the ``stable'' instance, it was not possible to achieve stability with $\tau_0=.1$
In this ``stable'' case, a thin slice cooresponds to $\mu_0=.1, \tau_0=10^{-2} \;  \&  \; \tau_s=10^{-4}$.  Labeled volumes are
$\rm v_{\widetilde{\mat{z}_k}} = \left( {\rm vol}_{ \widetilde{\mat{z}}_{k-1}\ot h[\widetilde{\mat{x}}_{k-1}] } \right) \times 100\% /N^3$ and
${\rm v}_{\widetilde{\mat{y}}_k} = \left( {\rm vol}_{\mat{s}  \ots  \widetilde{\mat{z}}_{k}} \right) \times 100\% /N^3$.    
Line width increases with increasing system size. 
Also shown is the trace error, ${\rm e}_{k} = \left( N-{\rm tr}\,\mat{x}_k \right)/N$. }
\end{figure} 
\begin{figure}[h]\label{regularized_dual}
 \includegraphics[width=3.5in]{fig_33_tube_cond_10_regularized/33_tube_k10_regularized_dual.eps}
\caption{
Culled volumes in the thin slice, dual instance approximation of $\mat{s}^{-1/2}_{\tau_0 \mu_0}$
for the (3,3) nanotube, $\kappa(\mat{s})=10^{10}$ matrix series 
described in Section \ref{data}. The thin slice cooresponds to $\mu_0=.1, \tau_0=.1 \;  \&  \; \tau_s=.001$ 
with labeled volumes 
${\rm v}_{\widetilde{\mat{y}}_k}= \left( {\rm vol}_{  h[\widetilde{\mat{x}}_{k-1}] \ots \widetilde{\mat{y}}_k }  \right) \times 100\% /N^3$ and  
${\rm v}_{\widetilde{\mat{z}}_k}= \left( {\rm vol}_{\widetilde{\mat{z}}_{k-1} \ot  h[\widetilde{\mat{x}}_{k-1}]} \right) \times 100\% /N^3$.
Line width increases with increasing system size. 
Also shown is the trace error, ${\rm e}_{k} = \left( N-{\rm tr}\,\mat{x}_k \right)/N$. }
\end{figure} 

With the dual instance, the $\tt SpAMM$ approximation can be brought all the way to  $\tau_0 = .1$ in the case of $\mu_0 = .1$.
From this first slice  $\mat{s}^{-1/2}_{\tau_0, \mu_0}$ then, a next level shifted preconditioner can be found, 
$\mat{s}^{-1/2}_{\tau_0 \mu_1}$, based on the residual 
$\left(\mat{s}^{-1/2}_{\tau_0\mu_0} \right)^\dagger \, \oto \, \left(\mat{s}+\mu_1 \mat{I} \right)  \, \oto \,\mat{s}^{-1/2}_{\tau_0 \mu_0} $, with {\em e.g.} 
$\mu_1= .01$. It may then be possible to find the full ($\tt SpAMM$ most approximate) 
factor as the nested product of preconditioned thin slices;
\begin{eqnarray}
\mat{s}^{-1/2}_{\tau_0} &=& \mat{s}^{-1/2}_{\tau_0 \mu_n} \, \oto \, \mat{s}^{-1/2}_{\tau_0 \mu_{n-1}} \, \oto \, \dots 
\,  \mat{s}^{-1/2}_{\tau_0 \mu_0}\\
&=&\bigotimes^{\mu_n}_{\mu=\mu_0} \mat{s}^{-1/2}_{\tau_0 \mu} \, .
\end{eqnarray}In this way, 
iterative regularization can be used to find a product representation of the inverse square root at a $\tt SpAMM$ resolution 
potentially far more permisive than otherwise possible. 
Likewise, it may be possible to obtain the full factor with increasing $\tt SpAMM$ resolution in the product representation:
\begin{eqnarray}
\mat{s}^{-1/2} &=& \mat{s}^{-1/2}_{\tau_m} \, \otm \,  \mat{s}^{-1/2}_{\tau_{m-1}} \, \otmm \dots \, \mat{s}^{-1/2}_{\tau_0}\\ 
&=&\bigotimes^{\tau_m}_{\tau=\tau_0} \mat{s}^{-1/2}_{\tau} \, ,
\end{eqnarray}taken over the sequence $1 > \tau_0 > \tau_1 > \dots > \tau_n $.

The naive scheme requires $m+n$ total NS solves, and potentially as many multiplies to incrementally apply the full factor $\mat{s}^{-1/2}$.
However, a thin product representation may have advantages: ({\bf 1}) Each NS solve can be reduced 
to a generic, constant and reduced number of well behaved steps; 
({\bf 2}) Each NS solve can be brought into the regime of strongly contractive (compressive) identity iteration; 
({\bf 3}) Equation~(\ref{bound}) tightly bounds products about the cube-diagonal for each resolvent $\mat{I}_{\tau, \mu} \left( \mat{s} \right) $;
%%({\bf 4)} Each resolvent cooresponds to contractive volume culls; 
({\bf 4}) Each solve may achieve additional computational and mathematical acceleration  
through generic optimisation;
({\bf 5}) Advanced methods for exploiting locality may be developed,
based on the backwards accumulation of persistence data (Section \ref{}).

%, including \em e.g.} 
%scalar optimation of the NS map, the minimization of opperations in mixed $\tt SpAMM$/regularization/convergence protocols, 
%non-deterministic convergence acceleration, and the expoiting temporal locality. 

\section{Locality} 

A premier feature of the $n$-body algorithm is the ability to exploit data locality efficiently.  In the $ijk$ space, 
there are multiple locality principles that can be exploited by $\tt SpAMM$.  Fir
persistence
copy in place, tightness of the bound

curse of dimensionality
Each generic slice may be associated with different {\em temporal locality}


Metric locality is locality with respect to a Euclidean or generalized distance, {\em e.g.} of the basis.  

increasing the Euclidean locality, ``wrings the zeros'' out of the product space for systems with decay.
On moving from locality provided by the Hilbert order (space filling curve) to annealed sollutions to the
end-to-end TSP.    

\subsubsection{Lensing}

A feature of square root iteration with the $\ot$ kernel is localization of the culled octree towards identity iteration, 
$\widetilde{\mat{x}}_k \rightarrow \mat{I}\left( \widetilde{\mat{x}}_{k-1} \right)$.  Towards convergence,  
the product $\widetilde{\mat{y}}_k \ot \widetilde{\mat{z}}_k$ involves the product of large and small eigenvalues, and large and small norms, 
which are recursively brought towards unity along the $i=k$ diagonal.  Likewise, application of the NS map, Eq.~(\ref{cannonical}),  tend towards 
reflection about the $ijk$ cube-diagonal.   Because the $\tt SpAMM$ error obeys the multiplicative Cauchy-Schwarz bound, Eq.~(),  the 
cooresponding culled-octree can likewise follow the $i=j$ plane about the $ijk$ cube-diagonal, resolving the {\em relative} error in identity to within $\tau$.    This effect is shown in Figure \ref{x_lensing}.   
We call this identity related,  plane-wise concentration of the culled octree  about the cube-diagonal {\em lensing}.  

Lensing is an algebraic localization offering compression beyond  , 

complexity reduction relative to the naive (full) volume of the cube,
and also relative to sparsification strategies that preserve only absolute errors, as in Eq.~\ref{sparseapprox}.  
The lensed task space offers an enhanced locality of reference, and may also afford fast methods 
with costs approaching an in-place scalar multiply and copy, {\em e.g.} as $h_\alpha \rightarrow \mat{I}$ in Eq.~\ref{cannonical}.
Our thesis is that many problems in physical and information sciences can be brought to this lensed state, {\em e.g.} through preconditioning
as described here, and maintained as the NS residual is brought to a higher level of precision with a more complete $\ot$, 
and also with respect to an outer simulation loop, {\em e.g.} cooresponding to time iteration.

 \begin{figure}[h] \label{markofzorro}
 \fbox{ \includegraphics[width=1.in,trim={11.3cm 0cm 11.3cm 0cm},clip]{tube_dual_c6_x128_b64/x_19_scene1.png}} 
 \fbox{ \includegraphics[width=2.in,trim={2cm 0cm 10cm 0cm},clip]{tube_dual_c6_x128_b64/x_19_scene2.png}} 
 \caption{Views of the $\tau =0.03$ sign occlusion surface, for the 
 128x~u.c.~nanotube, at $\sim {14k \times 14k}$ and $\kappa(\mat{s})=10^6$. 
 This surface envelopes the $ijk$ volume of the $\ot$ kernel,  
 cooresponding to the unscaled dual iteration step $\mmat{x}_{19} \leftarrow \mmat{y}_{19} \ot \mmat{z}_{19} $ at $b=64$, $\tau=0.03$ and
 $\tau_y=10^{-3} \, \tau $.  The first pannel looks straight down the cube-diagonal $i=j=k$, from the upper bound towards (1,1,1).
 Remarkably, this surface forms an elongated $\times$, closely following intersection of the $i=j$  and $i=k$ planes 
 along the cube-diagonal. The second pannel looks along the cube-diagonal, with the upper bound at upper left, and (1,1,1) at lower right.}
 \end{figure}

In this section, we present  numerical experiments that highlight the effects of 
ill-conditioning, dimensionality, and the stability of different first order NS approaches to iteration with $\tt SpAMM$. 
We turn first to complexity reduction for $\ot$ in the basin of stability,  where we find a novel, compressive 
effect in the product octree.  This effect is shown in Fig.~\ref{markofzorro},  
for unscaled, inverse square root duals iteration, Eqs.~(\ref{dualsiteration}), on the 3,3 carbon 
nanotube metric at $\kappa=10^6$.  

In this example, the $\tt SpAMM$ octree culled from the $ijk$-cube is outlined by its occlusion surface, enclosing 
a volume that closely follows the $i=j$ and $i=k$ planes, forming an $\times$.  The banded distribution
of large norms along  matrix diagonals leads to cube-diagonal dominance, with plane-following 
a consequence of moderate ill-conditioning,  large norms along the plane-diagonals and their overlap in $ijk$
via the multiplicative bound, Eq.~(\ref{bound}). The tightness of this bound, and the compression gained relative
to  methods that control only the absolute error, {\em e.g.} as given by Eq.~(\ref{sparseapprox}), will hopefully
be quantified in future work. 

\subsubsection{Dimensionality}

\subsubsection{Strong Metric Locality}

unscaled, with hilbert order

unscaled, with salesman's order

\subsubsection{Misc}

\section{Conclusions and Outlook}

Reflecting about the cube diagonal is copy in place.  Cooresponds to lensing.

These  synnergistic effects can be combined, in place, with additional tecniques for 
accelerated computaion.

%%eg vs row-col picture.  Example of exact exchange w/DBSR

\bibliography{MatrixFunctions}

\end{document}
