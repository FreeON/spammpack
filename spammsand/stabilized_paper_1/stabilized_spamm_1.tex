\documentclass[letterpaper,twocolumn,amsmath,amsfont,amssymb,english,aps,jcp,preprintnumbers,groupaddress,nofootinbib,tightenlines,floatfix]{revtex4}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{amsthm}

%\documentclass[aps,prb,letterpaper,twocolumn,nofootinbib,showkeys]{revtex4-1}
%\documentclass[aps,amssymb,prl,letterpaper,twocolumn,nofootinbib,showkeys]{revtex4-1}

%\usepackage[backend=bibtex]{biblatex}

%    backend=biber,
%    style=authoryear,
%    natbib=true,
%    sortlocale=en_US,
%    url=false,
%    doi=true,f
%    eprint=false
%]{biblatex}
%\usepackage{hyperref}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\mmat}[1]{\widetilde{\boldsymbol{#1}}}
\newcommand{\matT}[1]{\boldsymbol{#1}^\dagger}
\newcommand{\ot}{  {\scriptstyle \otimes}_{ \tau } }
\newcommand{\ots}{ {\scriptstyle \otimes}_{ \! \tau_s } }
\newcommand{\oto}{ {\scriptstyle \otimes}_{ \! \tau_0 } }
\newcommand{\otm}{ {\scriptstyle \otimes}_{ \! \tau_m } }
\newcommand{\otmm}{ {\scriptstyle \otimes}_{ \! \tau_{m-1}}}

\newtheorem{thm}{\protect\theoremname}
  \theoremstyle{plain}
  \newtheorem{lem}[thm]{\protect\lemmaname}
  \theoremstyle{remark}
  \newtheorem{rem}[thm]{\protect\remarkname}
  \theoremstyle{plain}
  \newtheorem{prop}[thm]{\protect\propositionname}

  \providecommand{\lemmaname}{Lemma}
  \providecommand{\propositionname}{Proposition}
  \providecommand{\remarkname}{Remark}

\providecommand{\theoremname}{Theorem}


%\hypersetup{pdftitle={FreeON Project Report 1}}
%\hypersetup{pdfauthor={Matt Challacombe and Nicolas Bock}}
%\hypersetup{pdfsubject={A SpAMM Single Newton Schulz Preconditioner: Fighting Error with Error}}

%\bibstyle{aipnum4-1}

\begin{document}

\title{An $N$-Body Solver for Square Root Iteration}

\author{Matt Challacombe}
\email{matt.challacombe@freeon.org}
\homepage{http://www.freeon.org}

\author{Terry Haut}
\email{haut@lanl.gov}

\author{Nicolas Bock}
\email{nicolasbock@freeon.org}
\homepage{http://www.freeon.org}

\affiliation{Theoretical Division, Los Alamos National Laboratory}

\begin{abstract}
We develop the Sparse Approximate Matrix Multiply $n$-body solver for first order Newton Schulz iteration of the 
matrix square root and inverse square root.
The solver performs an $n$-body occlusion-cull, yeilding a bounded relative error in the matrix-matrix product
and reduced complexity for problems with structured metric decay.  This complexity reduction cooresponds to 
the hierarchical resolution of algebraic structures within recursive volume of the product, a consequence of 
metric locality.  For square root iteration, strongly localized sub-volumes are  culled about 
plane-diagonals and along the cube-diagonal,  cooresponding to resolution of the identity.  

The main contributions of this paper are bounds on the $\tt SpAMM$ product and 
demonstration of a new algebraic locality that develops in these sub-volumes with strongly 
contractive identity iteration. This contraction cooresponds to the deflation of sub-volumes 
onto plane diagonals of the resolvent, and to a stronger bound on the $\tt SpAMM$ product.  

Also, we carry out a first order Fr\"{e}chet analyses for single and 
dual channel instances of the square root iteration, and look at bifurcations due to ill-conditioning and a 
too-agresive $\tt SpAMM$ approximation.  Then, we show that extreme $\tt SpAMM$ approximations and strongly contractive 
identity iteration can be achieved through iterated regularization, and demonstrate the potential for orders of magnitude 
acceleration with product representation of the inverse factor.   

\end{abstract}

\maketitle
\section{Introduction}
In many areas of application, long range, high value correlations lead to matrix equations with decay properties.
By decay, we mean an approximate inverse relationship between matrix elements and an associated distance; 
this may be a simple inverse relationship between matrix elements and the Cartesian distance between
corresponding support functions, or it may involve a non-Euclidean distance, 
 {\em  e.g.}~a generalized measure between character strings in a training library \cite{}. 

Matrix equations with decay have history and recent development in the statistics and statistical physics litterature
\cite{penrose1974,voit00, Anselin2003, Hardin2013, Krishtal2014}.   Also recently, methods for meshfree interpolation 
are demonstrating remarkable predictive power through delocalized correlations and cooresponding 
ill-conditioned matrix equations with extreme slow decay \cite{}, a problem equivalent to the LCAO Gaussian basis problem 
in quantum chemistry \cite{}.  Generally, local support functions are correlated 
through Lowdin's symmetric orthogonalization based on the matrix inverse square root \cite{Lowdin56, naidu11}, 
yeilding representation independent matrix equations. 
In electronic structure, important long-range correlations manifest in slow decay properties 
of the gap shifted matrix sign function, as projector of the effective Hamiltonian (Fig.~\ref{figure1}).  
Both of these matrix problem with decay, the matrix sign function and the matrix inverse square root, 
are related by Higham's identity:
\begin{equation}
\rm{sign} \left( \begin{bmatrix} 0 & \mat{s}      \\ \mat{I}       & 0\end{bmatrix} \right)  =
                 \begin{bmatrix} 0 & \mat{s}^{1/2} \\ \mat{s}^{-1/2} & 0\end{bmatrix}  .
\end{equation}
The theory and computation of these matrix functions is given in Higham's reference \cite{Higham08}.

\begin{figure}[h]\label{figure1}
 \includegraphics[width=3.5in]{decay_picture.png}
  \caption{Examples from electronic structure of decay for the
    spectral projector (gap shifted sign function) with respect to
    local (atomic) support.  Shown is decay for systems with
    correlations that are short (insulating water), medium
    (semi-conducting 4,3 nanotube), and long (metalic 3,3 nanotube)
    ranged, from exponential (insulating) to algebraic (metallic). }
\end{figure}


A well conditioned matrix $\mat{s}$ may often correspond to matrix
sign and inverse square root functions with rapid exponential decay,
and be amenable to {\em ad hoc} matrix truncation or ``sparsification'', 
$\bar{\mat{s}} = \mat{s}+ \mat{\epsilon}^{\mat{s}}_\tau$,
where $\mat{\epsilon}^{\mat{s}}_\tau$ is the error introduced
according to some criterion $\tau$.  
%Supporting this approximation are useful bounds to matrix function elements \cite{Benzi99b, }.
The criterion $\tau$ might be a drop-tolerence,
$\epsilon^{\mat{s}}_{\tau} = \{-s_{ij}*\hat{\mat{e}}_i \, | \, |s_{ij}|<\tau \}$,
a radial cutoff,
$\epsilon^{\mat{s}}_{\tau} = \{-s_{ij}*\hat{\mat{e}}_i \, | \, \lVert \mat{r}_i - \mat{r}_j \rVert > \tau \}$,
or some other approach to truncation, perhaps involving a sparsity
pattern chosen {\em a priori} for computational expedience.
Then, the sparse general matrix-matrix multiply ($\tt{SpGEMM}$)
\cite{Gustavson78, Toledo97, challacombe00, bowler00} may be employed, yielding fast
solutions for multiplication rich iterations with fill-in modulated by truncation.
%These and related incomplete/inexact approaches to the computation of
%sparse approximate matrix functions often lead to ${\cal O}(n)$
%algorithms, finding wide use in technologically important
%preconditioning schemes, the information sciences, electronic
%structure and many other disciplines. 
Comprehensive surveys of these
methods in the numerical linear algebra are given by Benzi
\cite{Benzi99,Benzi02}, and by Bowler \cite{Bowler12} and Benzi \cite{Benzi13}
for electronic structure.

Often however, matrix truncation is ineffective for ill-conditioned problems, 
because of slow decay, and because of increased numerical sensitivities to poorly 
controled (absolute) truncation errors, {\em e.g.}~in the matrix-product:
\begin{equation} \label{sparseapprox}
\overline{ \mat{a} \cdot \mat{b} }\; = \; \mat{a}\cdot\mat{b} \; +\; \mat{\epsilon}^{\mat{a}}_\tau \cdot \mat{b} \;+\;
 \mat{a} \cdot \mat{\epsilon}^{\mat{b}}_\tau  \; + \;   {\mathcal O}(\tau^2) \, .
\end{equation}
An alterative approach is to find a reduced rank approximation closed under the opperations of interest \cite{HSS}. 
However, compression to a reduced rank may be expensive if the rank is not much much smaller than the dimension.
Both of these methods, truncation and rank reduction, are focused on matrix data as the target for compresion.
In this contribution, our target for compression is instead the matrix product itself.  For problems with
decay, we show that an underlying metric locality, together with a new form of algebraic locality, 
can lead to complexity reduction under contractive iteration and the $n$-body occlusion-cull. The organization of 
this paper follows.

\tableofcontents

\section{Sparse Approximate Matrix Multiplication ($\tt SpAMM$)}

In this contribution, we consider an $N$-body approach to the
approximation of matrix functions with decay, based on the quadtree data structure \cite{wise, samet}
\begin{equation}
\mat{a}^i = \begin{bmatrix} \,  \mat{a}^{i+1}_{00} \, & \,  \mat{a}^{i+1}_{01} \,  \\[0.2cm]  \, \mat{a}^{i+1}_{10} \,  & \,\mat{a}^{i+1}_{11} \, \end{bmatrix} \, ,
\end{equation}
and orderings that are locality preserving \cite{}.  Orderings that
preserve data locality are well developed and {\bf \em generic} in the database theory
\cite{}, providing fast spatial and metric queries.  Locality
enabled, fast data access is central to  $b$-body approximations
\cite{}, and an important problem for enterprise \cite{} and runtime
systems \cite{}, moreso with memory hierarchies becoming increasingly
asynchronous and decentralized \cite{cache}.  For matrices with
decay, orderings that preserve locality lead to blocked-by-magnitude
matrix structures with well segregated neighborhoods, inhabited by
matrix elements of like size, and efficiently resolved by the quadtree
data structure \cite{}.

\subsection{Occlusion-Cull }

The Sparse Approximate Matrix Multiply ($\tt SpAMM$) carries out occlusion-culling to find only the
most important sub-volumes in an approximate matrix product. 
$\tt SpAMM$ has evolved from a row-coloumn oriented skipout mechanism within the
BCSR and DBCSR structures \cite{}, to hierachical approaches based on the quadtree and related to 
the occlusion-culling found in advanced mechanics and graphics methodologies \cite{}, 
with occlusion the avoidence of unessesary tree-work and culling the collection of significant tasks. 
Here, we ammend the $\tt SpAMM$ occlusion-cull with the recursion:
\begin{widetext}
\begin{equation}
\mat{a}^{i} \ot \mat{b}^{i} =
\left\{
        \begin{array}{ll}
                 \emptyset \quad \tt{if}\quad \lVert \mat{a}^i \rVert \lVert \mat{b}^i \rVert < \tau \lVert a \rVert \lVert b \rVert \\[0.2cm]
                 \mat{a} ^i \cdot \mat{b}^i \quad  \tt{if}(i=\tt{leaf}) \\[0.2cm]
\begin{bmatrix} \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{00} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{10} \; , \; &
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11}  \\[0.2cm]
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11} \; , \; &
                \mat{a}^{i+1}_{00} \ot \mat{b}^{i+1}_{01} +\mat{a}^{i+1}_{01} \ot \mat{b}^{i+1}_{11}
\end{bmatrix}  \quad \tt{else}
                \end{array}
              \right.  \, ,
\end{equation}
\end{widetext}
which bounds the relative occlusion error
\begin{equation}\label{bound}
\frac{\lVert \mat{\Delta}^{a \cdot b}_{\tau} \rVert}{n^2 }  \, \leq \, \tau \, \lVert \mat{a} \rVert  \,  \lVert \mat{b} \rVert \, ,
\end{equation}
that occurs in the approximate product
\begin{equation}
\widetilde{\mat{a}\cdot \mat{b}} \,  \equiv \, \mat{a} \ot \mat{b} \,
  = \, \mat{a} \cdot \mat{b} + \mat{\Delta}^{a \cdot b}_{\tau} \, ,
\end{equation}
where $\lVert \cdot \rVert \equiv \lVert \cdot \rVert_F$ is a sub-multiplicative norm \cite{}.

\subsection{Bound}

We now prove (\ref{bound}).

\begin{prop}
\label{lem:SpAMM mult, prop}
Let $\tau_{A,B} = \tau \| A \| \| B \| $. Then for each $i,j$,

\[
\left|\left(A\otimes_{\tau}B\right)_{ij}-\left(A\cdot B\right)_{ij}\right|\leq n\tau_{A,B},
\]
and
\[
\left\Vert A\otimes_{\tau}B-A\cdot B\right\Vert _{F}\leq n^{2}\tau_{A,B}.
\]
\end{prop}

\begin{proof}


We first show the following technical result: it is possible to choose $\alpha_{lij}\in\left\{ 0,1\right\} $ such that 

\begin{equation}
\left(A\otimes_{\tau}B\right)_{ij}=\sum_{l=1}^{n}A_{il}B_{lj}\alpha_{lij},\label{eq:spamm form, lemma}
\end{equation}
In addition, if $\alpha_{lij}=0$, then \textup{$\left|A_{il}\right|\left|B_{lj}\right|<\tau_{A,B}$. }. To show this, we use 
induction on the number $k_{\max}$ of levels. 

First, if $k_{\max}=0$,
\[
A\otimes_{\tau}B=\begin{cases}
0 & \,\,\text{if}\,\,\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}<\tau_{A,B},\\
A\cdot B & \,\,\text{else}.
\end{cases}
\]
Therefore, $A\otimes_{\tau}B$ is of the form (\ref{eq:spamm form, lemma})
with either all $\alpha_{lij}=0$ or all $\alpha_{lij}=1$. Moreover,
if $\alpha_{lij}=0$, then $\left|A_{il}\right|\left|B_{lj}\right|\leq\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}<\tau_{A,B}$. 

Now assume that the claim holds for $k_{\max}-1$. We show that it
holds for $k_{\max}$. Indeed, if $\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}<\tau_{A,B}$,
we have that $A\otimes_{\tau}B=0$, which is of the form (\ref{eq:spamm form, lemma})
with all $\alpha_{lij}=0$. Also, if $\alpha_{lij}=0$, then $\left|A_{il}\right|\left|B_{lj}\right|<\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}<\tau_{A,B}$.

Now assume that $\left\Vert A\right\Vert _{F}\left\Vert B\right\Vert _{F}\geq\tau_{A,B}$.
Then
\[
A\otimes_{\tau}B=\left(\begin{array}{cc}
A_{11}\otimes_{\tau}B_{11}+A_{12}\otimes_{\tau}B_{21} & A_{11}\otimes_{\tau}B_{12}+A_{12}\otimes_{\tau}B_{22}\\
A_{21}\otimes_{\tau}B_{11}+A_{22}\otimes_{\tau}B_{21} & A_{21}\otimes_{\tau}B_{21}+A_{22}\otimes_{\tau}B_{22}
\end{array}\right).
\]
We need to consider four cases: $i\leq n/2$ and $j\leq n/2$, $i>n/2$
and $j>n/2$, $i>n/2$ and $j\leq n/2$, and, finally, $i>n/2$ and
$j>n/2$. Since the analysis is similar for all four cases, we only
consider $i\leq n/2$ and $j\leq n/2$. We have that 
\begin{eqnarray*}
\left(A\otimes_{\tau}B\right)_{ij} & = & \left(A_{11}\otimes_{\tau}B_{11}+A_{12}\otimes_{\tau}B_{21}\right)_{ij}\\
 & = & \sum_{l=1}^{n/2}\left(A_{11}\right)_{il}\left(B_{11}\right)_{lj}\alpha_{lij}^{(1)} + \\
 & & \,\,\,\,\,\,\,\, \sum_{l=1}^{n/2}\left(A_{12}\right)_{il}\left(B_{21}\right)_{lj}\alpha_{lij}^{(2)}\\
 & = & \sum_{l=1}^{n}A_{il}B_{lj}\alpha_{lij},
\end{eqnarray*}
where we used the induction hypothesis in the second equality.

Now suppose that $\alpha_{lij}=0$ for some $l$. Then $\tilde{\alpha}_{lij}^{(1)}=0$
if $l\leq n/2$ or $\tilde{\alpha}_{l-n/2,ij}^{(2)}=0$ $l>n/2$.
If, e.g., $\tilde{\alpha}_{l-n/2,ij}^{(2)}=0$, then $\left|A_{il}\right|\left|B_{lj}\right|=\left|\left(A_{12}\right)_{i,l-n/2}\right|\left|\left(B_{21}\right)_{l-n/2,j}\right|<\tau_{A,B}$,
where we used the induction hypothesis in the final inequality. The
analysis for $l\leq n/2$ is similar, and the claim
follows.

We can now finish the proof of Proposition~\ref{lem:SpAMM mult, prop}. Indeed, by (\ref{eq:spamm form, lemma}),
\begin{eqnarray*}
\left|\left(A\otimes_{\tau}B\right)_{ij}-\left(A\cdot B\right)_{ij}\right| & \leq & \sum_{l=1}^{n}\left|A_{il}B_{lj}\right|\left|\alpha_{lij}-1\right|\\
 & = & \sum_{\alpha_{lij}=0}\left|A_{il}B_{lj}\right|.
\end{eqnarray*}
In addition, if $\alpha_{lij}=0$, then $\left|A_{il}B_{lj}\right|<\tau_{A,B}$
and the lemma follows.


\end{proof}

\subsection{Related Research} 

$\tt SpAMM$ is perhaps most closely related to the Strassen-like branch of fast matrix multiplication 
\cite{springerlink:10.1007/BF02165411,Ballard2014}.
In the Strassen-like approach, disjoint volumes in (abstract) tensor intermediates are omitted recursively \cite{}.  
In the $\tt SpAMM$ approach to fast multixplication, the numerically most significant volumes in 
 n\"{a}ive ($ijk$) tensor intermediates are culled, with error bounded by Eq.~(\ref{bound}).  
This bound makes $\ot$ a {\em stable} form of fast multiplication,  as explained by Demmel, Dumitriu and Holz (DDH; Ref.~\cite{Demmel07}).

$\tt SpAMM$ is a $n$-body method for fast matrix multiplication, related to the 
generalized methods popularized by Grey \cite{Gray2001,Gray2003}. In our development,
generalization reflects the {\em genericity} \cite{} of recursive data access \cite{}, 
enabling range querries, metric querries, higher dimensional querries and so on, with common frameworks, 
structures and runtimes.  So far, we have prototyped $n$-body solvers for the five mainstay solvers in modern electronic structure theory, 
involving Fock exchange \cite{}, semi-local exchange-correlation functionals \cite{}, 
the Hartree (Coulomb) interaction \cite{}, matrix sign function \cite{} and the matrix inverse square root (this work).
This contribution is cornerstone for the simplification and evolution of these solvers.  

Top-down $n$-body recursion and breadth-first map-reduction may be viewed as two sides of 
the same problem \cite{Aluru}.  Emergent data frameworks and 
functional programming languages that support generic recursion and map skelitinizations may enable 
early exploitation of commodity (decentralized) concurence by scientific $n$-body solvers, 
as well as software sustainability \cite{softwaresustainanbilty}.
For centralized, distributed architectures, $n$-body methods offer well established protocols for turning spatial and metric locality into 
data and temporal locality \cite{}.  Recently, Driscoll {\em et. al} showed perfect strong scaling and communication optimality 
for pairwise $n$-body methods \cite{Driscoll13}.   Bridging the gap between 
$n$-body solver and fast matrix multiplication, we recently demonstrated strong scaling for fast matrix multiplication ($\tt SpAMM$) \cite{}.  

This work offers a data local alternative to fast non-deterministic methods for sampling the product, 
which include sketching \cite{Sarlos2006,Drineas2006,Mahoney2012,Pagh2013,Sivertsen2014,Woodruff2015},
joining \cite{Mishra92,Hoel94,Jacox03,Chen07,Amossen09,Lieberman08,Kim09}, 
sensing \cite{} and probing \cite{}.  These  methods involve a wheighted (probablistic) 
and on the fly sampling, with the potential for complexity reduction in the case of random distributions. 
$\tt SpAMM$ also employs on the fly wheighted sampling,  but with 
compresion through locality, brought about by algebraic correlations (towards identity) and also in the
metric structure, through strong Euclidean locality.

Methods that achieve compression in the stream of product intermediates are different from reduced rank algorithms that achieve 
matrix compression in a step that preceeds multiplication (seperability)  \cite{}.   However, matrix compressions are 
generally compatible with the quadtree, as are additional fast (generalized) solvers that add complex functionality ({\em e.g.} 
in electronic structure theory \cite{}).  Thus, generality 
and interoperability may enable deeply layered, thin and generic solvers easy access to in place data. 
Further, language support may provide simple (skeletonized) frameworks for generic recursion, 
offering opportunities to greatly simplify codebase at the systems level, 
lowering barriers to entry and enhancing concurence (in the Erlang sense). 


Finally, previous work on the scaled NS iteration has heavily influenced this work.  Formost is 
Higham, Mackey, Mackey and T (HMMT; Ref.~\cite{}) masterwork on convergence of NS iteration under all groups,
wherein HMMT also develop Fr\"{e}chet analyses for single square root iteration at the fixed point.
Also, important inspiration comes from Chen and Chow's \cite{} approach to scaled NS iteration for ill-conditioned problems \cite{}, 
and from the Helgaker groups work on NS iteration, whose notation we follow in part \cite{}.  

% which leads to a non-associative algebra and error flows with
% properties of the Lie bracket
% \begin{equation} \label{braket}
% \widetilde{\left[ \mat{a} , \mat{b} \right]} \equiv \mat{a} \ot \mat{b}-\mat{b} \ot \mat{a}
% =  \left[ \mat{a} , \mat{b} \right]
% + \mat{\Delta}^{a\cdot b}_{\tau} -\mat{\Delta}^{b\cdot a}_{\tau} \,.
% \end{equation}

% Finally, the

\section{First Order Newton-Shulz Iteration}

There are two common, first order NS iterations; the sign iteration
and the square root iteration, related by the square, $\mat{I}\left(
\cdot \right)= {\rm sign}^2\left( \cdot \right) $.  These equivalent
iterations converge linearly at first, then enter a basin of stability
marked by super-linear convergence.  

\subsection{Sign Iteration}

For the NS sign iteration, this basin is marked by a behavioral change
in the difference $\delta \mat{X}_k = \widetilde{\mat{X}}_k -\mat{X}_k
= {\rm sign} \left(\mat{X}_{k-1}+\delta \mat{X}_{k-1} \right) -{\rm
  sign} \left(\mat{X}_{k-1} \right)$, where $\delta \mat{X}_{k-1}$ is
some previous error.  The change in behavior is associated with the
onset of idempotence and the bounded eigenvalues of ${\rm sign}'\left(
\cdot \right)$, leading to stable iteration when ${\rm sign}'\left(
\mat{X}_{k-1} \right) \delta \mat{X}_{k-1} < 1 $.  Global perturbative
bounds on this iteration have been derived by Bai and Demmel
\cite{Bai98usingthe}, while Byers, He and Mehrmann \cite{} developed
asymptotic bounds.  The automatic stability of sign iteration is a
well developed theme in Ref.\cite{Higham08}.

\subsection{Square Root Iteration}

In this work, we are concerned with resolution of the identity \cite{}
\begin{equation}
\mat{I} \left( \mat{s} \right) =\mat{s}^{1/2} \cdot \mat{s}^{-1/2} \, ,
\end{equation}
and its low-complexity computation with fast methods.  

Starting with eigenvalues rescaled to the domain $(0,1]$ with the easily obtained 
largest eigenvalue,   $\mat{s} \leftarrow \mat{s}/s_{N-1}$, and with $\mat{z}_0=\mat{I}$ and 
$\mat{x}_0=\mat{y}_0=\mat{s}$, the cooresponding canonical,  ``dual'' channel square root iteration is:
\begin{eqnarray}\label{cannonical}
\mat{y}_k &\leftarrow& h_\alpha \left[ \mat{y}_{k-1} \cdot \mat{z}_{k-1} \right] \cdot \mat{y}_{k-1}  \nonumber \\
\mat{z}_k &\leftarrow& \mat{z}_{k-1} \cdot h_\alpha \left[ \mat{y}_{k-1} \cdot \mat{z}_{k-1} \right] \; ,
\end{eqnarray}
converging as ${\mat{y}}_k \rightarrow \mat{s}^{1/2}$, ${\mat{z}}_k \rightarrow \mat{s}^{-1/2}$ and
${\mat{x}}_k \rightarrow {\mat{I}}$, with eigenvalues aggregated towards 0 or 1 by 
the NS map $h_\alpha[\mat{x}]=\frac{\sqrt{\alpha}}{2} \left(3-\alpha \mat{x} \right)$ \cite{}.  
As in the case of sign iteration, this cannonical iteration was shown by Higham, Mackey, Mackey and
Tisseur \cite{Higham2005} to remain strongly bounded in the superlinear regime, by idempotent Frechet derivatives about the fixed point
$\left(\mat{s}^{1/2},\mat{s}^{-1/2}\right)$, in the direction $\left(
\delta \mat{y}_{k-1} , \delta \mat{z}_{k-1} \right)$:
\begin{eqnarray}
\delta \mat{y}_k &=& \frac{1}{2} \delta \mat{y}_{k-1} - \frac{1}{2} \mat{s}^{1/2} \cdot \delta \mat{z}_{k-1} \cdot \mat{s}^{1/2} \\
\delta \mat{z}_k &=& \frac{1}{2} \delta \mat{z}_{k-1} - \frac{1}{2} \mat{s}^{-1/2} \cdot \delta \mat{y}_{k-1} \cdot \mat{s}^{-1/2} \;.
\end{eqnarray}
In addition to the dual channel instance, we also consider the ``single'' channel version of square root iteration,
\begin{eqnarray}\label{single}
\mat{z}_k &\leftarrow& \mat{z}_{k-1} \cdot h_\alpha \left[ \mat{x}_{k-1} \right] \; , \nonumber \\
\mat{x}_k &\leftarrow&  \mat{z}^\dagger_{k} \cdot \mat{s} \cdot \mat{z}_{k-1} \; .
\end{eqnarray}

\subsection{Mapping}

$\ot$ is also a map.  atm, stocastic unrpredictable, but really deterministic.  excersize Control

In this contribution, we consider another aspect of convergence,
namely the (hopefully) linear approach towards stability of the
iteration
\begin{equation}
\widetilde{\mat{x}}_k \leftarrow
 \widetilde{\mat{y}}_k \left( \widetilde{\mat{x}}_{k-1} \right)
\, \ot \, \widetilde{\mat{z}}_k \left( \widetilde{\mat{x}}_{k-1} \right) \, ,
\end{equation}
made difficult by ill-conditioning and a sketchy $\ot$.


Then, using
\begin{equation}
  h_\alpha \left[ \widetilde{\mat{x}}_{k-2} \right]
=  h_\alpha \left[ \mat{x}_{k-2} \right] +  h'_\alpha  \delta \mat{x}_{k-2} \, 
\end{equation}


Initially, $h'_\alpha$ at the smallest eigenvalue $x_0$ controls the
rate of progress towards idempotence.  As recently shown by Jie and
Chen \cite{Chen2014}, for very ill-conditioned problems, a factor of
two reduction in the number of NS steps can be achieved by choosing
$\alpha \sim 2.85$, which is at the edge of stability.  As argued by
Pan and Schreiber \cite{Pan1991}, Jie and Chen \cite{Chen2014},
switching or damping the scaling factor towards $\alpha=1$ at
convergence is important, shifting emphasis away from the behavior of
$x_0$ towards {\em e.g.}~$x_i \in [0.01,1]$, emphasizing overall
convergence of the broad distribution \cite{Pan and Scriber}.  In an
approximate algebra like $\tt SpAMM$, the potential for eigenvalues to
fluctuate out of the domain of convergence must be considered.  This
is addressed in Section \ref{}.


\section{Error Flows in Square Root Iteration}

\subsection{Stability}

There are a number of equivalent instances of the square root iteration, related by commutations and transpositions. 
Nominally, $\mat{y}^{\rm dual}$ is equivalent to $\mat{y}^{\rm single}_k \equiv \mat{z}^\dagger_{k} \cdot \mat{s}$.  
However, with ill-conditioning and in only double precision, these two instances may diverge rapidly due to compounding  errors.
In the case of the dual iteration, the $\widetilde{\mat{y}}^{\rm dual}_k$ channel does not retain contact with the eigenvectors of $\mat{s}$, 
and can diverge easily from this basis. In the single instance, conection with $\mat{s}$ is retained at each step, but at the price 
the update involving a broad spectral resolution, {\em e.g.} involving $\mat{s}^{-1/2} \cdot \mat{s}$  at convergence. 

Stability in the square root iteration is determined to first order by the differential 
\begin{equation} \label{firstorderdual}
\delta \mat{x}_k = \,  { \mat{x}}_{\delta \widehat{\mat{y}}_{k-1}}  \, {\scriptstyle \times} \, \delta y_{k-1}
                 \, + \,  { \mat{x}}_{\delta \widehat{\mat{z}}_{k-1}}  \, {\scriptstyle \times} \, \delta z_{k-1}  \, + {\cal O}(\tau^2) 
\end{equation}
which must remain bounded below 1 to avoid divergence.   The cooresponding Fr\"{e}chet derivatives are
\begin{equation}
  \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}
= \lim_{\tau \rightarrow 0} \slantfrac{ \mat{x} \left( \mat{y}_{k-1} + \tau \delta \widehat{\mat{y}}_{k-1} ,  {\mat{z}}_{k-1}  \right)
                                     -\mat{x}_k    }{\tau} 
 \end{equation}
and
 \begin{equation}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} = \lim_{\tau \rightarrow 0}
\slantfrac{ \mat{x} \left( {\mat{y}}_{k-1} , \mat{z}_{k-1} +\tau  \delta \widehat{\mat{z}}_{k-1} \right) - \mat{x}_k   }{\tau}  \, , 
 \end{equation}
along unit directions of the previous errors $\delta \widehat{\mat{y}}_{k-1}$ and $\delta \widehat{\mat{z}}_{k-1}$, by an ammount
determined by the displacements $\delta y_{k-1} = \lVert \delta \mat{y}_{k-1} \rVert$  and  $\delta z_{k-1}=\lVert \delta \mat{z}_{k-1} \rVert$.
In the single instance, we have simply:
\begin{equation} \label{firstordersingle}
\delta \mat{x}_k = \,  { \mat{x}}_{\delta \widehat{\mat{z}}_{k-1}}  \, {\scriptstyle \times} \, \delta z_{k-1}  \, + {\cal O}(\tau^2)  \, .
\end{equation}

This formulation makes plain changes about the resolvent, seperating orientational effects 
of the directional derivatives, set mostly by the underlying exact linear algebra, from 
changes to error displacements, which involve both the action of derivatives on previous errors, $k-1$, as well as 
the $\tt SpAMM$ occlusion errors local to NS step $k$.


\subsection{Fr\"{e}chet Derivatives}
%\subsubsection{Dual}
 
In the dual instance, Fr\"{e}chet derivatives occuring in Eq.~(\ref{firstorderdual}) are:

% \begin{equation}
%   \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}
% =   \mat{y}_{\delta \widehat{ \mat{y}}_{k-1}} \cdot \mat{z}_{k}  + \mat{y}_{k}  \cdot \mat{z}_{\delta \widehat{ \mat{y}}_{k-1}}
%  \end{equation}
% and
%  \begin{equation}
%  \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} =
% \mat{y}_{\delta \widehat{ \mat{z}}_{k-1}} \cdot \mat{z}_{k}  + \mat{y}_{k}  \cdot \mat{z}_{\delta \widehat{ \mat{z}}_{k-1}} \, .
%  \end{equation}


% For $\mat{x}_{\delta \widehat{ \mat{y}}_{k-1}}$ we have
% \begin{multline}
%  \mat{y}_{\delta \widehat{ \mat{y}}_{k-1}} = h_\alpha \left[ \mat{x}_{k-1} \right] \cdot \delta \widehat{\mat{y}}_{k-1} \\
%                                      + h'_\alpha \cdot \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1} \cdot \mat{y}_{k-1}
% \end{multline}
% and
% \begin{equation}
%  \mat{z}_{\delta \widehat{ \mat{y}}_{k-1}} =  \mat{z}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1}
% \end{equation}
% yeilding
% Also, for $\mat{x}_{\delta \widehat{ \mat{z}}_{k-1}}$ we have
% \begin{equation}
%  \mat{y}_{\delta \widehat{ \mat{z}}_{k-1}} =  {\mat{y}}_{k-1} \cdot  h'_\alpha \delta \widehat{ \mat{z}}_{k-1} \cdot  \mat{y}_{k-1}
% \end{equation}
% and
% \begin{multline}
%  \mat{z}_{\delta \widehat{ \mat{z}}_{k-1}} = \delta \widehat{\mat{z}}_{k-1} \cdot   h_\alpha \left[ \mat{x}_{k-1} \right] \\
%                                      + \mat{z}_{k-1} \cdot {\mat{y}}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{z}}_{k-1}  \; ,
% \end{multline}
% yeilding

\begin{multline}\label{dxdy}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} =  {\mat{y}}_{k-1} \cdot  h'_\alpha \delta \widehat{ \mat{z}}_{k-1} \cdot  \mat{y}_{k-1}  \cdot \mat{z}_{k} \\
\qquad + \mat{y}_k \cdot  \delta \widehat{\mat{z}}_{k-1} \cdot   h_\alpha \left[ \mat{x}_{k-1} \right] \\
\qquad \qquad +  \mat{y}_{k} \cdot  \mat{z}_{k-1} \cdot {\mat{y}}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{z}}_{k-1} \, .
\end{multline}

and 

\begin{multline}\label{dxdz}
  \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}} = h_\alpha \left[ \mat{x}_{k-1} \right]  \cdot \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k} \\
+  h'_\alpha  \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1} \cdot  \mat{y}_{k-1} \cdot  \mat{z}_{k} \\
 + \mat{y}_{k} \cdot \mat{z}_{k-1} \cdot h'_\alpha \delta \widehat{\mat{y}}_{k-1} \cdot \mat{z}_{k-1}  \, .
\end{multline}


Closer to a fixed point orbit,  $\mat{y}_k \cdot \mat{z}_{k-1} \rightarrow \mat{I}$, $\mat{y}_{k-1} \cdot \mat{z}_{k} \rightarrow \mat{I}$,
$h_\alpha \left[ \mat{x}_{k} \right] \rightarrow \mat{I}$ and $h'_\alpha \rightarrow - \frac{1}{2}$ \cite{higham2005}.  Then,

\begin{equation} \label{yorbit}
 \mat{x}_{\delta \widehat{ \mat{y}}_{k-1}} \rightarrow \delta \widehat{\mat{y}}_{k-1} \cdot \left( \mat{z}_k-\mat{z}_{k-1} \right)
\end{equation}
and
\begin{equation} \label{zorbit}
 \mat{x}_{\delta \widehat{ \mat{z}}_{k-1}} \rightarrow \left( \mat{y}_k-\mat{y}_{k-1} \right) \cdot \delta \widehat{\mat{z}}_{k-1} .
\end{equation}
Likewise, in the single channel instance:
\begin{multline}
 \mat{x}_{\widehat{\mat{z}}_{k-1}} \rightarrow  \left(  \mat{z}^\dagger_{k} - \mat{z}^\dagger_{k-1} \right) \cdot \mat{s} \cdot \delta \widehat{\mat{z}}_{k-1} \\
+  \delta \widehat{\mat{z}}^\dagger_{k-1} \cdot  \mat{s}  \cdot \left(  \mat{z}_{k} - \mat{z}_{k-1} \right)  \, .
\end{multline}

About the fixed point then, directional contributions along $\delta \widehat{\mat{y}}_{k-1}$ and $\delta \widehat{\mat{z}}_{k-1}$ are 
shut tightly as  $\mat{x}_{\delta \widehat{\mat{x}}_{k-1}} \rightarrow  \mat{I}$.

\subsection{Displacements}

In this analysis, we've seperated the directional component of the error from its distance, because in addition to the
previous compounding error, each displacement contains also a first order $\tt SpAMM$ source error.  Its simpler to 
consider these effects serpately, at least in this first contribution. 

To understand $\delta \mat{z}_{k-1}$, we partially unwind the approxinate  $\widetilde{\mat{z}}_{k-1}$;
%\begin{equation}
\begin{eqnarray} \label{widetildez}
 \widetilde{\mat{z}}_{k-1} &=&  \widetilde{\mat{z}}_{k-2}  \, \ot \, h_\alpha[\widetilde{\mat{x}}_{k-2}]\\
&=& \Delta^{\widetilde{\mat{z}}_{k-2} \cdot h_\alpha \left[ \widetilde{\mat{x}}_{k-2}\right]}_\tau
+ \widetilde{\mat{z}}_{k-2} \cdot h_\alpha\left[ \widetilde{\mat{x}}_{k-2}\right]
%\end{equation}
\end{eqnarray}
Then, using
\begin{equation}
  h_\alpha \left[ \widetilde{\mat{x}}_{k-2} \right]
=  h_\alpha \left[ \mat{x}_{k-2} \right] +  h'_\alpha  \delta \mat{x}_{k-2} \, 
\end{equation}
and taking $\mat{z}_{k-1}$ from both sides, we find  
%Eq.~(\ref{widetildez}) yeilds 
\begin{multline}
 \delta {\mat{z}}_{k-1} =\Delta^{\widetilde{\mat{z}}_{k-2} \cdot h_\alpha \left[ \widetilde{\mat{x}}_{k-2}\right]}_\tau
\\ +\delta \mat{z}_{k-2} \cdot h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]
+ \mat{z}_{k-2} \cdot h'_\alpha \delta \mat{x}_{k-2}  \, ,
\end{multline}
bounded by 
% \begin{multline}
%  \delta {z}_{k-1} <
% \lVert \mat{z}_{k-2} \rVert \left( \;  \tau \, \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]  \rVert
% + h'_\alpha \delta {x}_{k-2} \right)  \\ +
% \delta {z}_{k-2}  \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2}  \right] \rVert 
% \end{multline}
 \begin{multline}
  \delta {z}_{k-1} <
 \lVert \mat{z}_{k-2} \rVert \left( \tau \, \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]  \rVert
 + h'_\alpha  \delta y_{k-2} \lVert z_{k-2} \rVert \right)  \\ 
 + \delta {z}_{k-2} \left( \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2}  \right] \rVert  + \lVert y_{k-2} \rVert \right) .
 \end{multline}


primary error channels contibuting to $\delta z_{k-1}$ are through the first order $\tt SpAMM$ error
$ \tau \lVert \mat{z}_{k-2} \rVert \lVert h_\alpha \left[\widetilde{\mat{x}}_{k-2} \right]\rVert$
and the volatile term $h'_\alpha  \delta y_{k-2} { \lVert \mat{z}_{k-2} \rVert }^2$.


 
\begin{eqnarray}
\widetilde{\mat{y}}^{\rm single}_{k-1} &=& \widetilde{\mat{z}}^{\dagger}_{k-1} \, \ot \, \mat{s} \\
                                  &=& \mat{\Delta}^{\widetilde{\mat{z}}^\dagger_{k-1} \cdot \, \mat{s}} \, + \,
\left( \widetilde{\mat{z}}_{k-2} \cdot h_\alpha[ \widetilde{\mat{x}}_{k-2}] \right)^\dagger \, \cdot \, \mat{s}
\end{eqnarray}




%  \begin{eqnarray}
% \widetilde{\mat{y}}_{k-1}&=&
% h_\alpha [ \widetilde{\mat{x}}_{k-2}] \, \ots \, \widetilde{\mat{y}}_{k-2}  \\
% &=&  \mat{\Delta}^{  h_\alpha[ \widetilde{\mat{x}}_{k-2}] \cdot \widetilde{\mat{y}}_{k-2} }_{\tau_s}  \, + \,
%      h_\alpha[ \widetilde{\mat{x}}_{k-2}] \cdot \widetilde{\mat{y}}_{k-2}
% \end{eqnarray}
% \begin{multline}
% \delta \mat{y}_{k-1}= \mat{\Delta}^{h_\alpha [ \widetilde{\mat{x}}_{k-2}] \cdot \mat{y}_{k-2} }_\tau  \\ +
% h'_\alpha \delta \mat{x}_{k-2} \cdot \mat{y}_{k-2}
% + h_\alpha [ \widetilde{\mat{x}}_{k-2}] \cdot \delta \mat{y}_{k-2} \, ,
% \end{multline}

corresponding to  basis corruption and controlled by $\ots$, with $\tau_s \ll \tau$. 
As above, we can unwind this sensitive term, to find 
\begin{multline}
\delta y_{k-2} <   \lVert \mat{y}_{k-3} \rVert  \left( \tau_s \lVert h_\alpha [ \widetilde{\mat{x}}_{k-3}] \rVert + h'_\alpha \delta z_{k-3} \right )\\
+  \delta y_{k-3}  \left( \lVert \widetilde{\mat{z}}_{k-3}]  \rVert + \lVert h_\alpha [ \widetilde{\mat{x}}_{k-3}]  \rVert
  \right)
\, .
\end{multline}


\subsection{Single Channel Instance}


\subsection{Bifurcations}

\begin{figure}[h] \label{flow_noscale_dual}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_noscaling/33_nanotube_cond10_noscale_dual.eps}
\caption{Derivatives, displacements and the approximate trace of the unscaled, dual NS iteration for a (3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and $\tau_y=\{10^{-8}, 10^{-9}, 10^{-10}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}


% \begin{figure}[h]
% \includegraphics[width=3.5in]{fig_33_tube_cond_10_noscaling/33_nanotube_cond10_noscale_stab.eps}
% \caption{Derivatives, displacements and the approximate trace of the unscaled, singlelized NS iteration for a (3,3) 
% nanotube with $\kappa =10^{10}$. 
% Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and 
% $\tau_y=\{10^{-4}, 10^{-5}, 10^{-6}$  are the dashed lines.  The trace expectation is shown as a full black line. }
% \end{figure}


\begin{figure}[h] \label{flow_scaled_dual}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_scaled/33_tube_k10_scale_dual.eps}
\caption{Derivatives, displacements and the approximate trace of the scaled, singlelized NS iteration for a
(3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$,
$\tau=10^{-3}$ and $\tau_y=\{10^{-3},10^{-4},10^{-6}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}

\begin{figure}[h]\label{flow_scaled_stab}
\includegraphics[width=3.5in]{fig_33_tube_cond_10_scaled/33_tube_k10_scale_stab.eps}
\caption{Derivatives, displacements and the approximate trace of the unscaled, dual NS iteration for a (3,3) nanotube with $\kappa =10^{10}$.
Derivatives are full lines, whilst the displacements cooresponding to $b=64$, $\tau=10^{-3}$ and $\tau_y=\{10^{-8}, 10^{-9}, 10^{-10}\}$
are the dashed lines.  The trace expectation is shown as a full black line. }
\end{figure}

Differences in occlusion between single and dual magnified as bounds for s.z not as tight as bounds for h.y.

 *lot* of overlap too (reproducing hilberts etc).

\section{Iterated Regularization}\label{regularization}

\subsection{Rilely's Correction}

Shown in the preceeding section, singleiliyty limits application of the NS square root iteration under 
agressive $\tt SpAMM$ approximation.  These limits can be circumvented through Tikhonov regularization \cite{}, 
involving a small level shift of eigenvalues,  $\mat{s}_\mu \leftarrow \mat{s}+\mu \mat{I}$, leading to a more 
well conditioned matrix with $\kappa( \mat{s}_\mu) = \frac{\sqrt{s^2_{N-1} + \mu^2}}{\sqrt{s^2_0+\mu^2}}$ \cite{}.  
However, achieving substantial acceleration with severe ill-conditioning  may require a large level shift, 
producing inverse factors of little practical use.  One approach to recover a more accurate inverse
factor is Riley's method \cite{}; 
\begin{equation}
\mat{s}^{-1/2} = \mat{s}^{-1/2}_{\mu} \cdot \left( \mat{I}+\frac{\mu}{2} \mat{s}^{-1}_{\mu}
                                                   +\frac{3 \mu^2}{8} \mat{s}^{-2}_{\mu} + \dots
   \right) \; ,
\end{equation}
but this is ineffective when $\mu$ is large, and invovles powers of the full inverse. 

\subsection{Product Representation}

spamm: scoping on precision
regularization: scoping on condition number.

Here, we outline an alternative,  nested product represenation of the inverse factor and show preliminary 
results for a first, most approximate solution.  This most approximate (but effective) solution is (ideally) representative 
of one order in the precision, $\tau_0\sim .1$, and corrective by one order in the condition, $\mu_0\sim .1$,
yeilding a thin, $0^{\rm th}$ preconditioner, $\mat{s}^{-1/2}_{\tau_0 \mu_0}$.  
This ``thin'' iteration may bring spectral resolution into alignment with norm magnitudes 
towards the resolvent $\mat{I}_{\tau_0\mu_0}\equiv \widetilde{\mat{I}}\left(\mat{s}_{\tau_0\mu_0}\right)$,
strengthening Eq.~(\ref{bound}).

Culled $\tt SpAMM$ volumes for this most approximate solution are shown with increasing system size in 
Fig.~\ref{regularized_stab} for the single instance, and in Fig.~\ref{regularized_dual} for the dual instance, 
cooresponding to ``thin NS'' iteration for the (3,3) $\kappa(\mat{s})=10^{10}$ nanotube series.  
The behavior of these instances is very different; in the ``single'' 
instance, a single iteration could not be found at precision $\tau_0=.1$, even with $\mu_0=.1$ regularization.  
Also, this single iteration
sees a weakly convergent trace with inflating cull-volumes. On the other hand, volume of the dual iteration 
is strongly contracted with resolution of the identity.  
 
%most-approximate-yet-effective-by-one-order (MAYEBOO) vs
%most-approximate-yet-still-stable (MAYSS) 

\subsection{A Most Approximate Preconditioner}

Bottom run preconditioning 

\begin{figure}[h]\label{regularized_stab}
 \includegraphics[width=3.5in]{fig_33_tube_cond_10_regularized/33_tube_k10_regularized_stab.eps}
\caption{
Culled volumes in the thin slice, single instance approximation of $\mat{s}^{-1/2}_{\tau_0 \mu_0}$
for the (3,3) nanotube, $\kappa(\mat{s})=10^{10}$ matrix series 
described in Section \ref{data}.  In the ``single'' instance, it was not possible to achieve stability with $\tau_0=.1$
In this ``single'' case, a thin slice cooresponds to $\mu_0=.1, \tau_0=10^{-2} \;  \&  \; \tau_s=10^{-4}$, and volumes are
$\rm v_{\widetilde{\mat{z}_k}} = \left( {\rm vol}_{ \widetilde{\mat{z}}_{k-1}\ot h[\widetilde{\mat{x}}_{k-1}] } \right) \times 100\% /N^3$ and
${\rm v}_{\widetilde{\mat{y}}_k} = \left( {\rm vol}_{\mat{s}  \ots  \widetilde{\mat{z}}_{k}} \right) \times 100\% /N^3$.    
Line width increases with increasing system size. 
Also shown is the trace error, ${\rm e}_{k} = \left( N-{\rm tr}\,\mat{x}_k \right)/N$.}
\end{figure} 
\begin{figure}[h]\label{regularized_dual}
 \includegraphics[width=3.5in]{fig_33_tube_cond_10_regularized/33_tube_k10_regularized_dual.eps}
\caption{
Culled volumes in the thin slice, dual instance approximation of $\mat{s}^{-1/2}_{\tau_0 \mu_0}$
for the (3,3) nanotube, $\kappa(\mat{s})=10^{10}$ matrix series 
described in Section \ref{data}. The thin slice cooresponds to $\mu_0=.1, \tau_0=.1 \;  \&  \; \tau_s=.001$ 
with volumes 
${\rm v}_{\widetilde{\mat{y}}_k}= \left( {\rm vol}_{  h[\widetilde{\mat{x}}_{k-1}] \ots \widetilde{\mat{y}}_k }  \right) \times 100\% /N^3$ and  
${\rm v}_{\widetilde{\mat{z}}_k}= \left( {\rm vol}_{\widetilde{\mat{z}}_{k-1} \ot  h[\widetilde{\mat{x}}_{k-1}]} \right) \times 100\% /N^3$.
Line width increases with increasing system size. 
Also shown is the trace error, ${\rm e}_{k} = \left( N-{\rm tr}\,\mat{x}_k \right)/N$, which rapidly approaches $10^{-11}$ (not shown). }
\end{figure} 

These results reflect very different cull-spaces.  In the single instance, the spectral resolution of powers is not compresive, and  
$\widetilde{\mat{y}}^{\rm single}_k \rightarrow  \mat{s}^{-1/2}_{\tau_0 \mu_0} \oto \mat{s}_{\mu_0}$ is  poorly bound by Eq.~(\ref{bound}).
In the dual case however, $\widetilde{\mat{y}}^{\rm dual}_k \rightarrow  \mat{I}_{\tau_0 \mu_0} \oto \mat{s}^{1/2}_{\tau_0 \mu_0}$
and $\widetilde{\mat{z}}^{\rm dual}_k \rightarrow  \mat{s}^{-1/2}_{\tau_0 \mu_0} \oto \mat{I}_{\tau_0 \mu_0}$, 
with Eq.~(\ref{bound}) tightening to
\begin{equation}\label{boundY}
\Delta^{\mat{I}_{\tau_0 \mu_0} \cdot \mat{s}^{1/2}_{\tau_0 \mu_0}} <  \tau n \lVert \mat{s}^{1/2}_{\tau_0 \mu_0} \rVert
\end{equation}
and 
\begin{equation}\label{boundZ}
\Delta^{ \mat{s}^{-1/2}_{\tau_0 \mu_0}\cdot \mat{I}_{\tau_0 \mu_0}}  <  \tau n \lVert \mat{s}^{-1/2}_{\tau_0 \mu_0} \rVert \, ,
\end{equation}
as relative and absolute errors converge.  This tightening is compressive, leading to complexities that are quadtree copy in place.   

In the dual instance, the $\tt SpAMM$ approximation can be brought all the way to  $\tau_0 = .1$ in the case of $\mu_0 = .1$.
From this first slice  $\mat{s}^{-1/2}_{\tau_0, \mu_0}$ then, a next level shifted preconditioner can be found, 
$\mat{s}^{-1/2}_{\tau_0 \mu_1}$, based on the residual 
$\left(\mat{s}^{-1/2}_{\tau_0\mu_0} \right)^\dagger \, \oto \, \left(\mat{s}+\mu_1 \mat{I} \right)  \, \oto \,\mat{s}^{-1/2}_{\tau_0 \mu_0} $, with {\em e.g.} 
$\mu_1= .01$. It may then be possible to find the full ($\tt SpAMM$ most approximate) 
factor as the nested product of preconditioned thin slices;
\begin{equation}
\mat{s}^{-1/2}_{\tau_0} = \mat{s}^{-1/2}_{\tau_0 \mu_n} \, \oto \, \mat{s}^{-1/2}_{\tau_0 \mu_{n-1}} \, \oto \, \dots  \,  \mat{s}^{-1/2}_{\tau_0 \mu_0}
\end{equation}
In this way, 
iterative regularization can be used to find a product representation of the inverse square root at a $\tt SpAMM$ resolution 
potentially far more permisive than otherwise possible. 
Likewise, it may be possible to obtain the full factor with increasing $\tt SpAMM$ resolution in the product representation:
\begin{equation}
\mat{s}^{-1/2} = \mat{s}^{-1/2}_{\tau_m} \, \otm \,  \mat{s}^{-1/2}_{\tau_{m-1}} \, \otmm \dots \, \mat{s}^{-1/2}_{\tau_0}
\end{equation}taken over the sequence $1 > \tau_0 > \tau_1 > \dots > \tau_n $.  More generally,
\begin{equation} \label{spammsandwich}
\mat{s}^{-1/2} \equiv \bigotimes_{\substack{\tau=\tau_0 \\ \mu=\mu_0   } } {\left|\, \tau\, \mu \, ; \, \scriptstyle{\mat{s}^{-1/2}}  \right>}  \, ,
\end{equation}
acknowledging the potential for a flexible path between precision and regularization. The braket notation marks 
the potential for assymetries in the intermediate represenation.  

This thin product representation may have advantages: 
({\bf{1}}) Each thin solve involves a few generic and well behaved steps that may be narrowly optimized;
({\bf{2}}) Each thin solve can be brought rapidly into compressive identity iteration; 
({\bf{3}}) The {\tt SpAMM} bound is vastly strengthend, via Eqs.~(\ref{boundY}-\ref{boundZ}); 
({\bf{4}}) A new algebraic $n$-body form of locality is exploited; 
({\bf{5}}) The inverse factor can be applied incrementally;
({\bf{6}}) Slice update and application is ammenable to continous temporal partitioning based on {\em e.g.} persistence data.

%\begin{equation}
%\mat(I}\left(\mat{s}\right) = \bigotimes_{\substack{\tau=\tau_0 \\ \mu=\mu_0}}  \bigotimes_{\substack{\tau'=\tau_0 \\ \mu'=\mu_0}} 
%{_{\mat{s}^{1/2}} }{\left<\tau \mu \right|}
%{\left|\tau \mu \right>}_{\mat{s}^{-1/2}} 
%\end{equation}

%\pagebreak

\section{Locality} 


\begin{figure}[h] 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                       trim={0.cm 2.3cm 2.cm 1.cm},clip]
                       {fig_wtrbx_100_noregular/y_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noregular/y_dual_5_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noregular/y_dual_17_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noregular/x_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noregular/x_dual_5_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_noregular/x_dual_17_cant_x.png}} 
\caption{
The $ijk$ task and data space for construction of the unregularized preconditioner 
$\left|\tau_0=.001,\mu_0=.0\, ; \,\scriptstyle{\mat{s}^{-1/2}} \right>$, with
dual instance square root iteration, and for the 6-311G** metric of 100 periodic water molecules
at STP.  At top its  $\mat{y}_k=h_\alpha[ \mat{x}_{k-1} ] \ots \mat{y}_{k-1}$
for $k=0,4,\& 15$, while on the bottom we have $\mat{x}_k=  \mat{y}_{k}  \ot \mat{z}_{k}$ for $k=0,4, \& 15$.
Maroon is $\mat{a}$, purple is $\mat{b}$, green is $\mat{c}$,  and black is the volume ${\rm vol}_{a \ot b}$
in the product $\mat{c}=\mat{a} \ot \mat{b}$.}\label{Lensing3}
\end{figure}

\subsection{Metric, Spatial and Temporal Locality}

Astrophysical $n$-body algorithms employ range querries over spatial databases to hierarchically discover 
and compute approximations that commit only small errors.  Often, these spatial databases are ordered with a 
space filling curve (SFC) \cite{}, which maps points that are close in space to an index where they are also close. 
The block-by-magnitude structures empowering the $\tt SpAMM$ approximation are {\em metric localities}; 
in quantum chemical examples they coorespond to an underlying SFC ordering of Carteisan coordinates. 

Warren and Salmon showed how to parlay spatial locality into temporal locality, 
remapping and repartitioning the space filling curve to rebalance distributed $n$-body tasks,
based on accumulated histories (persistence data).
In a similar way, we showed how persistence can be used to achieve 
strong parallel scaling for $\tt SpAMM$ with commonly available runtimes \cite{}.  
Persistence data, providing temporal locality, may also be useful in mathematical approximation.  

%The locality of $\tt SpAMM$ volumes depends on opposing effects; ill-conditioning leads to delocalizations 
%and a broad spectral resolution, while strong metric seperation cooresponds to an increasing locality.  
%Unfortunately, seperation is diminished with increasing dimensionality.  

In Figure \ref{Lensing3}, we show $\ot$ volumes for square root iteration, cooresponding to the metric of a small, periodic water box 
with the large, 6-311G** basis.  For the 3-d periodic case, diminishing Cartesian seperations lead to long-skinny delocalizations (pillae)
and much denser matrices, relative to {\em e.g.} a one-dimensional nano-tube.  These delocalizations coorespond to weakness in Eq.~(\ref{bound}),  
and to the tighter thresholds required to maintain a single iteration in the MAYSS approximation.  This effect is even more pronounced in the 
single instance (not shown), where delocalizations are exagerated due to spectral resolutions that are broader.  
Eventually, Cartesian seperation will thin and trim the density of these delocalizations,  
leading to complexity reduction based on the effects of metric locality alone, as demonstrated in \ref{}.

\subsection{Algebraic Locality (Lensing)}

In Figures \ref{Lensing1} and \ref{Lensing2},  we show a new kind of locality that is uniquely exploited by $n$-body approximation 
of the square root iteration.  This {algebraic locality} develops compressively towards convergence 
as the contractive identity iteration develops.  
We call this compression {\bf  \em lensing},  involving collapse  of the culled volume about plane diagonals of the resolvent.
Lensing cooresponds to strengthening of Eq.(\ref{}) to yeild Eqs.~(\ref{boundY}) and (\ref{boundZ}), and strong convergence of the
directional derivatives Eq.(\ref{}-\ref{}) to $\mat{0}$.
This is an important, mitigating computational effect for the $\mat{y}_k$ channel that involves the 
tighter threshold, $\tau_s \sim 0.01 \times \tau$. 

In addition, non-Euclidian measures are relevent for achieving metric locality in 
the $\tt SpAMM$ algebra, including information measures, space filling curve generalizations, as well 
as graph reorderings that envelope matrix elements about the diagonal \cite{}, a common approach in 
structural mechanics.   In Figure \ref{Lensing4} we show development of a first, unregularized (MAYSS) 
preconditioner for such an example; the structural matrix $\mat{s}={\tt bcsstk14}$ is a $\kappa(\mat{s})=10^{10}$ matrix 
cooresponding to the roof of the Omni Coliseum in Atlanta \cite{}.  These results show remarkable 
gossamer sheeting and flattening along plane diagonals, at top for developmentent of $\mat{y}_k$, 
and hollow accumulation of ${\rm vol}_{\mat{y}_k \ot \mat{z}_k}$ looking down at $\mat{y}_k$ (along bottom).
Surprisingly, this example shows lensing for a relatively tight MAYSS approximation, while the 
equally ill-conditioned \& lower dimensional  $\kappa (\mat{s})=10^{10}$ nanotube MAYSS approximation remains full (dense) 
through U.C. $\times 128$.

\subsection{Potential for Complexity Reduction}
  
Finally, we show complexity reduction at convergence of the MAYEBOO approximation relative to the MAYSS approximation, 
in Fig.~\ref{Complex1} for periodic water boxes, and in Fig.~\ref{Complex2} for the ill-conditioned nano-tube. 
The two-orders difference between $\mat{y}_k$ and $\mat{z}_k$ volumes cooresponds precicely to $\tau_s \sim \tau \times {\tt .01}$, 
with $\mat{x}_k$ in between. Except for the slower trend in Fig.~(\ref{Complex1})'s $\mat{x}_k$ volume, we see the 
potential for continued strong acceleration with increasing system size.  Understanding these subtlties is the subject of future work. 

\begin{figure}[tb] 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/y_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/y_dual_4_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/y_dual_16_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/x_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/x_dual_2_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={3.cm 3.cm 5.cm 5.cm},clip]
                        {fig_33_8x_tube_cond_10_dual_thin_slices/x_dual_16_cant_x.png}} 
\caption{
The $ijk$ task and data space for construction of the MAYEBOO preconditioner 
$\left|\tau_0=.1,\mu_0=.1\, ; \,\scriptstyle{\mat{s}^{-1/2}} \right>$, with 
dual instance square root iteration,  and for an 8$\times$ U.C. (3,3) $\kappa(s)=10^{11}$ nanotube.
$\mat{y}_k$ appears wider than $\mat{z}_k$ because it is computed at a higher precision, $\tau_s=.001$,
and because the first multiply involves $\mat{s}^2$.  At top its  $\mat{y}_k=h_\alpha[ \mat{x}_{k-1} ] \ots \mat{y}_{k-1}$
for $k=0,4,\& 16$, while on the bottom we have $\mat{x}_k=  \mat{y}_{k}  \ot \mat{z}_{k}$ for $k=0,2, \& 16$.
Maroon is $\mat{a}$, purple is $\mat{b}$, green is $\mat{c}$,  and black is the volume ${\rm vol}_{a \ot b}$
in the product $\mat{c}=\mat{a} \ot \mat{b}$.}\label{Lensing1}
\end{figure}


\begin{figure}[tb] 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                       trim={0.cm 2.3cm 2.cm 1.cm},clip]
                       {fig_wtrbx_100_regularized/y_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_regularized/y_dual_4_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_regularized/y_dual_15_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_regularized/x_dual_0_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_regularized/x_dual_4_cant_x.png}} 
\fbox{ \includegraphics[width=2.45cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_wtrbx_100_regularized/x_dual_15_cant_x.png}} 
\caption{
The $ijk$ task and data space for construction of the MAYEBOO preconditioner 
$\left|\tau_0=.1,\mu_0=.1\, ; \,\scriptstyle{\mat{s}^{-1/2}} \right>$, with 
dual instance square root iteration  and for 6-311G** metric of 100 periodic water molecules
at STP.  At top its  $\mat{y}_k=h_\alpha[ \mat{x}_{k-1} ] \ots \mat{y}_{k-1}$
for $k=0,4,\& 15$, while on the bottom we have $\mat{x}_k=  \mat{y}_{k}  \ot \mat{z}_{k}$ for $k=0,4, \& 15$.
Maroon is $\mat{a}$, purple is $\mat{b}$, green is $\mat{c}$,  and black is the volume ${\rm vol}_{a \ot b}$
in the product $\mat{c}=\mat{a} \ot \mat{b}$.}\label{Lensing2}
\end{figure}

\begin{figure}[tb] 
\includegraphics[width=7.8cm,keepaspectratio=true,trim={0.cm 0.cm 0.cm 0.cm},clip]
                 {fig_regular_and_unleaded/pcnt_volume_water_boxes.eps} 
\caption{ 
Complexity reduction in metric square root iteration for periodic 6-311G** water. 
Shown is the ratio of lensed product volumes for the regularized most-approximate-yet-effective-by-one-order (MAYEBOO) 
approximation and the unregularized most-approximate-yet-still-stable (MAYSS) approximation.}\label{Complex1}
\end{figure}


\begin{figure}[tb] 
\includegraphics[width=7.8cm,keepaspectratio=true,trim={0.cm 0.cm 0.cm 0.cm},clip]
                 {fig_33_tube_cond_10_regularized/volume_vs_n3_tubes.eps} 
\caption{ 
Complexity reduction in metric square root iteration for periodic 6-311G** water. 
Shown is the ratio of lensed product volumes for the regularized most-approximate-yet-effective-by-one-order (MAYEBOO) 
approximation and the unregularized most-approximate-yet-still-stable (MAYSS) approximation.}\label{Complex2}
\end{figure}


\begin{figure}[tb] 
\fbox{ \includegraphics[width=3.8cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_bcsstk14/y_dual_14_cant_x.png}} 
\fbox{ \includegraphics[width=3.8cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_bcsstk14/y_dual_37_cant_x.png}} 
\fbox{ \includegraphics[width=3.8cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_bcsstk14/x_dual_0_cant_x2.png}} 
\fbox{ \includegraphics[width=3.8cm,keepaspectratio=true,
                        trim={0.cm 2.3cm 2.cm 1.cm},clip]
                        {fig_bcsstk14/x_dual_37_cant_x2.png}} 
\caption{
The $ijk$ task and data space for construction of the unregularized preconditioner 
$\left|\tau_0=.001,\mu_0=.0\, ; \,\scriptstyle{\mat{s}^{-1/2}} \right>$, with the 
dual instance of square root iteration  and for 6-311G** metric of 100 periodic water molecules
at STP.  At top its  $\mat{y}_k=h_\alpha[ \mat{x}_{k-1} ] \ots \mat{y}_{k-1}$
for $k=0,4,\& 15$, while on the bottom we have $\mat{x}_k=  \mat{y}_{k}  \ot \mat{z}_{k}$ for $k=0,4, \& 15$.
Maroon is $\mat{a}$, purple is $\mat{b}$, green is $\mat{c}$,  and black is the volume ${\rm vol}_{a \ot b}$
in the product $\mat{c}=\mat{a} \ot \mat{b}$.}\label{Lensing4}
\end{figure}

%The $n$-body solver carries out metric querries with occlusion-culling based on 
%with a bounded relative error in the product, given by Eq.~\ref{}. 
%$\tt SpAMM$ expoits metric locality, cooresponding to decay with Cartesian or non-Euclidian seperation, 
%and also algebraic locality, developing with identity iteration.  
%The $\tt SpAMM$ querry hierarchically resolves complex algebraic structures in the product space of square root iteration,  
%culling out the $i=k$ and $i=k$ planes and along the $ijk$ cube-diagonal.  In the case of the dual $\mat{y}_k$ and $\mat{z}_k$ channels,  
%these structures contract further as multiplication by near-identity is reached, with
%{\em lensing} about the plane diagonal and computational complexities tending towards quadtree-copy-in-place.

\section{Summary}
In this work, we developed the $n$-body solver $\tt SpAMM$ for square root iteration.   
Main contributions include a modified Cauchy-Schwarz criterion, Eq.~\ref{}, and proof that the 
cooresponding relative product error is bound by Eq.~(\ref{bound}).  Also, we demonstrated a new kind of 
algebraic locality, {lensing}, that develops with strongly contractive identity iteration.
 
In Section \ref{}, we looked at stability leading to the basin of convergence and sensitivity of the three
product channels $\mat{y}_k$, $\mat{z}_k$ and $\mat{x}_k$, for the $\tt SpAMM$ approximation in the 
cannonical ``dual'' instance, Eq.~(\ref{}), and for the ``single'' instance, Eq.~(\ref{}).
Consistent with HMMT \cite{}, the $\mat{z}_k$ channel is sensitized by the full inverse,
$\mat{s}^{-1}$, requiring a tighter threshold for that case, $\tau_s \ll \tau$.  Later, we find that 
extra cost is strongly mitigated by lensing.  Also in Section \ref{}, we looked at bifurcations 
of scaled and unscaled iterations for ill-conditioned systems, towards a most-approximate-yet-still-stable (MAYSS) 
preconditioner, and disected competing effects at the edge of stability,  between compounding displacement magnitudes and 
strongly convergent directional derivatives.  

{\bf OK, WELL ITS HARD TO WRITE THIS WITH THAT PART BEING A BIT OF A MESS ...}
In Section \ref{},  we introduced iterated regularization for ill-conditioning, and developed 
a product representation of thin 
 the potential for
 and found large differences between
single and dual channel instances.   For the regularized dual channel instance, we demonstrated iterations with 
volumes strongly contractive towards convergence, and we showed that this contraction cooresponds to   
Eq.~(\ref{bound}) tightenging significantly.   
strong contractive iterations 
how the full inverse factor can 
e proved that at convergence, 
be achieved by products of generic, regularized, well conditioned and increasingly more accurate solutions;
$\mat{s}^{1/2} = \bigotimes_{\substack{\tau \\ \mu } }  \left| \mu , \tau \, ;  \, \scriptstyle{\mat{s}^{-1/2}} \right>$
cooresponding to a first most-approximate-yet-effective-by-one-order (MAYEBOO) preconditioner
$ \left| \mu={\tt .1} , \tau = {\tt .1} \, ;  \, \scriptstyle{\mat{s}^{-1/2}} \right>$.   
We looked at the MAYEBOO approximation for both the single and dual instances, and found that even with the most
permisive regularization, spectral resolution in the single instance is too broad to achieve strong lensing.  
% $\mat{I}(\mat{s}) =  
%\bigotimes_{\substack{\tau' \\ \mu' } }\bigotimes_{\substack{\tau \\ \mu } }  
%\left< \mu , \tau \, ; \, \scrithe ptstyle{\mat{s}^{1/2}} \right|
%\left. \mu' , \tau' \, ;  \, \scriptstyle{\mat{s}^{-1/2}} \right>$

Finally, we looked at the MAYSS and MAYBOO approximations for periodic water systems, for ill-conditioned nanotubes, 
and for the ill-conditioned structural matrix ${\tt bcsstk14}$ for the Omni Coliseum in Atlanta \cite{}.  For the
problem of large basis periodic water systems, we find a MAYBOO/MAYSS volume compression of two to three orders. 
For the problem of an ill-conditioned nano-tube, we find a MAYBOO/MAYSS volume compression  of ...    
Remarkably, the ill-conditioned ${\tt bcsstk14}$ was able to achieved a strongly lensed state 
in the  MAYSS approximation, with remarkable gossamer sheeting and flattening along plane diagonals, 
and hollow, reticulate volumes of the resolvent.


\section{Conclusions}

This work is gauged against other methods for fast matrix multiplication discussed in Section \ref{}. 
Against $\tt SpMM$, the $n$-body approach offers a bounded control over relative errors in 
the product and the ability to resolve complex algebraic structures, about plane-diagonals of the $ijk$-cube, 
along tall-skiny pillae and for volumetric contractions to lower dimensional objects via lensing. 
The $n$-body method uniquely and synnergistically exploits two distinct forms of locality, metric locality cooresponding to 
a Cartesian or non-Euclidean decay principle, and algebraic locality cooresponding to contractive identity iteration. 
Also, strong parallel scaling for the ${\cal{O}}(n)$ electronic structure problem has been demonstrated with the $\tt SpAMM$ kernel, 
a feature that remains elusive for methods based on $\tt SpMM$ \cite{Bowler}.

Against methods for matrix compression \cite{}, as well as against Fast Matrix Multiplication of the Strassen type \cite{}, 
the quadtree data structure and the octree task space employed by $\tt SpAMM$ are entirely complimentary.  
In the case of sketch products \cite{}, persistence data can be used to identify and characterize pillae resulting 
from broad spectral resolutions; then, it may be possible to deploy streaming approaches for these tall-skinny delocalizations \cite{}.
Thus, the concurent application of fast methods for matrix multiplication may be enabled by the database framework supporting 
$n$-body approximations.  

Beyond the fast matrix multiply, $n$-body frameworks may enable additional, layered functionalities and economizations 
in complex solver ecosystems, with facile interopperability and mathematical agility, 
through generic recursion and skelitinization, and with common runtimes able to exploit temporal and data localities.
For example, we recently generalized $\tt SpAMM$ recursion to the problem of Fock-exchange, with a recursive triple (hextree) 
metric querry on the Almlof-Alhrichs direct SCF criteria \cite{}.  
Also, mathematical equivalence with the matrix sign function, Eq.~(\ref{}), and close structural relationships with the 
polar decomposition may enable to extend functionality of the $n$-body iterations developed here. 

Despite these compeling features and related xxx, $n$-body square-root iteration must be gauged by its ability to compute 
a high quality inverse factor.   Here, we have only looked at complexity and stability of the most 
approximate preconditioners; the most-approximate-yet-still-stable (MAYSS) approximation and the regularized 
most-approximate-yet-effective-by-one-order (MAYEBOO) approximations.    However, these results are encouraging, showing 
the potential for several to many orders of magnitude reduction in complexity for the regularized approximation 
relative to the unregularized approximation, made possible by   
construction of a much lower precision preconditioner than would otherwize be possible, via Eq.~(\ref{}), 
and also by opperations in the strongly contractive regime, under Eqs.~(\ref{boundY}-\ref{boundZ}).
These and related preliminary results \cite{} suggest that a $\tt SpAMM$ sandwich of thin,
generic iterations may enable a competitive computational approach that avoids explicit computation of the 
ill-conditioned factor. 

\appendix 

\section{Implementation}

%\subsection{programming}

FP, F08, OpenMP 4.0
In the current implementation, all persistence data
(norms, flops, branches \& {\em etc.}) are accumulated compactly in the backward recurrence.  This persistence data
 that may be achieved by minimal locally essential trees \cite{}.

%\subsection{scaling}

%\subsection{singleilization}

For these reasons, maintaining connection to the eigenvectors of $\mat{s}$ through 
a tighter first product is nessesary.  In the single instance, and with a 
tighter ``{\em s}'' product, $\tau_s \ll \tau$, we find very interesting left/right differences; 
namely, the right first product 
\begin{equation} 
\widetilde{\mat{x}}^R_k \leftarrow \widetilde{\mat{z}}^\dagger_{k} \, \ot  \, \left( \mat{s} \,  \ots \, \widetilde{\mat{z}}_{k-1}  \right) \; ,
\end{equation}
is  different from the left first product 
\begin{equation} 
\widetilde{\mat{x}}^L_k \leftarrow \left(  \widetilde{\mat{z}}^\dagger_{k} \, \ots \, \mat{s} \right) \,  \ot  \, \widetilde{\mat{z}}_{k-1} \; .
\end{equation}

%\subsection{regularization}
damping the inversion and the small value to be added c is called Marquardt-Levenberg coefficient

%\subsection{convergence}

Map switching and etc based on TrX

\section{Data} \label{data}

%\subsubsection{double exponential ill-conditioning}
3,3 carbon nanotube with diffuse $sp$-function
double exponential (Fig.)

%\subsubsection{three-dimensional, periodic}
%\subsubsection{water boxes}
%\subsubsection{ordering}
%\subsubsection{Matrix Market}

\section{TEMPERARY:NOTES FOR REGULARIZATION SECTION} 

The idea of preconditioning is that we can use a low tolerance $\tau_{0}$
(e.g. $\tau_{0}=10^{-2}$) to cheaply obtain an approximation $R_{0}\approx S^{-1/2}$.
Then since $S_{1}\equiv R_{0}SR_{0}$ is close to the identity matrix
$I$, 
\[
\left\Vert R_{0}SR_{0}-I\right\Vert _{F}\lesssim\tau_{0},
\]
we can use Newton Schulz on $S_{1}$ with a higher tolerance $\tau_{1}$
to get an accurate approximation $R_{1}=S_{1}^{-1/2}$ and using only
a few iterations:
\[
\left\Vert R_{1}S_{1}R_{1}-I\right\Vert _{F}\lesssim\tau_{1}.
\]
In particular, the matrix $S_{1}$, being close to the identity, is
better conditioned than $S$ and computing $S_{1}^{-1/2}$ requires
much fewer Newton Schulz iterations. Moreover, since 
\[
\left\Vert \left(R_{1}R_{0}\right)S\left(R_{0}R_{1}\right)-I\right\Vert _{F}\lesssim\tau_{1},
\]
we see that $R_{1}R_{0}$ is a $\tau_{1}$ approximation to $S^{-1/2}$.
Notice that, from the stability bound for SpAMM, we can replace all
of the exact matrix multiplications with SpAMM multiplications. 

To formalize this, let $S_{0}=S$, and suppose that $R_{j}$ is the
approximation to $S_{j}^{-1/2}$ obtained via the Newton Schulz iteration
with SpAMM tolerance $\tau_{j}$, so that
\[
\left\Vert R_{j}\otimes_{\tau_{j}}S_{j}\otimes_{\tau_{j}}R_{j}-I\right\Vert _{F}\lesssim\tau_{j}.
\]
Then define $S_{j+1}=R_{j}\otimes_{\tau_{K}}S_{j}\otimes_{\tau_{K}}R_{j}$
and let $R_{j+1}$ the approximation to $S_{j+1}^{-1/2}$ obtained
via the Newton Schulz iteration with SpAMM tolerance $\tau_{j+1}$,
so that 
\[
\left\Vert R_{j+1}\otimes_{\tau_{K}}S_{j+1}\otimes_{\tau_{K}}R_{j+1}-I\right\Vert _{F}\lesssim\tau_{j+1}.
\]
Then since $S_{j+1}=R_{j}\otimes_{\tau_{K}}S_{j}\otimes_{\tau_{K}}R_{j}$,
\[
\left\Vert R_{j+1}\otimes_{\tau_{K}}\left(R_{j}\otimes_{\tau_{K}}S_{j}\otimes_{\tau_{K}}R_{j}\right)\otimes_{\tau_{K}}R_{j+1}-I\right\Vert _{F}\lesssim\tau_{j+1}.
\]
In general, defining 
\[
R_{\text{left}}\equiv R_{j+1}\otimes_{\tau_{K}}R_{j}\otimes_{\tau_{K}}R_{j-1}\dots\otimes_{\tau_{K}}R_{0},
\]
and 
\[
R_{\text{right}}\equiv R_{0}\otimes_{\tau_{K}}R_{1}\dots\otimes_{\tau_{K}}R_{j}\dots\otimes_{\tau_{K}}R_{j+1},
\]
it follows by induction that 
\[
\left\Vert R_{\text{left}}\otimes_{\tau_{K}}S\otimes_{\tau_{K}}R_{\text{right}}-I\right\Vert _{F}\lesssim\tau_{K}.
\]


Now, by stability of SpAMM,
\[
\left\Vert R_{\text{left}}SR_{\text{right}}-I\right\Vert _{F}\lesssim\tau_{K}.
\]
Also,
\[
R_{j+1}\otimes_{\tau_{K}}R_{j}\otimes_{\tau_{K}}R_{j-1}\dots\otimes_{\tau_{K}}R_{0} 
\]
can be written as
\[ 
 \left(R_{0}\otimes_{\tau_{K}}R_{1}\dots\otimes_{\tau_{K}}R_{j}\dots\otimes_{\tau_{K}}R_{j+1}\right)^{\text{T}}+\mathcal{O}\left(\tau_{K}\right).
\]
Therefore, $R_{\text{left}}=R_{\text{right}}^{\text{T}}+\mathcal{O}\left(\tau_{K}\right)$,
and so 
\begin{equation}
\left\Vert R_{\text{left}}SR_{\text{left}}^{\text{T}}-I\right\Vert _{F}\lesssim\tau_{K}.\label{eq:product decomposition error bound}
\end{equation}


We can therefore write the following symbolic representation 
\[
S^{-1/2}=S_{\tau_{j+1}}^{-1/2}\otimes_{\tau_{j+1}}S_{\tau_{j}}^{-1/2}\otimes_{\tau_{j}}S_{\tau_{j-1}}^{-1/2}\dots\otimes_{\tau_{1}}S_{\tau_{0}}^{-1/2}+\mathcal{O}\left(\tau_{j+1}\right),
\]
where $S_{\tau_{k}}^{-1/2}$ is a $\tau_{k}$ approximation to the
inverse square root of $S_{k}=S_{\tau_{k-1}}^{-1/2}\otimes_{\tau_{k-1}}S_{k-1}\otimes_{\tau_{j}}S_{\tau_{k-1}}^{-1/2}$. 




\bibliography{MatrixFunctions}

\end{document}
