/** \mainpage SpAMM
 *
 * \brief Sparse Approximate Matrix Multiply (SpAMM) library.
 *
 * SpAMM is a library for spare approximate matrices.
 *
 * The public functions of this library are all declared in spamm.h. Useful
 * helper functions are documented in spamm_ll.h and spamm_mm.h.
 *
 * The documentation is organized into the following chapters:
 *
 * - \subpage spamm_introduction "Introduction"
 * - \subpage spamm_algorithm "Algorithm"
 * - \subpage spamm_milestones "Future Milestones"
 * - \subpage spamm_design_goals "SpAMM Design Goals"
 * - \subpage spamm_performance "SpAMM Performance"
 *
 * \version @PACKAGE_VERSION_STRING@ (@LOCAL_VERSION@)
 *
 * \author Nicolas Bock <nicolasbock@gmail.com>
 * \author Matt Challacombe <matt.challacombe@gmail.com>
 */

/** \page spamm_introduction SpAMM - Introduction
 *
 * A spamm_t matrix is defined as an \f$ N \f$-tree on the 2-dimensional
 * matrix elements. The matrix elements at the lowest level are stored in
 * dense matrix blocks. The matrix is padded with zeros so that the tree depth
 * \f$ d \f$ is integer according to
 *
 * \f[
 * M = M_{\mathrm{block}} M_{\mathrm{child}}^d
 * \]
 * \[
 * N = N_{\mathrm{block}} N_{\mathrm{child}}^d
 * \f]
 *
 * where \f$ M \f$ (\f$ N \f$) is the number of rows (columns) of the matrix,
 * \f$ M_{\mathrm{block}} \f$ (\f$ N_{\mathrm{block}} \f$) is the number of
 * rows (columns) of the dense matrix blocks at the lowest tree level, \f$
 * M_{\mathrm{child}} \f$ (\f$ N_{\mathrm{child}} \f$) is the number of rows
 * (columns) of the children nodes per node in the matrix tree, and \f$ d \f$
 * is the depth of the tree. This means that the matrix tree is a quadtree for
 * \f$ M_{\mathrm{child}} = N_{\mathrm{child}} = 2 \f$.
 *
 * More details can be found in the \ref spamm_algorithm "Algorithm" section
 * of this manual.
 */

/** \page spamm_algorithm SpAMM - Algorithm
 *
 * The multiplication is divided into a symbolic part and a multiplication
 * part. The symbolic part constructs the matrix product convolution such that
 * it creates up to N copies of each \f$ C_{ij} \f$ block to allow for full
 * parallelism in the multiplication part. This data redundancy circumvents
 * possible race conditions in \f$ C \f$ and we can take full advantage of SMP
 * in the multiply. The multiplication part then works through the convolution
 * stream and parallelizes multiplies if possible (i.e. if we are running on
 * SMP). In a final stage, the redundant \f$ C \f$ blocks are summed and the
 * final \f$ C \f$ matrix is constructed.
 *
 * -# Symbolic: Construct convolution with redundant \f$ C_{ij} \f$ blocks.
 * -# Multiplication: Work through convolution and parallelize work as much as
 *  possible.
 * -# Sum: Sum redundant \f$ C_{ij} \f$ blocks to construct final \f$ C \f$
 *   matrix.
 */

/** \page spamm_milestones SpAMM - Milestones
 *
 * This chapter summarizes achieved and planned future milestones.
 *
 * \section spamm_sec_future_milestones Future Milestones
 *
 * - Clean up spamm_multiply.
 * - Benchmark for now with standard lapack (netlib) _not_ using SSE.
 * - Check proper ordering of matrix elements and multiply.
 *
 * \section spamm_sec_achieved_milestones Achieved Milestones
 *
 * - 2010-06-18: Implemented spamm_tree_pack(). Added test for this function.
 * - 2010-06-18: Implemented linear quadtree support in spamm_add().
 * - 2010-06-23: Implemented linear quadtree support in spamm_multiply().
 * - 2010-06-24: Implemented full multiply including beta != 1.0.
 * - 2010-06-25: Added more testcases and organized them.
 * - 2010-07-02: Consolidated 3 benchmark testcase: dense, diagonal, and
 *   column_row. All are combined in the multiply benchmark.
 */

/** \page spamm_design_goals SpAMM Design Goals
 *
 * - Serial execution
 *   - Use standard gemm kernel from BLAS (Goto)
 *   - Use small self-made kernel: SpAMM kernel (for instance on 9x9 blocks
 *     with Peano curve ordering)
 * - SMP execution
 *   - Use standard gemm kernel (Goto)
 *   - Use SpAMM kernel
 * - GPU execution
 * - Combined execution with work queue. In this case we put the multiply
 *   curve in a queue and allow for SMP and GPU execution. Chunks are
 *   processed to involve all components.
 *
 * See also section \ref spamm_milestones "Milestones" for a detailed
 * break-down of these goals.
 */

/** \page spamm_performance SpAMM Performance Results
 *
 * - How close can the stream multiply performance be pushed to match GotoBLAS
 *   large N perfmance? If stream is slower, why? Understand better what Goto
 *   does to make his multiply fast.
 *
 * \image latex GotoBLAS_on_Q8200.eps "The measured performance of GotoBLAS single precision sgemm() on Intel Core2 Q8200." width=\linewidth
 */
