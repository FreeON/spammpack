/** @file spamm.h */

/** \mainpage SpAMM - Overview
 *
 * \brief The Sparse Approximate Matrix Multiply (SpAMM) library.
 *
 * The public functions of this library are declared in spamm.h,
 * spamm_config.h, spamm_error.h, spamm_hashtable.h, spamm_kernel.h, and
 * spamm_timer.h, spamm_convert.h.
 *
 * The documentation is organized into the following chapters:
 *
 * - \subpage spamm_introduction "Introduction"
 * - \subpage spamm_algorithm "Algorithm"
 * - \subpage spamm_milestones "Future Milestones"
 * - \subpage spamm_design_goals "SpAMM Design Goals"
 * - \subpage spamm_performance "SpAMM Performance"
 *
 * \version @PACKAGE_VERSION_STRING@ (@LOCAL_VERSION@)
 *
 * \author Nicolas Bock <nicolasbock@gmail.com>
 * \author Matt Challacombe <matt.challacombe@gmail.com>
 */

/** \page spamm_introduction SpAMM - Introduction
 *
 * A spamm_t matrix is defined as an \f$ N \f$-tree on the 2-dimensional
 * matrix elements. The matrix elements at the lowest level are stored in
 * dense matrix blocks. The matrix is padded with zeros so that the tree depth
 * \f$ d \f$ is integer according to
 *
 * \f[
 * M = M_{\mathrm{block}} M_{\mathrm{child}}^d
 * \]
 * \[
 * N = N_{\mathrm{block}} N_{\mathrm{child}}^d
 * \f]
 *
 * where \f$ M \f$ (\f$ N \f$) is the number of rows (columns) of the matrix,
 * \f$ M_{\mathrm{block}} \f$ (\f$ N_{\mathrm{block}} \f$) is the number of
 * rows (columns) of the dense matrix blocks at the lowest tree level, \f$
 * M_{\mathrm{child}} \f$ (\f$ N_{\mathrm{child}} \f$) is the number of rows
 * (columns) of the children nodes per node in the matrix tree, and \f$ d \f$
 * is the depth of the tree. This means that the matrix tree is a quadtree for
 * \f$ M_{\mathrm{child}} = N_{\mathrm{child}} = 2 \f$.
 *
 * More details can be found in the \ref spamm_algorithm "Algorithm" section
 * of this manual.
 */

/** \page spamm_algorithm SpAMM - Algorithm
 *
 * The multiplication is divided into a symbolic part and a multiplication
 * part. The symbolic part constructs the matrix product convolution such that
 * it creates up to N copies of each \f$ C_{ij} \f$ block to allow for full
 * parallelism in the multiplication part. This data redundancy circumvents
 * possible race conditions in \f$ C \f$ and we can take full advantage of SMP
 * in the multiply. The multiplication part then works through the convolution
 * stream and parallelizes multiplies if possible (i.e. if we are running on
 * SMP). In a final stage, the redundant \f$ C \f$ blocks are summed and the
 * final \f$ C \f$ matrix is constructed.
 *
 * -# Symbolic: Construct convolution with redundant \f$ C_{ij} \f$ blocks.
 * -# Multiplication: Work through convolution and parallelize work as much as
 *  possible.
 * -# Sum: Sum redundant \f$ C_{ij} \f$ blocks to construct final \f$ C \f$
 *   matrix.
 */

/** \page spamm_milestones SpAMM - Milestones
 *
 * This chapter summarizes achieved and planned future milestones.
 *
 * \section spamm_sec_future_milestones Future Milestones
 *
 * - Clean up spamm_multiply.
 * - Benchmark for now with standard lapack (netlib) _not_ using SSE.
 * - Check proper ordering of matrix elements and multiply.
 *
 * \section spamm_sec_achieved_milestones Achieved Milestones
 *
 * - 2010-06-18: Implemented spamm_tree_pack(). Added test for this function.
 * - 2010-06-18: Implemented linear quadtree support in spamm_add().
 * - 2010-06-23: Implemented linear quadtree support in spamm_multiply().
 * - 2010-06-24: Implemented full multiply including beta != 1.0.
 * - 2010-06-25: Added more testcases and organized them.
 * - 2010-07-02: Consolidated 3 benchmark testcase: dense, diagonal, and
 *   column_row. All are combined in the multiply benchmark.
 */

/** \page spamm_design_goals SpAMM - Design Goals
 *
 * - Serial execution
 *   - Use standard gemm kernel from BLAS (Goto)
 *   - Use small self-made kernel: SpAMM kernel (for instance on 9x9 blocks
 *     with Peano curve ordering)
 * - SMP execution
 *   - Use standard gemm kernel (Goto)
 *   - Use SpAMM kernel
 * - GPU execution
 * - Combined execution with work queue. In this case we put the multiply
 *   curve in a queue and allow for SMP and GPU execution. Chunks are
 *   processed to involve all components.
 *
 * See also section \ref spamm_milestones "Milestones" for a detailed
 * break-down of these goals.
 */

/** \page spamm_performance SpAMM - Performance Results
 *
 * Performance \f$ p \f$ is measured by measuring walltime (or user time from
 * getrusage()) \f$ t \f$ as
 *
 * \f[
 * p = M \times N \times ( 2 K + 1 ) / t
 * \f]
 *
 * because the matrix product
 *
 * \f[
 * C_{ij} = \beta C_{ij} + \sum_{k} \alpha A_{ik} B_{kj}
 * \f]
 *
 * involves 2 multiply and 1 add operation per term in the sum over \f$ k \f$.
 * There are \f$ M \times N \f$ elements in matrix \f$ C \f$. The first term
 * \f$ \beta C_{ij} \f$ contributes \f$ M \times N \f$ multiplies.
 *
 * Open questions:
 *
 * - How close can the stream multiply performance be pushed to match GotoBLAS
 *   large N performance? If stream is slower, why? Understand better what
 *   Goto does to make his multiply fast.
 *
 * - The 4x4 SSE kernel in <tt>prototype/kernel_comparison</tt> is performing
 *   at around 80\% of Goto neglecting cache misses. Presumably one has to
 *   stream through more data to achieve higher throughput than this. We are
 *   now integrating this SSE kernel into a stream multiply that will achieve
 *   this.
 */
